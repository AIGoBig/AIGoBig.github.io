https://zhuanlan.zhihu.com/p/91395504



# CART(分类和回归树)

classification and regression tree. CART结构为二叉树.

# Boosting Decision Tree(提升树)

提升树算法：拟合残差

# GBDT(梯度提升树)

gbdt使用的决策树为CART回归树，因为gbdt每次迭代要拟合的是梯度值，是连续值。

# XGBoost(极端梯度提升)

# LGBM(轻量级的高效梯度提升树)

# 面试问题总结

**5.1 XGBoost的优点和缺点：**

优点：

1. 精度高：GBDT 只用到一阶泰勒展开，而 XGBoost 对损失函数进行了二阶泰勒展开。引入二阶导一方面是为了增加精度，另一方面也是为了能够自定义损失函数。
2. 灵活性更强：传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这时xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。
3. 正则化：在目标函数中加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、叶子节点权重的 L2 范式。正则项降低了模型的方差，使学习出来的模型更加简单，有助于防止过拟合。
4. Shrinkage：相当于学习速率。XGBoost 在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间
5. 列抽样：借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算；
6. 缺失值处理：对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向。
7. 可以并行化操作：块结构可以很好的支持并行计算。
8. 可并行的近似直方图算法：树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。
9. 相当于预剪枝：当增益大于阈值时才让节点分裂，上式中的gamma即阈值，它是正则项里叶子节点数T的系数，所以xgboost在优化目标函数的同时相当于做了预剪枝。

缺点：

1. 节点分类需要遍历数据集：虽然利用预排序和近似算法可以降低寻找最佳分裂点的计算量，但在节点分裂过程中仍需要遍历数据集；
2. 预排序空间复杂度高：预排序过程的空间复杂度过高，不仅需要存储特征值，还需要存储特征对应样本的梯度统计值的索引，相当于消耗了两倍的内存。
3. 找到特别精确的分割点，可能存在过拟合。

**5.2 XGBoost和lightGBM怎么调参**

**5.2.1 XGBoost**

**(1) XGBoost重要参数**

xgboost主要分为三类参数:

1.通用参数 general parameter

booster: 每次迭代的面试问题总结

**5.1 XGBoost的优点和缺点：**

优点：

1. 精度高：GBDT 只用到一阶泰勒展开，而 XGBoost 对损失函数进行了二阶泰勒展开。引入二阶导一方面是为了增加精度，另一方面也是为了能够自定义损失函数。
2. 灵活性更强：传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这时xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。
3. 正则化：在目标函数中加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、叶子节点权重的 L2 范式。正则项降低了模型的方差，使学习出来的模型更加简单，有助于防止过拟合。
4. Shrinkage：相当于学习速率。XGBoost 在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间
5. 列抽样：借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算；
6. 缺失值处理：对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向。
7. 可以并行化操作：块结构可以很好的支持并行计算。
8. 可并行的近似直方图算法：树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。
9. 相当于预剪枝：当增益大于阈值时才让节点分裂，上式中的gamma即阈值，它是正则项里叶子节点数T的系数，所以xgboost在优化目标函数的同时相当于做了预剪枝。

缺点：

1. 节点分类需要遍历数据集：虽然利用预排序和近似算法可以降低寻找最佳分裂点的计算量，但在节点分裂过程中仍需要遍历数据集；
2. 预排序空间复杂度高：预排序过程的空间复杂度过高，不仅需要存储特征值，还需要存储特征对应样本的梯度统计值的索引，相当于消耗了两倍的内存。
3. 找到特别精确的分割点，可能存在过拟合。

**5.2 XGBoost和lightGBM怎么调参**

**5.2.1 XGBoost**

**(1) XGBoost重要参数**

xgboost主要分为三类参数:

1.通用参数 general parameter

booster: 每次迭代的**模型选择**，gbtree或者gbliner

silent: 控制打印信息，(过时，被verbosity代替)

nthread：并行的线程数，默认使用最大的核数

2.Booster参数：Parameters for Tree Booster

eta: shinkage**防止过拟合**，默认0.3

gamma:公式中**叶子节点个数的参数**，也就是分类后损失函数的增益，增益大于这个阈值，才会对节点进行分裂。

max_depth:默认为0，**树的最大深度**，用来**避免过拟合**。

min_child_weight: 最小的**叶子节点样本权重和**，如果节点分裂造成一个叶子节点的样本权重和小于该值，则放弃分裂。

max_delta_step: 该参数仙子每棵树权重改变的最大步长。一般用不到，但是在数据样本极度不平衡时，可能对逻辑回归有帮助。

subsample：训练样本随机采样的比例。

colsample_bytree, colsample_bylevel, colsample_bynode: 列采样的参数设置。bytree表示在构建每棵树的时候使用。bylevel表示构建每层节点的时候使用，bynode在每次分裂的时候使用。

lambda: **L2正则化项**。默认为1.

alpha：**L1的正则化项**.

scale_pos_weight: 控制**正负样本的平衡**，**用于不平衡数据**。

3.学习任务参数：控制训练目标的表现

objective:定义**损失函数**。常用值有binary:logistic; multi:softmax; multi:softprob

eval_metic: 验证集的**评估指标**。rmse, mae, logloss, error, merror, mlogloss, auc

seed:随机数的种子，设置它可以复现随机数的结果，也可以用于调整参数。

**(2) XGBoost调参技巧**

a.当出现**过拟合**时，有两类参数可以缓解：

- 第一类参数：用于**直接控制模型的复杂度。包括max_depth, min_child_weight, gamma等参数**
- 第二类参数：用于增加随机性，从而使得模型在训练时对于噪声不敏感，包括subsample, colsample_bytree.

也可以直接**检查步长eta，此时需要增加num_round参数**.

b.当遇到**数据不平衡时(如广告点击率预测任务)**，有两种方式提高模型的预测性能：

- 如果关心的是预测的AUC.

可以通过**scale_pos_weight参数来平衡正负样本的权重**; 使用AUC来评估。

- 如果关心的是预测的正确率

不能重新平衡正负样本**；设置max_delta_step为一个有限的值(如1)，从而有助于收敛**

**5.2.2 lightGBM:**

（1）lightGBM重要参数:

![img](/img/in-post/20_07/v2-36bd366ca641c791ef7036b3e77d626f_720w-20200830112742043.jpg)

![img](/img/in-post/20_07/v2-69a1b88698e57b2edcb7891306c19145_720w-20200830112742031.jpg)

![img](/img/in-post/20_07/v2-e30fd66a9b2ec9148078860fcb5cc643_720w-20200830112742100.jpg)

（2）lightGBM调参技巧：

**针对更快的训练速度**：

- 通过设置bagging_fraction和bagging_feq参数来使用Bagging方法
- 通过设置feature_fraction参数来使用特征抽样
- 使用较小的max_bin
- 使用save_binary将数据保存为二进制文件，来加速数据加载
- 使用parallel learning

**针对更好的准确率**：

- 使用较小的learning_rate和较大的num_iterations.
- 使用更大的num_leaves（可能导致过拟合）
- 使用较大的max_bin（学习速度可能变慢）
- 使用更大的训练数据集
- 尝试dart-训练时候是有用dropout
- 交叉验证

**缓解过拟合**：

- 使用较小的max_bin （分桶粗一些）
- small num_leaves (不要在单棵树上分的太细)
- 使用min_datai_in_leaf和min_sum_hessian_in_leaf（确保叶子节点还有足够多的数据）
- 通过bagging_fractontion和bagging_freq来使用bagging
- 通过featrure_fration来使用特征抽样
- 使用更大的训练数据
- 使用lambda_l1，,lambda_l2和min_gain_to_split来使用正则
- 尝试使用max_depth来避免使用过深的树.