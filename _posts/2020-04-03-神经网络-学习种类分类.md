## 监督学习、无监督学习、半监督学习、强化学习、自监督学习

Reference

[监督学习、无监督学习、半监督学习、强化学习、自监督学习](https://blog.csdn.net/gdengden/article/details/84196883)

#### 监督学习（Supervised Learning）

监督学习是使用已知正确答案的示例来训练网络的。

步骤1：数据集的创建和分类

步骤2：训练

步骤3：验证

步骤4：使用

#### 无监督学习（Unsupervised Learning）

无监督学习适用于你具有数据集但无标签的情况。无监督学习采用输入集，并尝试查找数据中的**模式**。比如，将其组织成群（聚类）或查找异常值（异常检测）。

一些无监督的学习技术包括：

自编码（Autoencoding）

主成分分析（Principal components analysis）

随机森林（Random forests）

K均值聚类（K-means clustering）

如果你想要了解有关无监督学习的更多信息，可以观看Udacity的课程。

无监督学习中最有前景的最新发展之一是Ian Goodfellow（当时在Yoshua Bengio的实验室工作时提出）的一个想法，称为**“生成对抗网络（generative adversarial networks）”**，其中我们将两个神经网络相互联系：一个网络，我们称之为生成器，负责生成旨在尝试欺骗另一个网络的数据，而这个网络，我们称为鉴别器。这种方法实现了一些令人惊奇的结果，例如可以从文本字符串或手绘草图生成如照片版逼真图片的AI技术。

#### 半监督学习（Semi-supervised Learning）

半监督学习在训练阶段结合了**大量未标记的数据和少量标签数据**。与使用所有标签数据的模型相比，使用训练集的训练模型在训练时可以更为准确，而且训练成本更低。

==为什么使用未标记数据有时可以帮助模型更准确，关于这一点的体会就是：即使你不知道答案，但你也可以通过学习来知晓，有关可能的值是多少以及特定值出现的频率。?????==

#### 强化学习（Reinforcement Learning）

强化学习是针对你再次没有标注数据集的情况而言的，但你还是有办法来区分是否越来越接近目标（回报函数（reward function））

> DeepMind在Nature上发表了一篇文章，描述了一个将强化学习与深度学习结合起来的系统，该系统学会该如何去玩一套Atari视频游戏，一些取得了巨大成功（如Breakout），而另一些就没那么幸运了（如Montezuma’s Revenge（蒙特祖玛的复仇））。
>
> Nervana团队（现在在英特尔）发表了一个很好的解惑性博客文章，对这些技术进行了详细介绍，大家有兴趣可以阅读一番。
>

#### 自监督学习

==Yan Lecun 自监督学习：机器能像人一样学习吗？ 110页PPT+视频==

#### Q: 与上一篇度量学习论文有什么联系?

