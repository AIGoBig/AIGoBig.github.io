# 总览

前期知识

![image-20200814210848408](/img/in-post/20_07/image-20200814210848408.png)



![image-20200814211006951](/img/in-post/20_07/image-20200814211006951.png)

#### WMT数据集

语言翻译

<img src="/img/in-post/20_07/image-20200814211210793.png" alt="image-20200814211210793" style="zoom:50%;" />

#### 参考指标bleu

![image-20200814211559646](/img/in-post/20_07/image-20200814211559646.png)

<img src="/img/in-post/20_07/image-20200814211749924.png" alt="image-20200814211749924" style="zoom:50%;" />

#### transform big

#### self-attention

可以降低时间复杂度

具有更强的可解释性, 显示了不同词语间的关联信息.

![image-20200814212529964](/img/in-post/20_07/image-20200814212529964.png)

## transformer 历史意义

![image-20200814212846684](/img/in-post/20_07/image-20200814212846684.png)

1.  提出self-attention, 拉开非序列化模型序幕
2. 为预训练模型到来打下基础
3. bert等

![image-20200814213158860](/img/in-post/20_07/image-20200814213158860.png)



