---
layout: post
comments: true
mathjax: false
title: ""
subtitle: ''
author: "Sun"
header-style: text
tags:
  - 
  - 
  - 
---

# optimizer
#### 作用

根据网络反向传播的梯度信息更新网络参数, 降低loss函数值

#### 结构

1. 优化器需要知道模型的**参数空间**, 因此需要训练前将网络参数放入优化器内

   ```python
   optimizer_G = Adam(model_G.parameters(), lr=train_c.lr_G)  # lr 使用的是初始lr
   optimizer_D = Adam(model_D.parameters(), lr=train_c.lr_D)	
   ```

2. 需要知道反向传播的**梯度信息**

   使用`optimizer.step()`函数, 对参数空间中的grad进行操作, **放在mini-batch内**

   例如使用负梯度下降法

   ```python
   for p in group['params']:
       if p.grad is None:  
           continue
       d_p = p.grad.data
   
       p.data.add_(-group['lr'], d_p)
   ```

   注意: 

   1. 使用前先清零 `optimizer.zero_grad()`, 防止使用的这个grad就得同上一个mini-batch有关

   2. ==跟在反向传播后面, 因为优化器更新参数需要基于反向梯度==

      ```python
      loss.backward()  # 反向传播
      optimizer.step()
      ```

   `scheduler.step（）`按照Pytorch的定义是用来更新优化器的学习率的，一般是**按照epoch为单位**进行更换，即多少个epoch后更换一次学习率，因而scheduler.step()**放在epoch这个大循环下**。

# Loss

## Cross Entropy Error Function（交叉熵损失函数）

`t.nn.CrossEntropyLoss`

### 1.表达式

#### (1) 二分类

在二分的情况下，模型最后需要预测的结果只有两种情况，对于每个类别我们的预测得到的概率为 ![[公式]](https://www.zhihu.com/equation?tex=p) 和 ![[公式]](https://www.zhihu.com/equation?tex=1-p) 。此时表达式为：

![[公式]](https://www.zhihu.com/equation?tex=L+%3D+%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%7D+L_i+%3D+%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%7D-%5By_i%5Ccdot+log%28p_i%29+%2B+%281-y_i%29%5Ccdot+log%281-p_i%29%5D+%5C%5C)

其中：
\- ![[公式]](https://www.zhihu.com/equation?tex=y_i) —— 表示样本i的label，正类为1，负类为0
\- ![[公式]](https://www.zhihu.com/equation?tex=p_i) —— 表示样本i预测为正的概率

#### (2) 多分类

多分类的情况实际上就是对二分类的扩展：

![[公式]](https://www.zhihu.com/equation?tex=L+%3D+%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%7D+L_i+%3D+%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%7D+-%5Csum_%7Bc%3D1%7D%5EMy_%7Bic%7D%5Clog%28p_%7Bic%7D%29+%5C%5C)

其中：
\- ![[公式]](https://www.zhihu.com/equation?tex=M) ——类别的数量；
\- ![[公式]](https://www.zhihu.com/equation?tex=y_%7Bic%7D) ——指示变量（0或1）,如果该类别和样本i的类别相同就是1，否则是0；
\- ![[公式]](https://www.zhihu.com/equation?tex=p_%7Bic%7D) ——对于观测样本i属于类别 ![[公式]](https://www.zhihu.com/equation?tex=c) 的预测概率。

**例1:**

![v2-0c49d6159fc8a5676637668683d41762_720w](https://pic3.zhimg.com/80/v2-0c49d6159fc8a5676637668683d41762_720w.jpg)

![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D++++%5Ctext%7Bsample+1+loss%7D+%3D+-+%280%5Ctimes+log0.3+%2B+0%5Ctimes+log0.3+%2B+1%5Ctimes+log0.4%29+%3D+0.91+%5C%5C++++%5Ctext%7Bsample+2+loss%7D+%3D+-+%280%5Ctimes+log0.3+%2B+1%5Ctimes+log0.4+%2B+0%5Ctimes+log0.3%29+%3D+0.91+%5C%5C++++%5Ctext%7Bsample+3+loss%7D+%3D+-+%281%5Ctimes+log0.1+%2B+0%5Ctimes+log0.2+%2B+0%5Ctimes+log0.7%29+%3D+2.30+%5C%5C+%5Cend%7Baligned%7D+%5C%5C)

![[公式]](https://www.zhihu.com/equation?tex=L%3D%5Cfrac%7B0.91%2B0.91%2B2.3%7D%7B3%7D%3D1.37+%5C%5C)

**例2:**

![img](https://pic3.zhimg.com/80/v2-6d31cf03185b408d5e93fa3e3c05096e_720w.jpg)



![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D++++%5Ctext%7Bsample+1+loss%7D+%3D+-+%280%5Ctimes+log0.1+%2B+0%5Ctimes+log0.2+%2B+1%5Ctimes+log0.7%29+%3D+0.35+%5C%5C++++%5Ctext%7Bsample+2+loss%7D+%3D+-+%280%5Ctimes+log0.1+%2B+1%5Ctimes+log0.7+%2B+0%5Ctimes+log0.2%29+%3D+0.35+%5C%5C++++%5Ctext%7Bsample+3+loss%7D+%3D+-+%281%5Ctimes+log0.3+%2B+0%5Ctimes+log0.4+%2B+0%5Ctimes+log0.4%29+%3D+1.20+%5C%5C+%5Cend%7Baligned%7D+%5C%5C)

![[公式]](https://www.zhihu.com/equation?tex=L%3D%5Cfrac%7B0.35%2B0.35%2B1.2%7D%7B3%7D%3D0.63+%5C%5C)

可以发现，交叉熵损失函数可以捕捉到**模型1**和**模型2**预测效果的差异。

### 2. 函数性质

![img](https://pic3.zhimg.com/80/v2-f049a57b5bb2fcaa7b70f5d182ab64a2_720w.jpg)

可以看出，该函数是凸函数，求导时能够得到全局最优值。

### 3. 学习过程

交叉熵损失函数经常用于分类问题中，特别是在神经网络做分类问题时，也经常使用交叉熵作为损失函数，此外，由于交叉熵涉及到计算每个类别的概率，所以交叉熵几乎每次都和**sigmoid(或softmax)函数**一起出现。

我们用神经网络最后一层输出的情况，来看一眼整个模型预测、获得损失和学习的流程：

1. 神经网络最后一层得到每个类别的得分**scores**；
2. 该得分经过**sigmoid(或softmax)函数**获得概率输出；
3. 模型预测的类别概率输出与真实类别的one hot形式进行交叉熵损失函数的计算。

学习任务分为二分类和多分类情况，我们分别讨论这两种情况的学习过程。

#### 3.1 二分类情况

![img](https://pic1.zhimg.com/80/v2-d44fea1bda9338eaabf8e96df099981c_720w.jpg)

> 二分类交叉熵损失函数学习过程

如上图所示，求导过程可分成三个子过程，即拆成三项偏导的乘积：

![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D++%5Cfrac%7B%5Cpartial+L_i%7D%7B%5Cpartial+w_i%7D+%26%3D+%5Cfrac%7B%5Cpartial+L_i%7D%7B%5Cpartial+p_i%7D%5Ccdot+%5Cfrac%7B%5Cpartial+p_i%7D%7B%5Cpartial+s_i%7D%5Ccdot+%5Cfrac%7B%5Cpartial+s_i%7D%7B%5Cpartial+w_i%7D+%5C%5C++%26%3D+%5B-%5Cfrac%7By_i%7D%7Bp_i%7D%2B%5Cfrac%7B1-y_i%7D%7B1-p_i%7D%5D+%5Ccdot+%5Csigma%28s_i%29%5Ccdot+%5B1-%5Csigma%28s_i%29%5D%5Ccdot+x_i+%5C%5C++%26%3D+%5B-%5Cfrac%7By_i%7D%7B%5Csigma%28s_i%29%7D%2B%5Cfrac%7B1-y_i%7D%7B1-%5Csigma%28s_i%29%7D%5D+%5Ccdot+%5Csigma%28s_i%29%5Ccdot+%5B1-%5Csigma%28s_i%29%5D%5Ccdot+x_i+%5C%5C++%26%3D+%5B-%5Cfrac%7By_i%7D%7B%5Csigma%28s_i%29%7D%5Ccdot+%5Csigma%28s_i%29%5Ccdot+%281-%5Csigma%28s_i%29%29%2B%5Cfrac%7B1-y_i%7D%7B1-%5Csigma%28s_i%29%7D%5Ccdot+%5Csigma%28s_i%29%5Ccdot+%281-%5Csigma%28s_i%29%29%5D%5Ccdot+x_i+%5C%5C++%26%3D+%5B-y_i%2By_i%5Ccdot+%5Csigma%28s_i%29%2B%5Csigma%28s_i%29-y_i%5Ccdot+%5Csigma%28s_i%29%5D%5Ccdot+x_i+%5C%5C++%26%3D+%5B%5Csigma%28s_i%29-y_i%5D%5Ccdot+x_i+%5C%5C+%5Cend%7Baligned%7D+%5C%5C)

可以看到，我们得到了一个非常漂亮的结果，所以，使用交叉熵损失函数，不仅可以很好的衡量模型的效果，又可以很容易的的进行求导计算。

### 缺点

sigmoid(softmax)+cross-entropy loss 擅长于学习类间的信息，因为它采用了类间竞争机制，它只关心对于正确标签预测概率的准确性，忽略了其他非正确标签的差异，导致学习到的特征比较散。基于这个问题的优化有很多，比如对softmax进行改进，如L-Softmax、SM-Softmax、AM-Softmax等。



# Val模式

`model.eval() `,`model.train() ` 

eval（）时，框架会自动把BN和DropOut固定住，不会取平均，而是用训练好的值，不然的话，一旦test的batch_size过小，很容易就会被BN层导致生成图片颜色失真极大

model.train() 和 model.eval() 一般在模型训练和评价的时候会加上这两句，主要是针对由于model 在训练时和评价时 Batch Normalization 和 Dropout 方法模式不同；**因此，在使用PyTorch进行训练和测试时一定注意要把实例化的model指定train/eval；**



Reference

[损失函数 - 交叉熵损失函数](https://zhuanlan.zhihu.com/p/35709485)
$$

$$
