# 飞行器项目

#### 1.项目介绍

针对飞行环境变化导致飞行器飞行性能下降的问题，使用人工智能算法模型，对飞行器进行飞行轨迹规划，使飞行器在全程满足射程、落速、躲避拦截弹、禁飞区等条件下，达到飞行性能最佳。我们尝试了基于强化学习和基于机器学习的方法，这两类方法使用的算法不同，生成数据集的方式也不同，但是两类方法的前提都是需要搭建仿真环境，即飞行器在某个状态下选择不同的动作后，仿真环境能够返回飞行器的下一个状态信息。

#### 2. 强化学习方法

因为强化学习多用在需要与环境交互的场景下，所以基于环境变化对飞行器进行路径规划很适合使用强化学习的方法来解决。

我们的整体思路是：①将飞行器作为智能体，②根据气动力学约束与任务中的一些要求对飞行器的仿真环境进行搭建，包括对飞行器射程、落速和拦截弹、禁飞区等参数进行初始化，还有对飞行器在飞行过程中升力系数、阻力系数、大气密度计算等建模。③然后通过DQN网络来进行强化学习，进而决策飞行器每一步需要执行的最优动作，即选择合适的攻角和后掠翼角，来改变其自身的姿态，达到飞行性能最佳的目的。

在这里，我们将每一时刻飞行器的位置信息、速度信息、到目标点的距离信息、拦截弹和禁飞区的信息作为环境的观测值（状态）。

对每个状态执行的动作为：选择合适的攻角和后掠翼角。（攻角-15-+15，后掠翼角三种）

奖励reward设置为，当飞行器满足任务要求，对其进行一个奖赏，否则对其进行惩罚。奖励的值为 当前的高度/70，失败给-10。

**获得初始训练数据：**为了使强化学习具有一定的随机程度，所以初始训练数据是通过e-贪心算法，设置e=0.9，即以0.9的概率选择模型输出的最优动作，以0.1的概率随机选择动作。智能体在当前状态s下执行某个动作a后，仿真环境会计算智能体的下一个状态s'，并判断本次规划是否成功（标志位done），计算本次动作的奖励信号r，将四元组 [s, a, r, s'] 作为一条训练样本，放到DQN的记忆区，重复这个过程，直到训练样本数量到达记忆区容量（1024）。

获得初始训练数据后，DQN使用记忆区中的初始训练数据对QNet进行一次训练，使用更新后的模型指导智能体进行一次动作选择与执行，仿真环境计算后，得到一条新的训练样本，即新的状态和reward，放入DQN记忆区顶端，同时删除DQN记忆区中最底部的数据。重复本步骤进行交互训练，直到达到设定的停止条件飞（行器在某一幕的成功或失败）。

（得到初始训练数据后，用记忆区的数据训练并更新模型。之后的训练数据通过更新后的模型来决策执行新的动作，接下来每飞行一步，将这一步的动作与状态替换记忆区最旧的数据，用新的整个记忆区训练样本训练模型一次，用更新后的模型指导飞行器再飞行一步，再更新记忆区的训练样本数据，循环进行。）

**训练过程：** 由于执行动作后得到的reward是不变的，所以相应的损失函数（均方误差）为：L(θ) = MSE((r + γQnext)- Q)，即当前的reward + γ预期未来收益值Qnext减去执行当前动作得到的价值评估值Q。实际上在拟合当前reward r。（实际上它们的差就是 **实际reward** 减去 **现有模型认为在s下采取a时能得到的reward值（Q' - Q = r'）**）

![img](https://pic1.zhimg.com/80/v2-7f296080b1e4eb1e190ec8113e16fa68_720w.png)

因为DQN中包括两个DNN网络分别为**实时网络和滞后网络（target net）**，这两个网络的结构完全一样（四层：输入层，两层隐层，输出层），但是参数不同。实时网络用于计算执行本次动作得到的价值评估值Q，网络的参数每次迭代时都会进行更新；滞后网络用于计算执行本次动作后的预期未来收益值Qnext，每隔一定的迭代次数后（20），将实时网络的参数同步到滞后网络参数。（好处是使得上面公式中target所标注的部分是暂时固定的，我们不断更新`θ`追逐的是一个固定的目标，而不是一直改变的目标。）

每次从记忆区（经验池）中随机抽取少量四元组作为一个batch（512），来更新实时网络的参数，进行训练，直到达到设定的**停止条件**（总迭代次数5000）。训练完成，保存模型参数。



**测试阶段**：

① 初始化仿真环境、状态空间、动作空间、智能体状态。

② 智能体每次调用QNet选择最佳动作并执行。

③ 仿真环境判断智能体是否成功达到指定目标点或不满足约束条件而失败。如果成功到达或失败，则仿真结束，否则回到步骤②重复进行。

禁飞区：半径150的圆柱体。最小安全距离30，当小于这个安全距离表明失败。

#### 3.机器学习方法

将整个飞行器的飞行轨迹分为前段巡航阶段和末端打击段，前段和末端均设置一个目标位置，根据飞行器是否能到达对应位置和到达对应位置的速度（到达指定的全程距离时的速度）作为标签，将路径规划任务作为一个二分类任务和回归任务。使用线性回归、逻辑回归、Xgboost和LightGBM模型进行训练，将飞行器当前的状态（飞行器的位置、速度、高度、攻角、后掠翼角、禁飞区距离、与目标位置的距离等）作为模型的输入，**仿真过程中，每次调用训练好的模型对所有可行的动作，即对攻角与后掠翼角的组合进行预测，根据分类模型预测成功达到目标的概率大小与回归模型预测的到达目标的速度大小，选择概率最大或速度最大的最佳动作并执行。**

#### 训练数据生成

在每个阶段，飞行器每步使用随机选择策略选择一个动作，直到飞行器达成本阶段设置的目标，或因为不满足约束条件而失败，完成一次仿真，得到一条飞行路径，同时得到此条路径终点的信息，如是否到达，末速度大小。

**预处理：**

对每条路径中的每个点的状态信息*s*，与此条路径终点的信息*s*’,组合成[*s*, *s*’]的形式作为一条训练数据，*s*’中的是否到达标志和末速度大小作为本条训练数据的标签数据。（对于每一条路径，以飞行仿真时间步为单位拆分成多条训练样本，把在当前时间步的状态信息（包括决策动作）和这条路径最终是否到达终点和末速度大小拼接在一起，成为一条训练样本。）

但是由于随机选择动作的策略，使得成功到达的轨迹较少，大多为失败的轨迹，所以为了让模型更容易收敛，我们负样本进行下采样处理。使最终的正负样本比例在1:5左右。

**评价指标：**

分类指标使用AUC（ROC曲线下方的面积，随机选择一个正样本和负样本，分类器对正样本的打分大于该负样本的概率）：希望飞行器优先选择最终能够成功到达终点的动作（把成功的动作排在失败动作之前）。

回归指标使用MSE（均方误差）。

分类任务：前段LightGBM的AUC：0.898；末段XGBoost的AUC：0.863

最终进行实验发现将两个阶段**全部当成分类任务**，且前段**使用Lightgbm和末段使用Xgboost模型**的组合方式可以成功生成飞行路径。

**机器学习方法相比于前面基于强化学习的方法，落速较慢，而且需要人工对大量的数据进行标注与预处理，而强化学习方法不需要对数据进行预处理，实现自动化。**



**正负样本的比例差距大，导致分类效果差的理论依据？**我用一个简单的例子说明下这个问题：当训练集中正样本占90%，负样本占10%。此时训练过程中正样本的loss会占主导，网络会特别照顾正样本，而且正常情况下用常规网络会用准确率来评价训练结果，因此在上述样本不均衡情况下即使网络将所有样本分为正样本，精度仍然有90%。但这显然是不正确的网络。

抽样是解决样本分布不均衡相对简单且常用的方法，包括过抽样和欠抽样两种。

**过抽样**（也叫上采样、over-sampling）方法通过增加分类中少数类样本的数量来实现样本均衡，最直接的方法是简单复制少数类样本形成多条记录，这种方法的缺点是如果样本特征少而可能导致过拟合的问题；经过改进的过抽样方法通过在少数类中加入随机噪声、干扰数据或通过一定规则产生新的合成样本，例如SMOTE算法。

**欠抽样**（也叫下采样、under-sampling）方法**通过减少分类中多数类样本的样本数量来实现样本均衡，最直接的方法是随机地去掉一些多数类样本来减小多数类的规模，缺点是会丢失多数类样本中的一些重要信息**。

总体上，过抽样和欠抽样更适合大数据分布不均衡的情况，尤其是第一种（过抽样）方法应用更加广泛。



#  强化学习相关知识

#### 1. 强化学习

强化学习就是学习：做什么（即如何将当前的环境状态映射成动作）才能使得数值化的收益信号最大化。

强化学习的**四个核心要素**：策略(当前环境状态下选择执行动作的策略)、收益信号（reward）、价值函数(Q)、对环境建立的模型（仿真环境搭建）。

讲强化学习先讲其适用的场景。强化学习多用在需要与环境交互的场景下，即给定一个环境的状态（`State`），程序根据某种策略（`Policy`）选出一个对应的行为（`Action`），而执行这个`Action`后环境又会发生改变，即状态会转换为新的状态`S'`，且每执行完一个`Action`后程序会得到一个激励值（`Reward`），而**程序就依据得到的激励值的大小调整其策略，使得在所有步骤执行完后，即状态到达终止状态（`Terminal`）时，所获得的`Reward`之和最大**。

总结一下，在强化学习中，我们关注的有如下几点：

- 环境观测值/状态 **S**tate
- 动作选择策略 **P**olicy（Q学习是一种关于策略的选择方式）
- 执行的动作/行为 **A**ction
- 得到的奖励 **R**eward
- 下一个状态 **S’**

<img src="C:\Users\LDC\AppData\Roaming\Typora\typora-user-images\image-20210613171136134.png" alt="image-20210613171136134" style="zoom:80%;" />

**强化学习执行流程如下图所示，`Agent`是我们的飞行器，它观察当前环境`Environment`并获得当前状态`state`（根据建立的仿真环境得到当前的状态信息：飞行器当前的空间位置、速度大小、速度方向、距离目标点的距离、距离拦截弹的距离、距离禁飞区的距离），依据它的选择策略`Policy`对当前状态`state`做出相应动作`action`（选择相应的攻角与后掠翼角），此时能得到一个激励值`reward`，且环境`Environment`改变了，因此执行完当前的动作`Agent`会得到一个新的状态`state`，并继续执行下去。**



Q:仿真环境通过攻角和后掠翼角可得到速度？

<img src="https://pic1.zhimg.com/v2-dab98a1a364ebf9a4690b2d5b7e12e30_r.jpg" alt="preview" style="zoom:50%;" />

#### 2. Q-Learning —— “策略” 选择的方式

Q学习算法是强化学习中的一种，更准确的说，是一种关于策略的选择方式。

实际上，我们可以发现，强化学习的核心和训练目标就是**选择一个合适的策略`Policy`，使得在每个`epoch`结束时得到的`reward`之和最大。**
Q学习的思想是：`Q(S, A)` = 在状态`S`下，采取动作`A`后，**未来**将得到的**奖励`Reward`值之和**。



#### **2.1 问题：**如果能知道选取那个动作能使得未来得到的**奖励之和最大**，那么选取哪个动作就很确定了。但是，**`Q`函数的值到底是怎么得到的呢？**

举例：

- **状态State**：将每一帧作为一个状态，取小鸟离下一个地面上柱子在水平和竖直方向上的距离作为状态的观测值，即下图中的(△x, △y)；

<img src="https://pic3.zhimg.com/80/v2-9813e35d0f6dd91c4ce7e48e261f1136_720w.jpg" alt="img" style="zoom:50%;" />



- **行为Action**：对每一个状态（每一帧），只有两种选择：**跳，不跳**；
- **奖励Reward**：小鸟活着时给每帧奖励1，死亡时奖励-1000。



#### 2.2 **问题：在该游戏中，程序是如何选择该跳还是不该跳呢？**（如何在当前环境状态下，选择执行什么动作呢？）

按照前面说的Q学习算法，那么它应该是需要有一个`Q(S, A)`价值函数的，可以知道在什么状态时采取什么样的行为能得到最大的Reward之和。在这个游戏中，很显然**状态和动作的组合都是有限的**，因此可以维护一个`S-A`表，其记录了在每个状态下，采用什么动作时能得到什么样的`Q`（奖励之和）值。表格形式如下，只要程序在运行中**不断更新这个表格，使其最终能收敛**，那么程序就能拿得到的`state`**通过查表的方式来判断它该选择什么样的行为，才能获得最大的Q值。**

![img](https://pic2.zhimg.com/80/v2-80cc46b1bc57c7e9846817986df90c69_720w.jpg)

####  2.3 **Q值的两种更新方法**

①一种是类似上面例子中的情况，状态和行为的组合是**可以穷尽**的情况，这时候往往采用的是`S-A`表格的形式记录Q值。

②如果状态和行为的组合**不可穷尽**，比如自动驾驶中输入的外界环境照片与车速之间的组合是有无穷种的，那么前一种方法显然就不适用了，**这时候常用的方式为将深度学习与Q学习结合起来，也就是DQN。**



#### 2.3.1 **S-A表格如何更新**

对于使用`S-A`表格的情况，需要如何更新其表格中的Q值，使得其在每一个状态下都能选择总体最优的策略呢？
这里首先引出Q值的更新方法：

![img](https://pic2.zhimg.com/80/v2-a86c19603efb3a6eaee35040ce0e106d_720w.png)


解释一下，`s`为状态State，`a`为采取的行为Action，`α`参数用来表示新的值对更新后值所造成的影响大小，`r`为在状态`s`下采取动作`a`后获得的奖励Reward，`γ`也是一个discount（折扣）值，即用来减小新值的影响的值。其中`α`和`γ`的范围都在`0~1`之间。

先看左边一部分，如果不看`(1 - α)`，那么就是在状态`s`下采取动作`a`时的**旧的Q值**，乘以`(1 - α)`是因为要更新它，但是也不能把旧的Q值全盘否定了呀。

再看右边部分，同样先不看`α`，里面是激励值`r`加上一部分，而后面那部分先去掉`γ`也不看，就是在下一个状态下的最大Q值，想一想，**状态s下采用动作a后得到的激励值r加上下一个状态下的最大Q值，不就是一个新的Q值么**，它这是在**不断使得激励值`r`收敛啊**，而像`α`和`γ`只是用来控制新的Q值和旧的Q值各占多少权重罢了。



#### 2.3.2 **策略Policy选择注意事项**

前面讲了，在强化学习中最重要的部分就是策略的选择，`S-A`表格说白了也不过是给选择哪个策略提供了一个参考。而在实际实验中，**如果对每个状态`s`值，都选择其能获得最大Q值的行为去执行，是有问题的！**
假使初始的S-A表格所有值全为0，那么在状态s采用随机一个行为（比如a1），并第一次获得reward后，如果reward值大于0，那么以后再遇见状态s时，程序都会直接采用行为a1，然而，还**有很多动作都没有尝试过，说不定采取其他的行为会使得它能得到的Q值更大。**

#### 2.3.3 e-贪心算法

因此在强化学习中，往往需要设置一个阈值`ε`来保持一定的随机程度

① 在每次做决定前，先生成一个随机数，如果这个随机数比`ε`小，那么就随机选取一个action，否则才选取当前已知条件下能使得Q值最大的action。

② 这个阈值`ε`往往一开始被设置地很大，而其值也会随着程序不断地迭代而慢慢衰减，一般也需要给其设置一个最小值，即衰减到最小值后就停止衰减了。这样的**好处是使得程序可以遍历所有的`S-A`对，以准确判断在给定状态下选择哪个行为最优**，而这种做法被称为**exploration（试探）**，这种算法叫做 **e-greddy**。

### 3. DQN - 深度Q网络

DQN属于DRL（深度强化学习）的一种，它是**深度学习与Q学习的结合体。**前面讲了采用`S-A`表格的局限性，当状态和行为的组合不可穷尽时，就无法通过查表的方式选取最优的Action了。这时候就该想到深度学习了，想**通过深度学习找到最优解在很多情况下确实不太靠谱，但是找到一个无限逼近最优解的次优解，倒是没有问题的。**（最优解：S状态下，选择最优动作A）

因此DQN实际上，总体思路还是用的Q学习的思路，不过对于**给定状态选取哪个动作所能得到的Q值，却是由一个深度神经网络来计算的**，其流程图如下：

![img](https://pic1.zhimg.com/80/v2-ed869e5520e2bfd920ec1ebd1b16d358_720w.jpg)

#### **3.1 DNN如何训练**

#### 3.1.1 获取训练集及损失函数

现在我们的选择哪个动作，是由DNN来做决定的，因此我们需要训练DNN以使其能达到令人满意的表现。这显然是一个**监督学习**的问题，那么**训练集**是什么，**标签**是什么，**损失函数**又是什么？

标签理论上应该是飞行器在S状态下，执行各最优动作之后的reward之和Q。但是，事实上我们不知道最优的Q值是多少。

**对状态s，执行动作a，那么得到的reward是一定的，而且是不变的！因此需要考虑从reward下手，让预测Q值和真实Q值的比较问题转换成让模型实质上 在拟合reward 的问题。**

![img](https://pic1.zhimg.com/80/v2-7f296080b1e4eb1e190ec8113e16fa68_720w.png)

公式描述的就是模型的**损失函数**，大括号外面就是求一个**均方差**，我们主要看括号里面。前面被target标出来的地方是这一步得到的reward+下一状态所能得到的最大Q值，它们减去这一步的Q值，那么实际上它们的差就是 **实际reward** 减去 **现有模型认为在s下采取a时能得到的reward值（Q' - Q = r'）**。



现在的问题就已经转换为需要一组训练集，它能够提供一批四元组（s, a, r, s’），其中s’为s执行a后的下一个状态。如果能有这样一个四元组，就能够用来训练DNN了，这就是我们要介绍的**experience reply(经验池，记忆区)**。

**Experience Reply**
前面提到我们需要一批四元组（s, a, r, s’）来进行训练，因此**我们需要缓存一批这样的四元组到经验池中以供训练之用**。由于每次执行一个动作后都能转移到下一个状态，并获得一个reward，因此我们**每执行一次动作后都可以获得一个这样的四元组**，也可以将这个四元组直接放入经验池中。
我们知道这种四元组之间是存在关联性的，因为状态的转移是连续的，**如果直接按顺序取一批四元组作为训练集，那么是容易过拟合的**，因为训练样本间不是独立的！为解决这个问题，我们可以简单地从经验池中随机抽取少量四元组作为一个batch，这样既保证了训练样本是**独立同分布**的，也使得每个batch**样本量不大**，能加快训练速度。

![preview](https://pic3.zhimg.com/v2-3b1bacc4074dbfd34d40b955d9696d6e_r.jpg)

#### 3.1.2 **Target Network**
上面的代码似乎已经能够正常运行了，为什么又冒出一个target network呢？回想下前面那个公式，这里重新搬到下面来，是不是之前说target和`θi-`都先忽略，现在就该解释一下了。

![img](https://pic1.zhimg.com/80/v2-7f296080b1e4eb1e190ec8113e16fa68_720w.png)


这个公式里`θi-`和`θi`肯定是有区别的，不然也犯不着用两个符号了。**事实上，我们需要设计两个DNN（实时训练网络eval net 和之滞后网络 target net），它们结构完全一样，但是参数不一样，即神经网络中各层的权重、偏置等，一个的参数是`θi-`，而另一个是`θi`。我们每次迭代中，更新的是`θi`而不更新`θi-`，且规定每运行C步（迭代C步之后，需要更新target net 中的权重，让其等于eval net 的权重）后让`θi- = θi`。而其`θi-`所在的网络就被称为target network。**

为什么要弄这么奇怪的东西？
这也是为了**防止过拟合**。试想如果只有一个神经网络，那么它就在会不停地更新，那么它所追求的目标是在一直改变的，即在θ改变的时候，不止Q(s, a)变了，max *Q(s’, a’)*也变了。这样的好**处是使得上面公式中target所标注的部分是暂时固定的，我们不断更新`θ`追逐的是一个固定的目标，而不是一直改变的目标。**

