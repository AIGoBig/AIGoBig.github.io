---
layout: post
Comment: true
mathjax: true
author: "Sun"
header-style: text
tags:
  - 算法
catalog: true

---



# 深度网络(NN)

## 如何解决某问题

### 解决过拟合的方法有哪些

- 数据增强
- 模型集成

	- Dropout

		- 相当于每次都在训练不同结构的神经网络
		- 类比bagging，可被认为是一种大规模深度神经网络的模型集成方法
		- 轻量级的bagging集成近似

- 参数正则化
- **批量归一化（Batch Normalization）**
- NN本质是学习数据分布，有两种分布不同：
	训练集和测试集数据分布不同，降低泛化能力。
随着网络训练，隐层网络参数变化会使网络在每次迭代拟合不同的分布，增大复杂度且易过拟合
- 数学原理：（x-均值）/ 标准差

### 解决小样本问题的track有哪些

- 数据增强
- 迁移学习
- 无监督、半监督
- GAN网络
- 网络优化

	- 考虑**样本相关性** ？？？？
	- 多使用残差学习和**特征融合**

## 不同的神经网络

### 深度网络都有哪些？

- VGG
- Resnet
- Inception V1-v3
- Densnet
- ResnXt
- SENet
- NAS
- 下面四个都是轻量化模型：
  - MobileNet V1-V2
  - mobileNet V3（novel）：https://arxiv.org/pdf/1905.02244.pdf
  - Xception
  - shufflenet V1-V2
  - squeezenet
- group convolution
- SEResNet，SEResNeXt
- NAS
- EfficientNet

### CNN的三大优势

- 局部连接

  - 更少的存储空间，还提高了统计效率
  - 深层的网络间接的将整个或者大部分图像进行了全连接。

- 权值共享

  - 减少参数数量

- 平移不变性

  - 不去关心特征出现的位置而只是关心它是否出现

### CNN的结构

- 输入层
- 卷积层

  - 卷积核，提取图像特征

- 激活函数

  - 增强网络的非线性表达能力
  - 为防止梯度弥散的发生  ？？？？

- 池化层

  - 降维，减少数据量

- 输出层

  - 将特征映射到标记空间进行了预测

- 目标函数(损失函数)

  - 计算预测值和实际值的误差

### ResNet为什么有效

参考：[为什么ResNet和DenseNet可以这么深？一文详解残差块为何有助于解决梯度弥散问题。](https://blog.csdn.net/nini_coded/article/details/79582902)

- 从梯度弥散、网络退化角度看

	- **梯度消失/爆炸**已经通过 normalized 、initialization 等方式得到解决。
	- 残差结构是为了解决**网络退化**的问题提出的，跨层输入相当于一个恒等映射，中间层只需拟合残差，因此层的加入不会使效果变差
- 在网络上堆叠这样的结构，**就算梯度消失，我什么也学不到，我至少把原来的样子恒等映射了过去**，相当于在浅层网络上堆叠了“**复制层**”，这样**至少不会比浅层网络差**。万一我不小心学到了什么，那就赚大了，由于我经常恒等映射，所以我学习到东西的概率很大。
- 从**梯度反向传播的角度**解释

	- 即过于深的网络在反传时容易发生梯度弥散，一旦某一步开始导数小于1，此后**继续反传**，传到前面时，用float32位数字已经无法表示梯度的变化了，相当于梯度没有改变，也就是浅层的网络学不到东西了。**这是网络太深反而效果下降的原因**。
- 加入ResNet中的shortcut结构之后，在**反传时，每两个block之间不仅传递了梯度，还加上了求导之前的梯度**，这相当于把每一个block中向前传递的梯度人为加大了，也就会**减小梯度弥散的可能性**。
- 解决**欠拟合**问题(待研究?????)

	- 在正向卷积时，对每一层做卷积其实只提取了图像的一部分信息，这样一来，越到深层，原始图像信息的丢失越严重，而仅仅是对原始图像中的一小部分特征做提取。 这显然会发生类似欠拟合的现象。
加入shortcut结构，相当于在**每个block中又加入了上一层图像的全部信息，一定程度上保留了更多的原始信息。**
- 由于每做一次卷积（包括对应的激活操作）都会浪费掉一些信息：比如卷积核参数的随机性（盲目性）、激活函数的抑制作用等等。这时，**ResNet中的shortcut相当于把以前处理过的信息直接再拿到现在一并处理，起到了减损的效果。**
- 加入shortcut后**相当于一个ensemble模型**

	- **输出的结果是前面各个block及其组合一起做的一个投票选出的结果**。即可以把ResNet网络看成是多个子网络并行，从实验中看，真实起作用的路径长度并不深，主要走是中等深度的网络。简单来说，就是**做了不同层次上的特征组合。**
- 特征具有层次性的角度看

	- **回到网络结构上面，浅层网络提取的是简单的特征**，而简单和复杂的特征适用于不同的样本，没有shortcut时，对所有样本的分类都是利用最复杂的特征判断，费时费力；**加入shortcut后，相当于保留了一些简单的特征用于判断，变得省时。这一观点主要解释了为什么ResNet网络能够更快收敛。**

### Dense-Net

与ResNet想法类似的DenseNet，是**把前面每一个block输出的梯度都skip connection到后面各个block的输出**，同时在同一层增加了类似Inception的选择式结构（待研究）？？？？？，介绍了参数，进一步提升了效果，该方法论文还获得了2017CVPR best paper。

![这里写图片描述](/img/in-post/20_07/70.jpeg)

### SE-ResNet



## 原理性知识

### BN（Batch Normalization）层的原理

BN层的**本质**：`γ`和`β`是需要学习的参数，本质是利用优化来改变方差大小和均值的位置。

> ![img](/img/in-post/20_07/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzIxOTk3NjI1,size_16,color_FFFFFF,t_70.png)

### BN层的作用

- 加快**训练速度**增大学习率（可以保证每一层的**输入分布稳定**，这可以使训练加速）
- 减小**梯度消失和梯度爆炸**
- 控制**过拟合**，可以少用或者不用dropout和正则
- **使网络对初始权重不敏感**

### BP算法改进方法

- 正则化

	- 防止神经网络的过度拟合，需要对代价函数改进，即加入正则化项
		- 第一项是均方误差，第二项是正则化，即权重衰减项，λ对两项的重要性进行控制。

- 动量因子的权值调节方法

		- 其中ｗ指代的是要更新的参数，ｚ表示迭代的次数，v表示动量，η为动量前的系数，J同前面的代价函数。

### 1X1卷积作用

### 深度分离卷积的理解

# 机器学习

### 特征

- 如何做特征组合？

	- 特征组合的思想很简单，通过将单独的特征进行组合（相乘或求笛卡尔积）而形成的合成特征。
比如属性A有三个特征，属性B有两个特征，笛卡尔积后就有六个组合特征，然后用one hot 或其他embedding方式给新的特征编码。

### 决策树

- 量化分类效果的方法

	- 信息增益（ID3）

		- 信息增益的度量标准：

			- 熵

		- ID3算法的核心思想

			- 以信息增益度量属性选择，选择分裂后信息增益最大的属性进行分裂。

		- 为了精确地定义信息增益，我们先定义信息论中广泛使用的一个度量标准称为熵（entropy）

			- 它刻画了

				- 任意样例集的纯度（purity）。

			- 给定包含关于某个目标概念的正反样例的样例集S，那么S相对这个布尔型分类的熵为：

				- 

				  p+为正样本概率，p-为负样本概率

				- 

	- 信息增益率（C4.5）
	- 基尼系数（CART）

- 决策树的灵魂

	- **即依靠某种指标进行树的分裂达到分类/回归的目的，总是希望纯度越高越好。**

- 决策树类型

	- 分类树分析是指预测结果是数据所属的类（比如某个电影去看还是不看）
	- 回归树分析是指预测结果可以被认为是实数（例如房屋的价格，或患者在医院中的逗留时间）
	- 分类回归树（CART，Classification And Regression Tree）分析是用于指代上述两种树的总称
	- 分类的目标是根据已知样本的某些特征，判断一个新的样本属于哪种已知的样本类，它的结果是离散值。而回归的结果是连续的值。当然，本质是一样的，都是特征（feature）到结果/标签（label）之间的映射。

### 集成学习

- 定义

	- 集成学习是指用某种策略将多个分类器预测的结果集成起来，作为最终的预测结果

- 分类

	- Boosting方法（串行，分类器互相依赖）

		- 基本思想

			- 将基分类器层层叠加，1. 每一层训练时对前一层错分样本给予更高的权重。 2.分类错误率低的基分类器给予更高的权重
			- 测试时，根据各层分类器的结果加权得到最终结果

		- 减少集成分类器的偏差
		- 分类

			- AdaBoost(自适应增强)

				- 基本思想

					- 前一个分类器分错/分对的样本的权值会得到加强/降低，加权后的全体样本再次被用来训练下一个基本分类器)

			- GBDT形式(梯度提升决策树)

				- 基本思想

					- GBDT的每一次计算都为了减少上一次的残差，进而在负梯度的方向上建立一个新的模型
					- 根据 当前模型损失函数的负梯度信息 来训练新加入的弱分类器，然后把训练好的弱分类器以累加的形式结合到现有模型里

				- 例子

					- XGBoost

						- 目标函数

							- 目标函数分为两个部分：误差函数(logistic损失函数、平方损失函数)和正则化项（定义模型的复杂度）
							- 将目标函数化简之后，目标函数只依赖于一阶导数g和二阶导数h
							- （将目标函数和正则化项结合化简，对w进行求导，求出最优w，代入目标函数中）

						- 与GBDT不同之处？

							- 用泰勒展开近似目标函数

						- XGBoost为什么使用泰勒二阶展开？为什么用二阶信息不用一阶？
						- XGBoost在什么地方做的剪枝，怎么做的？
						-  XGBoost如何分布式？特征分布式和数据分布式？ 各有什么存在的问题？
						- XGBoost里处理缺失值的方法？
						- XGBoost有那些优化？
						- xgboost对预测模型特征重要性排序的原理？
						- XGBoost如何寻找最优特征？是又放回还是无放回的呢？
						- GBDT和XGBoost的区别是什么？
						- lightgbm和xgboost有什么区别？他们的loss一样么？ 算法层面有什么区别？

	- bagging方法（可并行，分类器无依赖）

		- 基本思想

			- 将训练集分为若干子集，分别训练各个基分类器
			- 测试时，每个基分类器单独做出判断，通过投票方式做出最终决策

		- 减少集成分类器的方差
		- 例子

			- 随机森林

### sklearn

- 支持向量机

	- 区别

		- SVM=Support Vector Machine 是支持向量
		- SVR=Support Vector Classification就是支持向量机用于分类，
		- SVC=Support Vector Regression.就是支持向量机用于回归分析

	- 算法（python-sklearn）-- SVM的几种模型

		- svm.LinearSVC Linear Support Vector Classification.
		- svm.LinearSVR Linear Support Vector Regression.
		- svm.NuSVC Nu-Support Vector Classification.
		- svm.NuSVR Nu Support Vector Regression.
		- svm.OneClassSVM Unsupervised Outlier Detection.
		- svm.SVC C-Support Vector Classification.

			- sklearn系列之 sklearn.svm.SVC详解
- svm.SVR Epsilon-Support Vector Regression.

### 启发式搜索算法总结

> 启发式搜索算法蕴含着许多人生哲学，它虽不是数学方法，其思想更类似于人类解决问题的思想和一些人生中总结的道理，值得好好体会。最后用网上一段描述各种搜索算法的例子来作为总结：
>
> 为了找出地球上最高的山，一群有志气的兔子们开始想办法。
> （1）兔子朝着比现在高的地方跳去。他们找到了不远处的最高山峰。但是这座山不一定是珠穆朗玛峰。这就是爬山法，它不能保证局部最优值就是全局最优值。
> （2）兔子喝醉了。他随机地跳了很长时间。这期间，它可能走向高处，也可能踏入平地。但是，他渐渐清醒了并朝他踏过的最高方向跳去。这就是模拟退火。
> （3）兔子们知道一个兔的力量是渺小的。他们互相转告着，哪里的山已经找过，并且找过的每一座山他们都留下一只兔子做记号。他们制定了下一步去哪里寻找的策略。这就是禁忌搜索。
> （4）兔子们吃了失忆药片，并被发射到太空，然后随机落到了地球上的某些地方。他们不知道自己的使命是什么。但是，如果你过几年就杀死一部分海拔低的兔子，多产的兔子们自己就会找到珠穆朗玛峰。这就是遗传算法。

# 比赛项目知识点

# 参考

## 深度网络参考

[【计算机视觉算法岗面经】“吐血”整理：2019秋招资料](https://blog.csdn.net/liuxiao214/article/details/83043170)

[【计算机视觉算法岗面经】“吐血”整理：2019秋招面经](https://blog.csdn.net/liuxiao214/article/details/83043197)

[图像分类丨ILSVRC历届冠军网络「从AlexNet到SENet」](https://blog.csdn.net/woshicver/article/details/105140874)

## 面经

[深度学习分类模型面试题-面经（一）](https://blog.csdn.net/qq_21997625/article/details/105977807)

[腾讯NLP算法岗面经（offer已拿）](https://zhuanlan.zhihu.com/p/117450353)

## 