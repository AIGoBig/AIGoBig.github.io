---
layout: post
Comment: true
mathjax: true
author: "Sun"
header-style: text
tags:
  - 算法
catalog: true

---

主要对算法知识点按照深度网络、机器学习、特征工程和竞赛track来进行分类。

# 深度网络(NN)

## 神经网络原理

### 深度网络都有哪些？

- VGG
- Resnet
- Inception V1-v3
- Densnet
- ResnXt
- SENet
- NAS
- 下面几个是轻量化模型：
  - MobileNet V1-V2
  - mobileNet V3（novel）：https://arxiv.org/pdf/1905.02244.pdf
  - Xception
  - shufflenet V1-V2
  - squeezenet
- group convolution
- SEResNet，SEResNeXt
- NAS
- EfficientNet

### CNN的三大优势

- 局部连接

  - 更少的存储空间，还提高了统计效率
  - 深层的网络间接的将整个或者大部分图像进行了全连接。

- 权值共享

  - 减少参数数量

- 平移不变性

  - 不去关心特征出现的位置而只是关心它是否出现

### CNN的结构

- 输入层
- 卷积层

  - 卷积核，提取图像特征

- 激活函数

  - 增强网络的非线性表达能力
  - 为防止梯度弥散的发生 （待研究）

- 池化层

  - 降维，减少数据量

- 输出层

  - 将特征映射到标记空间进行了预测

- 目标函数(损失函数)

  - 计算预测值和实际值的误差

### ResNet为什么有效

参考：[为什么ResNet和DenseNet可以这么深？一文详解残差块为何有助于解决梯度弥散问题。](https://blog.csdn.net/nini_coded/article/details/79582902)

- 从梯度弥散、网络退化角度看

	- **梯度消失/爆炸**已经通过 normalized 、initialization 等方式得到缓解。残差结构是为了解决**网络退化**的问题提出的，跨层输入相当于一个恒等映射，中间层只需拟合残差，因此层的加入不会使效果变差
	- 在网络上堆叠这样的结构，**就算梯度消失，我什么也学不到，我至少把原来的样子恒等映射了过去**，相当于在浅层网络上堆叠了“**复制层**”，这样**至少不会比浅层网络差**。万一我不小心学到了什么，那就赚大了，由于我经常恒等映射，所以我学习到东西的概率很大。
- 从**梯度反向传播的角度**解释
  - 即过于深的网络在反传时容易发生梯度弥散，一旦某一步开始导数小于1，此后**继续反传**，传到前面时，用float32位数字已经无法表示梯度的变化了，相当于梯度没有改变，也就是浅层的网络学不到东西了。**这是网络太深反而效果下降的原因**。
  - 加入ResNet中的shortcut结构之后，在**反传时，每两个block之间不仅传递了梯度，还加上了求导之前的梯度**，这相当于把每一个block中向前传递的梯度人为加大了，也就会**减小梯度弥散的可能性**。
- 解决**欠拟合**问题角度解释(待研究)

  - 在正向卷积时，对每一层做卷积其实只提取了图像的一部分信息，这样一来，越到深层，原始图像信息的丢失越严重，而仅仅是对原始图像中的一小部分特征做提取。 这显然会发生类似欠拟合的现象。
  加入shortcut结构，相当于在**每个block中又加入了上一层图像的全部信息，一定程度上保留了更多的原始信息。**
  - 由于每做一次卷积（包括对应的激活操作）都会浪费掉一些信息：比如卷积核参数的随机性（盲目性）、激活函数的抑制作用等等。这时，**ResNet中的shortcut相当于把以前处理过的信息直接再拿到现在一并处理，起到了减损的效果。**
- 模型集成角度解释（加入shortcut后**相当于一个ensemble模型**）

	- **输出的结果是前面各个block及其组合一起做的一个投票选出的结果**。即可以把ResNet网络看成是多个子网络并行，从实验中看，真实起作用的路径长度并不深，主要走是中等深度的网络。简单来说，就是**做了不同层次上的特征组合。**
- 特征具有层次性的角度解释

	- **回到网络结构上面，浅层网络提取的是简单的特征**，而简单和复杂的特征适用于不同的样本，没有shortcut时，对所有样本的分类都是利用最复杂的特征判断，费时费力；**加入shortcut后，相当于保留了一些简单的特征用于判断，变得省时。这一观点主要解释了为什么ResNet网络能够更快收敛。**

### Dense-Net

与ResNet想法类似的DenseNet，是**把前面每一个block输出的梯度都skip connection到后面各个block的输出**，同时在同一层增加了类似Inception的选择式结构（待研究）？？？？？，介绍了参数，进一步提升了效果，该方法论文还获得了2017CVPR best paper。

![这里写图片描述](/img/in-post/20_07/70.jpeg)

### SE-ResNet

参考：[SENet（Squeeze-and-Excitation Networks）算法笔记](https://blog.csdn.net/u014380165/article/details/78006626)

论文：Squeeze-and-Excitation Networks
论文链接：https://arxiv.org/abs/1709.01507
代码地址：https://github.com/hujie-frank/SENet
PyTorch代码地址：https://github.com/miraclewkf/SENet-PyTorch

- **创新点**

  - 卷积操作融合了空间和特征通道信息。大量工作研究了空间部分，而结构重点关注**特征通道的关系**，并提出了Squeeze-and-Excitation(SE) block，对通道间的依赖关系进行建模，自适应校准通道方面的特征响应。
  - SE block并不是一个完整的网络结构，而是一个子结构，可以嵌到其他分类或检测模型中。

- **核心思想**

  - 在于通过网络**根据loss去学习特征权重，使得有效的feature map权重大**，无效或效果小的feature map权重小的方式训练模型达到更好的结果。(感觉有点类似通道attention（🚩待研究）)

- **SE block**
  ![1558429951926](/img/in-post/20_07/1519578-20190521200354163-230761194.png)

  > $𝐹_{𝑡𝑟}$表示transformation，在文中就是一系列标准的卷积操作而已；
  >
  > $𝐹_{𝑠𝑞}$表示squeeze，**产生通道描述**； （🚩待研究）
  >
  > $𝐹_{𝑒𝑥}$表示excitation，通过参数𝑊来**建模通道的重要性**；
  >
  > $𝐹_{𝑠𝑐𝑎𝑙𝑒}$表示reweight，将excitation输出的权重逐乘以先前特征，完成特征重标定。

  - squeeze：
    ![这里写图片描述](/img/in-post/20_07/SouthEast.jpeg)
    - 公式非常简单，就是一个global average pooling。
    - 这一步的结果相当于表明该层C个feature map的数值分布情况，或者叫全局信息。
  - Excitation
    ![这里写图片描述](/img/in-post/20_07/SouthEast-20210110002122772.jpeg)
    - 直接看最后一个等号，前面squeeze得到的结果是z，这里先用W1乘以z，就是一个全连接层操作，**W1的维度是C/r \* C，这个r是一个缩放参数，在文中取的是16，这个参数的目的是为了减少channel个数从而降低计算量**。又因为z的维度是1\*1\*C，所以W1z的结果就是1\*1\*C；然后再经过一个ReLU层，输出的维度不变；然后再和W2相乘，和W2相乘也是一个全连接层的过程，**W2的维度是C\*C/r**，因此**输出的维度就是1\*1\*C**；最后再经过sigmoid函数，得到s。
    - 也就是说最后得到的这个s的维度是1\*1\*C，C表示channel数目。**这个s其实是本文的核心，它是用来刻画tensor U中C个feature map的权重。而且这个权重是通过前面这些全连接层和非线性层学习得到的，因此可以end-to-end训练。这两个全连接层的作用就是融合各通道的feature map信息，因为前面的squeeze都是在某个channel的feature map里面操作。**
  - 在得到s之后，就可以对原来的tensor U操作了，就是下面的公式4。也很简单，就是channel-wise multiplication
    ![这里写图片描述](/img/in-post/20_07/SouthEast-20210110003108591.jpeg)
  - 

- **SE-ResNet Module**

  - Figure3是在ResNet中添加SE block的情况。
    <img src="/img/in-post/20_07/1519578-20190521200353846-413277730-20210109185335768.png" alt="1558430934954" style="zoom:80%;" />



### **EfficientNet**

- 思想：
  - 对网络的扩展可以通过**增加网络层数**（depth，比如从 ResNet (He et al.)从resnet18到resnet200 ）, 
  - 也可以通过**增加宽度**，比如WideResNet (Zagoruyko & Komodakis, 2016)和MobileNets (Howard et al., 2017) 
  - 可以**扩大网络的深度 (#channels), **
  - 还有就是**更大的输入图像尺寸(resolution)**也可以帮助提高精度。 （🚩待研究）
- 创新点：
  - **复合模型扩张方法**
  - **神经结构搜索技术**
- 结构：

![img](/img/in-post/20_07/v2-fcbff6e21eb9e7f9cce94c3bd935b84a_720w.jpg)

> 如下图所示: (a)是基本模型，（b）是增加宽度，（c）是增加深度，（d）是增大输入图像分辨率，（d）是EfficientNet，它从三个维度均扩大了，但是扩大多少，就是通过作者提出来的**复合模型扩张方法**结合**神经结构搜索技术**获得的。

## 原理性知识

### 解决过拟合的方法有哪些

- 数据增强
- 模型集成

  - Dropout

    - 相当于每次都在训练不同结构的神经网络
    - 类比bagging，可被认为是一种大规模深度神经网络的模型集成方法
    - 轻量级的bagging集成近似

- 参数正则化
- **批量归一化（Batch Normalization）**
  - NN本质是学习数据分布，有两种分布不同：
    训练集和测试集数据分布不同，降低泛化能力。
    随着网络训练，隐层网络参数变化会使网络在每次迭代拟合不同的分布，增大复杂度且易过拟合
  - 数学原理：（x-均值）/ 标准差

### 解决小样本问题的方法有哪些

- 数据增强
- 迁移学习
- 无监督、半监督
- GAN网络
- 网络优化

  - 考虑**样本相关性**（待研究）
  - 多使用残差学习和**特征融合**

### BN（Batch Normalization）层的原理

BN层的**本质**：`γ`和`β`是需要学习的参数，本质是利用优化来改变方差大小和均值的位置。

> ![img](/img/in-post/20_07/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzIxOTk3NjI1,size_16,color_FFFFFF,t_70.png)

### BN层的作用

- 加快**训练速度**增大学习率（可以保证每一层的**输入分布稳定**，这可以使训练加速）
- 减小**梯度消失和梯度爆炸**
- 控制**过拟合**，可以少用或者不用dropout和正则
- **使网络对初始权重不敏感**

### 网络优化方法

- 正则化

	- 防止神经网络的过度拟合，需要对代价函数改进，即加入正则化项
		![image-20210109224137602](/img/in-post/20_07/image-20210109224137602.png)
	- **第一项是均方误差，第二项是正则化**，即权重衰减项，λ对两项的重要性进行控制。
- **动量因子**的权值调节方法
![image-20210109224336007](/img/in-post/20_07/image-20210109224336007.png)
	- 其中ｗ指代的是要更新的参数，ｚ表示迭代的次数，v表示动量，η为动量前的系数，J同前面的代价函数。

### 1X1卷积作用（🚩待研究）

### GAP层的解释

+ 原因
  + 因为全连接层参数最多，所以现在的趋势是尽量**避免全连接**，近期的大部分论文**FC多用全局平均池化层（GAP，Global Average Pooling）的方法代替。**
+ 思想
  + **用 feature map 直接表示属于某个类的 confidence map**，比如有10个类，就在**最后输出10个 feature map**，每个feature map中的值加起来求平均值，这十个数字就是对应的概率或者叫置信度。然后把得到的这些平均值直接作为属于某个类别的 confidence value，**再输入softmax中分类**， 更重要的是实验效果并不比用 FC 差。
  + 主要是从模型压缩的应用中得到的灵感。经过实验加入Gap层的效果没有下降甚至有提升, 我们分析后者的优势是：
    + 1.因为FC的参数众多，这么做就**减少了参数的数量**（在最近比较火的**模型压缩**中，这个优势可以很好的压缩模型的大小）。
    + 2.因为减少了参数的数量，可以很好的**减轻过拟合**的发生。恰恰在小样本的情况下是非常容易过拟合的. 

# 机器学习

## 特征工程相关

### 如何做特征组合

- 特征组合的思想很简单，**通过将单独的特征进行组合（相乘或求笛卡尔积）而形成的合成特征。**比如属性A有三个特征，属性B有两个特征，笛卡尔积后就有六个组合特征，**然后用one hot 或其他embedding方式给新的特征编码**。

## 线性模型

### **岭回归模型(Ridge Regression)**:

- 就是对于一个线性模型，在原来的损失函数加入参数的*l2*范数的惩罚项, 限制参数

## 决策树模型相关

### 决策树的灵魂

- 即依靠某种指标进行树的分裂达到分类/回归的目的，总是希望纯度越高越好。

### 决策树量化分类效果的方法有哪些

- 信息增益（ID3）

- 信息增益率（C4.5）
- 基尼系数（CART）

### 信息增益（ID3）的理解

- ID3算法的**核心思想**：以信息增益度量属性选择，选择分裂后**信息增益最大**的属性进行分裂。

- 为了精确地定义信息增益，我们先定义信息论中广泛使用的一个**度量标准**称为熵（entropy）

  - 它**刻画了**任意样例集的纯度（purity）。

  - 给定包含关于某个目标概念的正反样例的样例集S，那么S相对这个布尔型分类的熵为：

    <img src="/img/in-post/20_07/image-20210109214727229.png" alt="image-20210109214727229" style="zoom:67%;" />

    <img src="/img/in-post/20_07/image-20210109224025591.png" alt="image-20210109224025591" style="zoom:67%;" />

    其中，p+为正样本概率，p-为负样本概率。

### 决策树的类型

- **分类树**分析是指预测结果是数据所属的类（比如某个电影去看还是不看）
- **回归树**分析是指预测结果可以被认为是实数（例如房屋的价格，或患者在医院中的逗留时间）
- **分类回归树**（CART，Classification And Regression Tree）分析是用于指代上述两种树的总称
- 分类的目标是根据已知样本的某些特征，判断一个新的样本属于哪种已知的样本类，它的结果是**离散值**。而回归的结果是**连续的值**。当然，**本质是一样的，都是特征（feature）到结果/标签（label）之间的映射。**

## 集成学习相关

### 集成学习的定义

- 集成学习是指**用某种策略将多个分类器预测的结果集成起来**，作为最终的预测结果

### 集成学习的分类

- Boosting方法（串行，分类器互相依赖）

- bagging方法（可并行，分类器无依赖）


### Boosting方法的思想和作用

- **基本思想：**
  - 训练时，将基分类器层层叠加，每一层训练时对**前一层错分样本给予更高的权重**，分类**错误率低的基分类器给予更高的权重**。
  - 测试时，根据各层分类器的**结果加权**得到最终结果。
- **主要作用**：
  - 减少集成分类器的**偏差**（基分类器层层叠加）

### Boosting方法的例子

1. AdaBoost(自适应增强)

   - 基本思想

     - **前一个分类器**分错/分对的样本的权值会得到加强/降低，**加权后的全体样本再次被用来训练下一个基本分类器**

2. GBDT(梯度提升决策树)

   - 基本思想

     - GBDT的**每一次计算都为了减少上一次的残差**，进而在负梯度的方向上建立一个新的模型
     - 根据**当前模型损失函数的负梯度信息**来训练新加入的弱分类器，然后把训练好的弱分类器以累加的形式结合到现有模型里

   - 例子——XGBoost

     - 目标函数

       - 目标函数分为两个部分：**误差函数**(logistic损失函数、平方损失函数)和**正则化项（**定义模型的复杂度）
       - 将目标函数化简之后，目标函数只依赖于一阶导数g和二阶导数h
       - 将目标函数和正则化项结合化简，对w进行求导，求出最优w，代入目标函数中

### bagging方法的思想和作用

- **基本思想**
  - 将训练集分为若干子集，分别训练各个基分类器
  - 测试时，每个基分类器单独做出判断，通过投票方式做出最终决策

- 作用
  - 减少集成分类器的**方差**（基分类器并行，根据统计学）（待研究）

### **bagging方法的例子**

- 随机森林

### XGBoost问题

- 与GBDT不同之处？

  - 用泰勒展开近似目标函数

- XGBoost为什么使用泰勒二阶展开？为什么用二阶信息不用一阶？
- XGBoost在什么地方做的剪枝，怎么做的？
- XGBoost如何分布式？特征分布式和数据分布式？ 各有什么存在的问题？
- XGBoost里处理缺失值的方法？
- XGBoost有那些优化？
- xgboost对预测模型特征重要性排序的原理？
- XGBoost如何寻找最优特征？是又放回还是无放回的呢？
- GBDT和XGBoost的区别是什么？
- lightgbm和xgboost有什么区别？他们的loss一样么？ 算法层面有什么区别？

### **LightGBM**

​    LightGBM 是一个**梯度 boosting 框架**，使用**基于学习算法的决策树**。它可以说是分布式的，高效的，有以下优势：更快的训练效率、低内存使用、更高的准确率、支持并行化学习、可处理大规模数据。

## sklearn库相关

### SVM、SVR、SVC区别

- SVM=Support Vector Machine 是支持向量
- SVR=Support Vector Classification就是支持向量机用于分类，
- SVC=Support Vector Regression. 就是支持向量机用于回归分析

> 算法（python-sklearn）-- SVM的几种模型
>
> - svm.LinearSVC Linear Support Vector Classification.
> - svm.LinearSVR Linear Support Vector Regression.
> - svm.NuSVC Nu-Support Vector Classification.
> - svm.NuSVR Nu Support Vector Regression.
> - svm.OneClassSVM Unsupervised Outlier Detection.
> - svm.SVC C-Support Vector Classification.
>
> 	- sklearn系列之 sklearn.svm.SVC详解
> - svm.SVR Epsilon-Support Vector Regression.

## 计算智能方法

### 启发式搜索算法总结

> 启发式搜索算法蕴含着许多人生哲学，它虽不是数学方法，其思想更类似于人类解决问题的思想和一些人生中总结的道理，值得好好体会。最后用网上一段描述各种搜索算法的例子来作为总结：
>
> 为了找出地球上最高的山，一群有志气的兔子们开始想办法。
> （1）兔子朝着比现在高的地方跳去。他们找到了不远处的最高山峰。但是这座山不一定是珠穆朗玛峰。这就是爬山法，它不能保证局部最优值就是全局最优值。
> （2）兔子喝醉了。他随机地跳了很长时间。这期间，它可能走向高处，也可能踏入平地。但是，他渐渐清醒了并朝他踏过的最高方向跳去。这就是模拟退火。
> （3）兔子们知道一个兔的力量是渺小的。他们互相转告着，哪里的山已经找过，并且找过的每一座山他们都留下一只兔子做记号。他们制定了下一步去哪里寻找的策略。这就是禁忌搜索。
> （4）兔子们吃了失忆药片，并被发射到太空，然后随机落到了地球上的某些地方。他们不知道自己的使命是什么。但是，如果你过几年就杀死一部分海拔低的兔子，多产的兔子们自己就会找到珠穆朗玛峰。这就是遗传算法。

# 项目、比赛track解释

## 神经网络

### TTA:

- 就是在测试的时候有一个transform，然后训练的时候使用多个transform，然后得到多个结果，然后让去平均值
- 增加鲁棒性,防止过拟合,避免原始图像显示的区域可能缺少某些特征,
- 包括**不同区域裁剪**和**更改缩放程度**等,提高了结果的稳定性和精准度.

### **标签平滑:**

- 使用原因:
  - 0/1这种 binary 或者说“hard”的答案, 容易导致网络对自己的预测过于自信,导致过拟合
  - 实际上类似的很多大型训练数据集中往往就会有错误标签, 要求网络对正确答案具有一定的怀疑能力, 以减少一定程度上围绕错误答案的极端情况下的建模。
- 使用组归一化（GroupNormalization）代替批量归一化（batch_normalization）
  - 解决当Batch_size过小导致的准确率下降

### **余弦退火**

- 在采用批次随机梯度下降算法时，神经网络应该越来越接近Loss值的全局最小值。当它逐渐接近这个最小值时，学习率应该变得更小来使得模型不会超调且尽可能接近这一点。余弦退火（Cosine annealing）利用余弦函数来降低学习率，进而解决这个问题，如下图所示
- 余弦退火方法的目的在于**逃离当前局部最优点**，寻找新的局部最优点。在每个周期计算完成后，保存不同局部最优点的模型参数。

<img src="/img/in-post/20_07/cosine_annealing_restarts.png" alt="img" style="zoom:50%;" />

# 参考

## 深度网络参考

[【计算机视觉算法岗面经】“吐血”整理：2019秋招资料](https://blog.csdn.net/liuxiao214/article/details/83043170)

[【计算机视觉算法岗面经】“吐血”整理：2019秋招面经](https://blog.csdn.net/liuxiao214/article/details/83043197)

[图像分类丨ILSVRC历届冠军网络「从AlexNet到SENet」](https://blog.csdn.net/woshicver/article/details/105140874)

## 面经

[深度学习分类模型面试题-面经（一）](https://blog.csdn.net/qq_21997625/article/details/105977807)

[腾讯NLP算法岗面经（offer已拿）](https://zhuanlan.zhihu.com/p/117450353)











