---
layout: post
comments: true
mathjax: false
title: "强化学习"
subtitle: 'q-learning'
author: "Sun"
header-style: text
tags:
  - RL/Reforce Learning
  - summary
  - 
---

# Q-Learning-走迷宫实例

**目标**是找到一条没有炸弹的路径，以最快的速度从起始状态到达目标状态。

**Q-Learning** 就是要学习在一个给定的 state 时，采取了一个特定的行动后，能得到的奖励是什么。

**Q表更新**

![image-20200528183051364](/img/in-post/20_03/image-20200528183051364.png)



![image-20200528183210453](/img/in-post/20_03/image-20200528183210453.png)

> 其中，
>  S 代表当前的状态，a 代表当前状态所采取的行动，
>  S’ 代表这个行动所引起的下一个状态，a’ 是这个新状态时采取的行动，
>  r 代表采取这个行动所得到的奖励 reward，γ 是 discount 因子，
>
> 由公式可以看出 s，a 对的 Q 值等于 即时奖励 + 未来奖励的 discount。
>  γ 决定了未来奖励的重要性有多大，
>  比如说，我们到了一个状态，它虽然离目标状态远了一些，但是却离炸弹远了一些，那这个状态的即时奖励就很小，但是未来奖励就很多。



**算法是：**

1. 初始化 Q table 为 0 
   1. 每一次遍历，随机选择一个**状态**作为起点
   2. 在当前状态 (S) 的**所有可选的行动中**选择一个 (a)
   3. 移动到**下一个状态** (S’)
   4. 在新状态上**选择 <u>Q 值最大</u>的那个行动** (a’)
   5. 用 Bellman Equation **更新  Q-table**
   6. 将**新状态设置为当前状态**重复第 1～5 步
2. 如果已经到了目标状态就结束

![image-20200528183437304](/img/in-post/20_03/image-20200528183437304.png)

