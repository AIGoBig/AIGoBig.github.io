<h1 id="introduction">Introduction</h1>
<h2 id="why-pytorch">Why pytorch？</h2>
<h3 id="更加python化更易于使用">更加“Python化”，更易于使用</h3>
<p>每一个Pytorch示例（CV和NLP）都有共同的结构：</p>
<pre class="shell"><code>data/
experiments/
model/
    net.py：指定神经网络架构、损失函数和评估指标。
    data_loader.py：指定数据应如何馈送到网络。
train.py：包含主训练循环。
evaluate.py：包含用于评估模型的主循环。
search_hyperparams.py
synthesize_results.py
evaluate.py
utils.py：用于处理超参数 / 日志 / 存储模型的实用功能。</code></pre>
<h3 id="有用的库">有用的库</h3>
<p>下面是一个综合列表，列出了计算机视觉、自然语言处理和生成库等不同领域的一些项目：</p>
<ul>
<li>pro_gan_pytorch：利用 ProGAN 功能。</li>
<li>BoTorch：使用<a href="https://www.infoq.cn/article/2014/07/programming-language-bayes">贝叶斯</a>优化。</li>
<li>ParlAI：用于共享、训练和测试对话模型。</li>
<li>OpenNMT-py：用于实现神经机器翻译系统。</li>
<li>MUSE：用于多语言词嵌入。</li>
<li><strong>skorch：用于将 scikit-learn 代码与 PyTorch 融合。</strong></li>
</ul>
<h3 id="易于实现数据并行">易于实现数据并行</h3>
<p>Distributed Data-Parallel（分布式数据并行）是 PyTorch 的一项特性，你可以将其与 Data-Parallel（数据并行）结合使用来处理需要大型数据集和模型的用例。</p>
<p>为实现数据并行，使用了<code>torch.nn.DataParallel</code>类</p>
<h3 id="增加了对移动设备的支持">增加了对移动设备的支持</h3>
<p>如Android和IOS设备部署的支持</p>
<p><strong>PyTorch Mobile 入门：</strong></p>
<ul>
<li>Android：<a href="https://pytorch.org/mobile/android" class="uri">https://pytorch.org/mobile/android</a></li>
<li>iOS：<a href="https://pytorch.org/mobile/ios" class="uri">https://pytorch.org/mobile/ios</a></li>
</ul>
<h3 id="易于调试">易于调试</h3>
<p>pytorch的动态计算图允许在代码执行时进行动态修改和快速调试</p>
<h1 id="使用方式">使用方式</h1>
<h2 id="torch-与-numpy">torch 与 numpy</h2>
<p>互相转换</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb2-1" data-line-number="1">np_data <span class="op">=</span> np.arange(<span class="dv">6</span>).reshape((<span class="dv">2</span>, <span class="dv">3</span>))</a>
<a class="sourceLine" id="cb2-2" data-line-number="2">torch_data <span class="op">=</span> torch.from_numpy(np_data)</a>
<a class="sourceLine" id="cb2-3" data-line-number="3">tensor2array <span class="op">=</span> torch_data.numpy()</a></code></pre></div>
<p>运算形式</p>
<ol type="1">
<li>注意torch中的数据形式是<code>Tensor</code></li>
</ol>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb3-1" data-line-number="1"><span class="co"># abs 绝对值计算</span></a>
<a class="sourceLine" id="cb3-2" data-line-number="2">data <span class="op">=</span> [<span class="op">-</span><span class="dv">1</span>, <span class="dv">-2</span>, <span class="dv">1</span>, <span class="dv">2</span>]</a>
<a class="sourceLine" id="cb3-3" data-line-number="3">tensor <span class="op">=</span> torch.FloatTensor(data)  <span class="co"># 转换成32位浮点 tensor</span></a>
<a class="sourceLine" id="cb3-4" data-line-number="4"><span class="bu">print</span>(</a>
<a class="sourceLine" id="cb3-5" data-line-number="5">    <span class="st">&#39;</span><span class="ch">\n</span><span class="st">abs&#39;</span>,</a>
<a class="sourceLine" id="cb3-6" data-line-number="6">    <span class="st">&#39;</span><span class="ch">\n</span><span class="st">numpy: &#39;</span>, np.<span class="bu">abs</span>(data),          <span class="co"># [1 2 1 2]</span></a>
<a class="sourceLine" id="cb3-7" data-line-number="7">    <span class="st">&#39;</span><span class="ch">\n</span><span class="st">torch: &#39;</span>, torch.<span class="bu">abs</span>(tensor)      <span class="co"># [1 2 1 2]</span></a>
<a class="sourceLine" id="cb3-8" data-line-number="8">)</a></code></pre></div>
<ol start="2" type="1">
<li>Autograd automatically supports Tensors with requires_grad set to True. 计算图可以通过链式法则求导。如果<code>variables</code>中的任何一个<code>variable</code>是 非标量(<code>non-scalar</code>)的，且<code>requires_grad=True</code>。</li>
</ol>
<h1 id="逻辑回归程序关系拟合">逻辑回归程序（关系拟合）</h1>
<ol type="1">
<li><p>Class里，<code>__init__()</code>是定义各层，<code>forward()</code>才是真正搭建网络</p></li>
<li><p><code>unsqueeze(1)</code>的使用是因为将数据从1维变2维，因为pytorch只能处理带patch的二维数据</p></li>
<li><p><code>hidden</code>是<code>Linear()</code>的一个实例，后面是它这一层的内容, 故可以对其传入参数</p></li>
<li><p>训练前对优化器和损失函数进行选择</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb4-1" data-line-number="1">optimizer <span class="op">=</span> t.optim.SGD(net.parameters(), lr<span class="op">=</span><span class="fl">0.2</span>)  <span class="co"># 传入net的参数 和 lr</span></a>
<a class="sourceLine" id="cb4-2" data-line-number="2">loss_func <span class="op">=</span> t.nn.MSELoss()      <span class="co"># 回归问题用均方差误差即可</span></a></code></pre></div></li>
<li><p>优化过程</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb5-1" data-line-number="1">    optimizer.zero_grad()   <span class="co"># 清空上一次计算的梯度，初始化为0.  相当于d_weights = [0] * n</span></a>
<a class="sourceLine" id="cb5-2" data-line-number="2">    loss.backward()         <span class="co"># 误差反向传播，计算梯度作为参数更新值.  相当于 w.grad = ▽loss/▽w</span></a>
<a class="sourceLine" id="cb5-3" data-line-number="3">    optimizer.step()        <span class="co"># 通过step()进行单次优化.  相当于 w = w - lr*w.grad b = b - lr*b.grad</span></a></code></pre></div>
<ol type="1">
<li><p>为什么要用<code>optimizer.zero_grad()</code>把梯度清零?</p>
<ol type="1">
<li>进来一个batch的数据, 就会计算一次梯度, 更新一次网络.</li>
<li>可以实现梯度累加等Trick: https://www.zhihu.com/question/303070254</li>
</ol></li>
<li><p><strong>每个batch的操作与基础代码对应关系</strong> pytorch中:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb6-1" data-line-number="1">        <span class="co"># zero the parameter gradients</span></a>
<a class="sourceLine" id="cb6-2" data-line-number="2">        optimizer.zero_grad()</a>
<a class="sourceLine" id="cb6-3" data-line-number="3">        <span class="co"># forward + backward + optimize</span></a>
<a class="sourceLine" id="cb6-4" data-line-number="4">        outputs <span class="op">=</span> net(inputs)</a>
<a class="sourceLine" id="cb6-5" data-line-number="5">        loss <span class="op">=</span> criterion(outputs, labels)</a>
<a class="sourceLine" id="cb6-6" data-line-number="6">        loss.backward()</a>
<a class="sourceLine" id="cb6-7" data-line-number="7">        optimizer.step()</a></code></pre></div>
<p>基础实现:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb7-1" data-line-number="1">    <span class="co"># gradient descent</span></a>
<a class="sourceLine" id="cb7-2" data-line-number="2">    weights <span class="op">=</span> [<span class="dv">0</span>] <span class="op">*</span> n</a>
<a class="sourceLine" id="cb7-3" data-line-number="3">    alpha <span class="op">=</span> <span class="fl">0.0001</span></a>
<a class="sourceLine" id="cb7-4" data-line-number="4">    max_Iter <span class="op">=</span> <span class="dv">50000</span></a>
<a class="sourceLine" id="cb7-5" data-line-number="5">    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(max_Iter):</a>
<a class="sourceLine" id="cb7-6" data-line-number="6">        loss <span class="op">=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb7-7" data-line-number="7">        <span class="co"># optimizer.zero_grad()</span></a>
<a class="sourceLine" id="cb7-8" data-line-number="8">        d_weights <span class="op">=</span> [<span class="dv">0</span>] <span class="op">*</span> n</a>
<a class="sourceLine" id="cb7-9" data-line-number="9">        <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(m):</a>
<a class="sourceLine" id="cb7-10" data-line-number="10">           <span class="co">#  outputs = net(inputs)</span></a>
<a class="sourceLine" id="cb7-11" data-line-number="11">            h <span class="op">=</span> dot(<span class="bu">input</span>[k], weights)</a>
<a class="sourceLine" id="cb7-12" data-line-number="12">            <span class="co"># loss.backward()</span></a>
<a class="sourceLine" id="cb7-13" data-line-number="13">            d_weights <span class="op">=</span> [d_weights[j] <span class="op">+</span> (label[k] <span class="op">-</span> h) <span class="op">*</span> <span class="bu">input</span>[k][j] <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(n)] </a>
<a class="sourceLine" id="cb7-14" data-line-number="14">            <span class="co"># loss = criterion(outputs, labels)</span></a>
<a class="sourceLine" id="cb7-15" data-line-number="15">            loss <span class="op">+=</span> (label[k] <span class="op">-</span> h) <span class="op">*</span> (label[k] <span class="op">-</span> h) <span class="op">/</span> <span class="dv">2</span></a>
<a class="sourceLine" id="cb7-16" data-line-number="16">        d_weights <span class="op">=</span> [d_weights[k]<span class="op">/</span>m <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(n)]</a>
<a class="sourceLine" id="cb7-17" data-line-number="17">        <span class="co"># optimizer.step()</span></a>
<a class="sourceLine" id="cb7-18" data-line-number="18">        weights <span class="op">=</span> [weights[k] <span class="op">+</span> alpha <span class="op">*</span> d_weights[k] <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(n)]</a>
<a class="sourceLine" id="cb7-19" data-line-number="19">        <span class="cf">if</span> i<span class="op">%</span><span class="dv">10000</span> <span class="op">==</span> <span class="dv">0</span>:</a>
<a class="sourceLine" id="cb7-20" data-line-number="20">            <span class="bu">print</span> <span class="st">&quot;Iteration </span><span class="sc">%d</span><span class="st"> loss: </span><span class="sc">%f</span><span class="st">&quot;</span><span class="op">%</span>(i, loss<span class="op">/</span>m)</a>
<a class="sourceLine" id="cb7-21" data-line-number="21">            <span class="bu">print</span> weight</a></code></pre></div>
<ol type="1">
<li><p><strong>optimizer.zero_grad()对应d_weights = [0] * n</strong> 梯度初始化为零, 一个batch的loss关于weight的导数是所有sample的loss关于weight的导数的累加和</p></li>
<li><p><strong>outputs = net(inputs)对应h = dot(input[k], weights)</strong></p>
<p>即前向传播求出预测的值</p></li>
<li><p><strong>loss = criterion(outputs, labels)对应loss += (label[k] - h) * (label[k] - h) / 2</strong></p>
<p>这一步很明显，就是求loss（其实我觉得这一步不用也可以，反向传播时用不到loss值，只是为了让我们知道当前的loss是多少）</p></li>
<li><p><strong>loss.backward()对应d_weights = [d_weights[j] + (label[k] - h) * input[k][j] for j in range(n)]</strong></p>
<p>即反向传播求梯度</p></li>
<li><p><strong>optimizer.step()对应weights = [weights[k] + alpha * d_weights[k] for k in range(n)]</strong></p>
<p>即更新所有参数</p></li>
</ol></li>
</ol></li>
<li><p>使用<code>imageio</code>进行GIF绘制。</p></li>
</ol>
<figure>
<img src="/img/in-post/20_07/LR_fitting.gif" alt="LR_fitting" /><figcaption>LR_fitting</figcaption>
</figure>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb8-1" data-line-number="1"><span class="im">import</span> numpy <span class="im">as</span> np</a>
<a class="sourceLine" id="cb8-2" data-line-number="2"></a>
<a class="sourceLine" id="cb8-3" data-line-number="3"><span class="im">import</span> torch.nn <span class="im">as</span> nn</a>
<a class="sourceLine" id="cb8-4" data-line-number="4"><span class="im">import</span> os</a>
<a class="sourceLine" id="cb8-5" data-line-number="5"><span class="im">from</span> tqdm <span class="im">import</span> tqdm</a>
<a class="sourceLine" id="cb8-6" data-line-number="6">os.environ[<span class="st">&#39;KMP_DUPLICATE_LIB_OK&#39;</span>]<span class="op">=</span><span class="st">&#39;True&#39;</span>       <span class="co"># macOS系统原因需要加上这一句可正常运行</span></a>
<a class="sourceLine" id="cb8-7" data-line-number="7"></a>
<a class="sourceLine" id="cb8-8" data-line-number="8"><span class="co">&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb8-9" data-line-number="9"><span class="co">建立数据集   </span></a>
<a class="sourceLine" id="cb8-10" data-line-number="10"><span class="co">&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb8-11" data-line-number="11"><span class="im">import</span> torch <span class="im">as</span> t</a>
<a class="sourceLine" id="cb8-12" data-line-number="12"><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</a>
<a class="sourceLine" id="cb8-13" data-line-number="13"><span class="im">import</span> imageio</a>
<a class="sourceLine" id="cb8-14" data-line-number="14"><span class="im">from</span> matplotlib.animation <span class="im">import</span> FuncAnimation</a>
<a class="sourceLine" id="cb8-15" data-line-number="15">plt.style.use(<span class="st">&#39;seaborn&#39;</span>)  <span class="co"># 设置使用的样式</span></a>
<a class="sourceLine" id="cb8-16" data-line-number="16"><span class="im">import</span> seaborn</a>
<a class="sourceLine" id="cb8-17" data-line-number="17"><span class="co"># y = a * x^2 + b</span></a>
<a class="sourceLine" id="cb8-18" data-line-number="18"><span class="co"># shape=(100, 1),1维变2维，因为pytorch只能处理带patch的二维数据</span></a>
<a class="sourceLine" id="cb8-19" data-line-number="19">x <span class="op">=</span> t.linspace(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">100</span>).unsqueeze(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb8-20" data-line-number="20">y  <span class="op">=</span> x.<span class="bu">pow</span>(<span class="dv">2</span>) <span class="op">+</span> <span class="fl">0.1</span> <span class="op">*</span> t.randn(x.size())</a>
<a class="sourceLine" id="cb8-21" data-line-number="21">x.sort()</a>
<a class="sourceLine" id="cb8-22" data-line-number="22">plt.scatter(x, y)</a>
<a class="sourceLine" id="cb8-23" data-line-number="23">plt.show()</a>
<a class="sourceLine" id="cb8-24" data-line-number="24"></a>
<a class="sourceLine" id="cb8-25" data-line-number="25"><span class="co">&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb8-26" data-line-number="26"><span class="co">建立神经网络</span></a>
<a class="sourceLine" id="cb8-27" data-line-number="27"><span class="co">&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb8-28" data-line-number="28"><span class="im">import</span> torch.nn.functional <span class="im">as</span> F  <span class="co"># 激励函数</span></a>
<a class="sourceLine" id="cb8-29" data-line-number="29"><span class="kw">class</span> Net(t.nn.Module):</a>
<a class="sourceLine" id="cb8-30" data-line-number="30">    <span class="co">&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb8-31" data-line-number="31"><span class="co">    __init__()是定义各层</span></a>
<a class="sourceLine" id="cb8-32" data-line-number="32"><span class="co">    forward()才是真正搭建网络</span></a>
<a class="sourceLine" id="cb8-33" data-line-number="33"><span class="co">    &quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb8-34" data-line-number="34">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_feature, n_hidden, n_output):</a>
<a class="sourceLine" id="cb8-35" data-line-number="35">        <span class="bu">super</span>(Net, <span class="va">self</span>).<span class="fu">__init__</span>()</a>
<a class="sourceLine" id="cb8-36" data-line-number="36">        <span class="co"># 其实每个层就是一个带参数的广义函数映射</span></a>
<a class="sourceLine" id="cb8-37" data-line-number="37">        <span class="va">self</span>.hidden <span class="op">=</span> nn.Linear(n_feature, n_hidden)  <span class="co"># hidden是属性，后面是它这一层的内容</span></a>
<a class="sourceLine" id="cb8-38" data-line-number="38">        <span class="va">self</span>.output <span class="op">=</span> nn.Linear(n_hidden, n_output)</a>
<a class="sourceLine" id="cb8-39" data-line-number="39"></a>
<a class="sourceLine" id="cb8-40" data-line-number="40">    <span class="kw">def</span> forward(<span class="va">self</span>, x):  <span class="co"># module中的forward功能</span></a>
<a class="sourceLine" id="cb8-41" data-line-number="41">        <span class="co"># 正向传播初入值， 用神经网络分析出输出值</span></a>
<a class="sourceLine" id="cb8-42" data-line-number="42">        <span class="co"># hidden获得的值是torch.nn.Linear的一个实例，然后Linear又是Module的子类，</span></a>
<a class="sourceLine" id="cb8-43" data-line-number="43">        <span class="co"># 所以hidden相当于is a Module，Module中实现了__call__()方法，Module类型的所有实例都是可以被当成一个方法来调用的。</span></a>
<a class="sourceLine" id="cb8-44" data-line-number="44">        x <span class="op">=</span> F.relu(<span class="va">self</span>.hidden(x))</a>
<a class="sourceLine" id="cb8-45" data-line-number="45">        x <span class="op">=</span> <span class="va">self</span>.output(x)</a>
<a class="sourceLine" id="cb8-46" data-line-number="46">        <span class="cf">return</span> x</a>
<a class="sourceLine" id="cb8-47" data-line-number="47"></a>
<a class="sourceLine" id="cb8-48" data-line-number="48"></a>
<a class="sourceLine" id="cb8-49" data-line-number="49">net <span class="op">=</span> Net(<span class="dv">1</span>, <span class="dv">10</span>, <span class="dv">1</span>)  <span class="co"># 输入x，隐藏单元10个，输出y</span></a>
<a class="sourceLine" id="cb8-50" data-line-number="50"><span class="bu">print</span>(net)  <span class="co"># 可以打印网络结构</span></a>
<a class="sourceLine" id="cb8-51" data-line-number="51"></a>
<a class="sourceLine" id="cb8-52" data-line-number="52"><span class="co">&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb8-53" data-line-number="53"><span class="co">训练网络</span></a>
<a class="sourceLine" id="cb8-54" data-line-number="54"><span class="co">&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb8-55" data-line-number="55">optimizer <span class="op">=</span> t.optim.SGD(net.parameters(), lr<span class="op">=</span><span class="fl">0.2</span>)  <span class="co"># 传入net的参数 和 lr</span></a>
<a class="sourceLine" id="cb8-56" data-line-number="56">loss_func <span class="op">=</span> t.nn.MSELoss()      <span class="co"># 回归问题用均方差误差即可</span></a>
<a class="sourceLine" id="cb8-57" data-line-number="57"></a>
<a class="sourceLine" id="cb8-58" data-line-number="58"><span class="co"># plt.ion()   # 设置为实时打印的画图</span></a>
<a class="sourceLine" id="cb8-59" data-line-number="59"></a>
<a class="sourceLine" id="cb8-60" data-line-number="60">image_list <span class="op">=</span> []</a>
<a class="sourceLine" id="cb8-61" data-line-number="61"><span class="cf">for</span> _ <span class="kw">in</span> tqdm(<span class="bu">range</span>(<span class="dv">200</span>)):</a>
<a class="sourceLine" id="cb8-62" data-line-number="62">    prediction <span class="op">=</span> net(x)</a>
<a class="sourceLine" id="cb8-63" data-line-number="63"></a>
<a class="sourceLine" id="cb8-64" data-line-number="64">    loss <span class="op">=</span> loss_func(prediction, y)  <span class="co"># 计算误差</span></a>
<a class="sourceLine" id="cb8-65" data-line-number="65"></a>
<a class="sourceLine" id="cb8-66" data-line-number="66">    <span class="co">&quot;&quot;&quot;优化&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb8-67" data-line-number="67">    optimizer.zero_grad()   <span class="co"># 清空上一步残余更新参数值，要把梯度清零</span></a>
<a class="sourceLine" id="cb8-68" data-line-number="68">    loss.backward()         <span class="co"># 误差反向传播，计算梯度作为参数更新值</span></a>
<a class="sourceLine" id="cb8-69" data-line-number="69">    optimizer.step()        <span class="co"># step()进行单次优化，将参数重新施加到net的parameters上</span></a>
<a class="sourceLine" id="cb8-70" data-line-number="70"></a>
<a class="sourceLine" id="cb8-71" data-line-number="71">    <span class="co">&quot;&quot;&quot;可视化&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb8-72" data-line-number="72">    <span class="cf">if</span> _ <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">0</span>:</a>
<a class="sourceLine" id="cb8-73" data-line-number="73">        <span class="co"># plot and show learning process</span></a>
<a class="sourceLine" id="cb8-74" data-line-number="74">        plt.cla()</a>
<a class="sourceLine" id="cb8-75" data-line-number="75">        plt.scatter(x.data.numpy(), y.data.numpy())</a>
<a class="sourceLine" id="cb8-76" data-line-number="76">        line <span class="op">=</span>plt.plot(x.data.numpy(), prediction.data.numpy(), <span class="st">&#39;r-&#39;</span>,  lw<span class="op">=</span><span class="dv">5</span>)</a>
<a class="sourceLine" id="cb8-77" data-line-number="77">        plt.text(<span class="fl">0.5</span>, <span class="dv">0</span>, <span class="st">&#39;Loss=</span><span class="sc">%.4f</span><span class="st">&#39;</span> <span class="op">%</span> loss.data.numpy(), fontdict<span class="op">=</span>{<span class="st">&#39;size&#39;</span>: <span class="dv">20</span>, <span class="st">&#39;color&#39;</span>:  <span class="st">&#39;red&#39;</span>})</a>
<a class="sourceLine" id="cb8-78" data-line-number="78"></a>
<a class="sourceLine" id="cb8-79" data-line-number="79">        <span class="co"># 生成动态图</span></a>
<a class="sourceLine" id="cb8-80" data-line-number="80">        plt.savefig(<span class="st">&quot;temp.jpg&quot;</span>)</a>
<a class="sourceLine" id="cb8-81" data-line-number="81">        plt.pause(<span class="fl">0.1</span>)</a>
<a class="sourceLine" id="cb8-82" data-line-number="82">        image_list.append(imageio.imread(<span class="st">&quot;temp.jpg&quot;</span>))  <span class="co"># 可以不用循环i，直接用列表形式</span></a>
<a class="sourceLine" id="cb8-83" data-line-number="83">        <span class="co"># print(image_list)</span></a>
<a class="sourceLine" id="cb8-84" data-line-number="84">        <span class="co"># duration 是图像间的间隔</span></a>
<a class="sourceLine" id="cb8-85" data-line-number="85">imageio.mimsave(<span class="st">&#39;LR_fitting.gif&#39;</span>, image_list,<span class="st">&#39;GIF&#39;</span>, duration<span class="op">=</span><span class="fl">0.2</span>)</a>
<a class="sourceLine" id="cb8-86" data-line-number="86"></a>
<a class="sourceLine" id="cb8-87" data-line-number="87"><span class="co"># plt.ioff()</span></a>
<a class="sourceLine" id="cb8-88" data-line-number="88"><span class="co"># plt.show()</span></a></code></pre></div>
<h1 id="分类程序区分类型">分类程序（区分类型）</h1>
<ol type="1">
<li><p>注意其与回归网络的不同之处：</p>
<ol type="1">
<li>网络输入输出节点数不同</li>
</ol>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb9-1" data-line-number="1">net <span class="op">=</span> Net(<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">2</span>)  <span class="co"># 输入输出类别需要因任务而异</span></a></code></pre></div>
<ol start="2" type="1">
<li><p>分类问题使用交叉熵损失函数, 回归问题使用MSE</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb10-1" data-line-number="1">loss_func <span class="op">=</span> t.nn.CrossEntropyLoss()      <span class="co"># 注意多分类用CrossEntropyLoss  [0, 0, 1] -&gt; [0.2, 0.1, 0.7]</span></a></code></pre></div></li>
<li><p>经过一个F.softmax()才是概率值, 取最大作为预测输出</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb11-1" data-line-number="1">prediction <span class="op">=</span> t.<span class="bu">max</span>(F.softmax(out), <span class="dv">1</span>)[<span class="dv">1</span>]    <span class="co"># max()[1]取索引(label)</span></a></code></pre></div></li>
</ol></li>
<li><p><code>torch.normal(_mean_, _std_, _*_, _generator=None_, _out=None_)</code> → <code>Tensor</code></p>
<p>返回从单独的<strong>正态分布</strong>中得出均值和标准差的随机数张量.</p>
<p><a href="https://s0pytorch0org.icopy.site/docs/master/generated/torch.mean.html#torch.mean"><code>mean</code></a>是具有每个输出元素的正态分布平均值的张量</p>
<p><a href="https://s0pytorch0org.icopy.site/docs/master/generated/torch.std.html#torch.std"><code>std</code></a>是一个张量，每个输出元素的正态分布的标准偏差</p>
<p><a href="https://s0pytorch0org.icopy.site/docs/master/generated/torch.mean.html#torch.mean"><code>mean</code></a>和<a href="https://s0pytorch0org.icopy.site/docs/master/generated/torch.std.html#torch.std"><code>std</code></a>的形状不需要匹配，但是每个张量中元素的总数必须相同.</p>
<p>Example:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb12-1" data-line-number="1"><span class="op">&gt;&gt;&gt;</span> torch.normal(mean<span class="op">=</span>torch.arange(<span class="fl">1.</span>, <span class="fl">11.</span>), </a>
<a class="sourceLine" id="cb12-2" data-line-number="2">                 std<span class="op">=</span>torch.arange(<span class="dv">1</span>, <span class="dv">0</span>, <span class="fl">-0.1</span>))</a>
<a class="sourceLine" id="cb12-3" data-line-number="3"></a>
<a class="sourceLine" id="cb12-4" data-line-number="4">tensor([ <span class="fl">1.0425</span>, <span class="fl">3.5672</span>, <span class="fl">2.7969</span>, <span class="fl">4.2925</span>, <span class="fl">4.7229</span>, <span class="fl">6.2134</span>,  <span class="fl">8.0505</span>, <span class="fl">8.1408</span>, <span class="fl">9.0563</span>, <span class="fl">10.0566</span>])</a></code></pre></div></li>
<li><p><code>torch.cat()</code>合并数据</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb13-1" data-line-number="1"><span class="co"># 注意 x, y 数据的数据形式是一定要像下面一样 (torch.cat 是在合并数据)</span></a>
<a class="sourceLine" id="cb13-2" data-line-number="2"><span class="co"># 合并x当做数据， y当做标签，注意标签的类型应该是LongTensor</span></a>
<a class="sourceLine" id="cb13-3" data-line-number="3">x <span class="op">=</span> t.cat((x0, x1), <span class="dv">0</span>).<span class="bu">type</span>(t.FloatTensor)  <span class="co"># FloatTensor = 32-bit floating</span></a>
<a class="sourceLine" id="cb13-4" data-line-number="4">y <span class="op">=</span> t.cat((y0, y1), ).<span class="bu">type</span>(t.LongTensor)    <span class="co"># LongTensor = 64-bit integer</span></a></code></pre></div></li>
<li><p>根据<code>c</code>的标签不同颜色染色</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb14-1" data-line-number="1">plt.scatter(x.data.numpy()[:, <span class="dv">0</span>], x.data.numpy()[:, <span class="dv">1</span>], c<span class="op">=</span>y.data.numpy(), s<span class="op">=</span><span class="dv">100</span>, lw<span class="op">=</span><span class="dv">0</span>, cmap<span class="op">=</span><span class="st">&#39;RdYlGn&#39;</span>)</a></code></pre></div></li>
</ol>
<figure>
<img src="/img/in-post/20_07/Classification-6439734.gif" alt="Classification" /><figcaption>Classification</figcaption>
</figure>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb15-1" data-line-number="1"><span class="im">import</span> torch.nn <span class="im">as</span> nn</a>
<a class="sourceLine" id="cb15-2" data-line-number="2"><span class="im">import</span> os</a>
<a class="sourceLine" id="cb15-3" data-line-number="3"><span class="im">from</span> tqdm <span class="im">import</span> tqdm</a>
<a class="sourceLine" id="cb15-4" data-line-number="4">os.environ[<span class="st">&#39;KMP_DUPLICATE_LIB_OK&#39;</span>]<span class="op">=</span><span class="st">&#39;True&#39;</span></a>
<a class="sourceLine" id="cb15-5" data-line-number="5"><span class="im">import</span> torch <span class="im">as</span> t</a>
<a class="sourceLine" id="cb15-6" data-line-number="6"><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</a>
<a class="sourceLine" id="cb15-7" data-line-number="7"><span class="im">import</span> imageio</a>
<a class="sourceLine" id="cb15-8" data-line-number="8">plt.style.use(<span class="st">&#39;seaborn&#39;</span>)  <span class="co"># 设置使用的样式</span></a>
<a class="sourceLine" id="cb15-9" data-line-number="9"></a>
<a class="sourceLine" id="cb15-10" data-line-number="10"></a>
<a class="sourceLine" id="cb15-11" data-line-number="11"><span class="co">&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb15-12" data-line-number="12"><span class="co">建立数据集   </span></a>
<a class="sourceLine" id="cb15-13" data-line-number="13"><span class="co">&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb15-14" data-line-number="14"><span class="co"># 假数据, 中心点分别为(2,2)和(-2,-2)的两个正态分布的数据</span></a>
<a class="sourceLine" id="cb15-15" data-line-number="15">n_data <span class="op">=</span> t.ones(<span class="dv">100</span>, <span class="dv">2</span>)         <span class="co"># 数据的基本形态</span></a>
<a class="sourceLine" id="cb15-16" data-line-number="16">x0 <span class="op">=</span> t.normal(<span class="dv">2</span><span class="op">*</span>n_data, <span class="dv">1</span>)      <span class="co"># 类型0(label) x data (tensor), shape=(100, 2)</span></a>
<a class="sourceLine" id="cb15-17" data-line-number="17">y0 <span class="op">=</span> t.zeros(<span class="dv">100</span>)               <span class="co"># 类型0(label) y data (tensor), shape=(100, )</span></a>
<a class="sourceLine" id="cb15-18" data-line-number="18">x1 <span class="op">=</span> t.normal(<span class="op">-</span><span class="dv">2</span><span class="op">*</span>n_data, <span class="dv">1</span>)     <span class="co"># 类型1 x data (tensor), shape=(100, 1)</span></a>
<a class="sourceLine" id="cb15-19" data-line-number="19">y1 <span class="op">=</span> t.ones(<span class="dv">100</span>)                <span class="co"># 类型1 y data (tensor), shape=(100, )</span></a>
<a class="sourceLine" id="cb15-20" data-line-number="20"></a>
<a class="sourceLine" id="cb15-21" data-line-number="21"><span class="co"># 注意 x, y 数据的数据形式是一定要像下面一样 (torch.cat 是在合并数据)</span></a>
<a class="sourceLine" id="cb15-22" data-line-number="22"><span class="co"># 合并x当做数据， y当做标签，注意标签的类型应该是LongTensor</span></a>
<a class="sourceLine" id="cb15-23" data-line-number="23">x <span class="op">=</span> t.cat((x0, x1), <span class="dv">0</span>).<span class="bu">type</span>(t.FloatTensor)  <span class="co"># FloatTensor = 32-bit floating</span></a>
<a class="sourceLine" id="cb15-24" data-line-number="24">y <span class="op">=</span> t.cat((y0, y1), ).<span class="bu">type</span>(t.LongTensor)    <span class="co"># LongTensor = 64-bit integer</span></a>
<a class="sourceLine" id="cb15-25" data-line-number="25"></a>
<a class="sourceLine" id="cb15-26" data-line-number="26"><span class="co"># 根据c的标签不同颜色染色</span></a>
<a class="sourceLine" id="cb15-27" data-line-number="27">plt.scatter(x.data.numpy()[:, <span class="dv">0</span>], x.data.numpy()[:, <span class="dv">1</span>], c<span class="op">=</span>y.data.numpy(), s<span class="op">=</span><span class="dv">100</span>, lw<span class="op">=</span><span class="dv">0</span>, cmap<span class="op">=</span><span class="st">&#39;RdYlGn&#39;</span>)</a>
<a class="sourceLine" id="cb15-28" data-line-number="28">plt.show()</a>
<a class="sourceLine" id="cb15-29" data-line-number="29"></a>
<a class="sourceLine" id="cb15-30" data-line-number="30"></a>
<a class="sourceLine" id="cb15-31" data-line-number="31"><span class="co">&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb15-32" data-line-number="32"><span class="co">建立神经网络</span></a>
<a class="sourceLine" id="cb15-33" data-line-number="33"><span class="co">&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb15-34" data-line-number="34"><span class="im">import</span> torch.nn.functional <span class="im">as</span> F  <span class="co"># 激励函数</span></a>
<a class="sourceLine" id="cb15-35" data-line-number="35"><span class="kw">class</span> Net(t.nn.Module):</a>
<a class="sourceLine" id="cb15-36" data-line-number="36">    <span class="co">&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb15-37" data-line-number="37"><span class="co">    __init__()是定义各层</span></a>
<a class="sourceLine" id="cb15-38" data-line-number="38"><span class="co">    forward()才是真正搭建网络</span></a>
<a class="sourceLine" id="cb15-39" data-line-number="39"><span class="co">    &quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb15-40" data-line-number="40">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_feature, n_hidden, n_output):</a>
<a class="sourceLine" id="cb15-41" data-line-number="41">        <span class="bu">super</span>(Net, <span class="va">self</span>).<span class="fu">__init__</span>()</a>
<a class="sourceLine" id="cb15-42" data-line-number="42">        <span class="va">self</span>.hidden <span class="op">=</span> nn.Linear(n_feature, n_hidden)  <span class="co"># hidden是属性，后面是它这一层的内容</span></a>
<a class="sourceLine" id="cb15-43" data-line-number="43">        <span class="va">self</span>.output <span class="op">=</span> nn.Linear(n_hidden, n_output)</a>
<a class="sourceLine" id="cb15-44" data-line-number="44"></a>
<a class="sourceLine" id="cb15-45" data-line-number="45">    <span class="kw">def</span> forward(<span class="va">self</span>, x):  </a>
<a class="sourceLine" id="cb15-46" data-line-number="46">        <span class="co"># module中的forward功能，正向传播初入值， 用神经网络分析出输出值</span></a>
<a class="sourceLine" id="cb15-47" data-line-number="47">        x <span class="op">=</span> F.relu(<span class="va">self</span>.hidden(x))</a>
<a class="sourceLine" id="cb15-48" data-line-number="48">        x <span class="op">=</span> <span class="va">self</span>.output(x)</a>
<a class="sourceLine" id="cb15-49" data-line-number="49">        <span class="cf">return</span> x</a>
<a class="sourceLine" id="cb15-50" data-line-number="50"></a>
<a class="sourceLine" id="cb15-51" data-line-number="51"></a>
<a class="sourceLine" id="cb15-52" data-line-number="52">net <span class="op">=</span> Net(<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">2</span>)  <span class="co"># 输入输出类别需要因任务而异</span></a>
<a class="sourceLine" id="cb15-53" data-line-number="53"><span class="bu">print</span>(net)  </a>
<a class="sourceLine" id="cb15-54" data-line-number="54"></a>
<a class="sourceLine" id="cb15-55" data-line-number="55"></a>
<a class="sourceLine" id="cb15-56" data-line-number="56"><span class="co">&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb15-57" data-line-number="57"><span class="co">训练网络</span></a>
<a class="sourceLine" id="cb15-58" data-line-number="58"><span class="co">&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb15-59" data-line-number="59">optimizer <span class="op">=</span> t.optim.SGD(net.parameters(), lr<span class="op">=</span><span class="fl">0.01</span>)  <span class="co"># 传入net的参数 和 lr, 为了演示将lr调小</span></a>
<a class="sourceLine" id="cb15-60" data-line-number="60">loss_func <span class="op">=</span> t.nn.CrossEntropyLoss()      <span class="co"># 注意多分类用CrossEntropyLoss  [0, 0, 1] -&gt; [0.2, 0.1, 0.7]</span></a>
<a class="sourceLine" id="cb15-61" data-line-number="61"></a>
<a class="sourceLine" id="cb15-62" data-line-number="62"><span class="co"># plt.ion()   # 设置为实时打印的画图</span></a>
<a class="sourceLine" id="cb15-63" data-line-number="63"></a>
<a class="sourceLine" id="cb15-64" data-line-number="64">image_list <span class="op">=</span> []</a>
<a class="sourceLine" id="cb15-65" data-line-number="65"><span class="cf">for</span> _ <span class="kw">in</span> tqdm(<span class="bu">range</span>(<span class="dv">100</span>)):</a>
<a class="sourceLine" id="cb15-66" data-line-number="66">    out <span class="op">=</span> net(x)    <span class="co"># 分类是out, [-2, -0.12, 20]， 需要再加入激活层转化为概率 ：F.softmax(out)</span></a>
<a class="sourceLine" id="cb15-67" data-line-number="67"></a>
<a class="sourceLine" id="cb15-68" data-line-number="68">    loss <span class="op">=</span> loss_func(out, y)  <span class="co"># 计算误差</span></a>
<a class="sourceLine" id="cb15-69" data-line-number="69"></a>
<a class="sourceLine" id="cb15-70" data-line-number="70">    <span class="co">&quot;&quot;&quot;优化&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb15-71" data-line-number="71">    optimizer.zero_grad()   <span class="co"># 清空上一步残余更新参数值，要把梯度清零</span></a>
<a class="sourceLine" id="cb15-72" data-line-number="72">    loss.backward()         <span class="co"># 误差反向传播，计算梯度作为参数更新值</span></a>
<a class="sourceLine" id="cb15-73" data-line-number="73">    optimizer.step()        <span class="co"># step()进行单次优化，将参数重新施加到net的parameters上</span></a>
<a class="sourceLine" id="cb15-74" data-line-number="74"></a>
<a class="sourceLine" id="cb15-75" data-line-number="75">    <span class="co">&quot;&quot;&quot;可视化&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb15-76" data-line-number="76">    <span class="cf">if</span> _ <span class="op">%</span> <span class="dv">2</span> <span class="op">==</span> <span class="dv">0</span>:</a>
<a class="sourceLine" id="cb15-77" data-line-number="77">        plt.cla()</a>
<a class="sourceLine" id="cb15-78" data-line-number="78">        <span class="co"># 过了一道 softmax 的激励函数后的最大概率才是预测值</span></a>
<a class="sourceLine" id="cb15-79" data-line-number="79">        prediction <span class="op">=</span> t.<span class="bu">max</span>(F.softmax(out), <span class="dv">1</span>)[<span class="dv">1</span>]    <span class="co"># max()[1]取索引(label)</span></a>
<a class="sourceLine" id="cb15-80" data-line-number="80">        pred_y <span class="op">=</span> prediction.data.numpy().squeeze()</a>
<a class="sourceLine" id="cb15-81" data-line-number="81">        target_y <span class="op">=</span> y.data.numpy()</a>
<a class="sourceLine" id="cb15-82" data-line-number="82">        plt.scatter(x.data.numpy()[:, <span class="dv">0</span>], x.data.numpy()[:, <span class="dv">1</span>], c<span class="op">=</span>pred_y, s<span class="op">=</span><span class="dv">100</span>, lw<span class="op">=</span><span class="dv">0</span>, cmap<span class="op">=</span><span class="st">&#39;RdYlGn&#39;</span>)</a>
<a class="sourceLine" id="cb15-83" data-line-number="83">        accuracy <span class="op">=</span> <span class="bu">sum</span>(pred_y <span class="op">==</span> target_y) <span class="op">/</span> <span class="fl">200.</span>  <span class="co"># 预测中有多少和真实值一样</span></a>
<a class="sourceLine" id="cb15-84" data-line-number="84">        plt.text(<span class="fl">1.5</span>, <span class="dv">-4</span>, <span class="st">&#39;Accuracy=</span><span class="sc">%.2f</span><span class="st">&#39;</span> <span class="op">%</span> accuracy, fontdict<span class="op">=</span>{<span class="st">&#39;size&#39;</span>: <span class="dv">20</span>, <span class="st">&#39;color&#39;</span>: <span class="st">&#39;red&#39;</span>})</a>
<a class="sourceLine" id="cb15-85" data-line-number="85"></a>
<a class="sourceLine" id="cb15-86" data-line-number="86">        <span class="co"># 生成动态图</span></a>
<a class="sourceLine" id="cb15-87" data-line-number="87">        plt.savefig(<span class="st">&quot;temp.jpg&quot;</span>)</a>
<a class="sourceLine" id="cb15-88" data-line-number="88">        plt.pause(<span class="fl">0.1</span>)</a>
<a class="sourceLine" id="cb15-89" data-line-number="89">        image_list.append(imageio.imread(<span class="st">&quot;temp.jpg&quot;</span>))  <span class="co"># 可以不用循环i，直接用列表形式</span></a>
<a class="sourceLine" id="cb15-90" data-line-number="90"></a>
<a class="sourceLine" id="cb15-91" data-line-number="91">imageio.mimsave(<span class="st">&#39;Classification.gif&#39;</span>, image_list,<span class="st">&#39;GIF&#39;</span>, duration<span class="op">=</span><span class="fl">0.05</span>)   <span class="co"># duration 是图像间的间隔</span></a>
<a class="sourceLine" id="cb15-92" data-line-number="92"></a>
<a class="sourceLine" id="cb15-93" data-line-number="93"><span class="co"># plt.ioff()</span></a>
<a class="sourceLine" id="cb15-94" data-line-number="94"><span class="co"># plt.show()</span></a></code></pre></div>
<h1 id="更快的搭建神经网络">更快的搭建神经网络</h1>
<p>由输出可以发现, net2将激活函数也纳入网络中了, 但net中激活函数是在forward()中才被调用, 说明:</p>
<p><code>net</code> 的好处就是你可以根据你的个人需要更加个性化你自己的<strong>前向传播过程</strong>, 比如(RNN).</p>
<p><code>net2</code>的好处是更加简单</p>
<blockquote>
<p>F.relu() 是一个function</p>
<p>nn.ReLU() 是一个class</p>
</blockquote>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb16-1" data-line-number="1">net2 <span class="op">=</span> nn.Sequential(</a>
<a class="sourceLine" id="cb16-2" data-line-number="2">    nn.Linear(<span class="dv">1</span>, <span class="dv">10</span>),</a>
<a class="sourceLine" id="cb16-3" data-line-number="3">    nn.ReLU(),</a>
<a class="sourceLine" id="cb16-4" data-line-number="4">    nn.Linear(<span class="dv">10</span>,<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb16-5" data-line-number="5">)</a>
<a class="sourceLine" id="cb16-6" data-line-number="6"><span class="bu">print</span>(net)</a>
<a class="sourceLine" id="cb16-7" data-line-number="7"><span class="bu">print</span>(net2)</a></code></pre></div>
<p>输出:</p>
<pre><code>Net(        # 正常搭建
  (hidden): Linear(in_features=1, out_features=10, bias=True)
  (output): Linear(in_features=10, out_features=1, bias=True)
)
Sequential(
  (0): Linear(in_features=1, out_features=10, bias=True)
  (1): ReLU()
  (2): Linear(in_features=10, out_features=1, bias=True)
)</code></pre>
<h1 id="保存和恢复模型">保存和恢复模型</h1>
<ol type="1">
<li><p>保存网络</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb18-1" data-line-number="1">    t.save(net1, <span class="st">&#39;./Try_ModelSaved/net.pkl&#39;</span>)  <span class="co"># 保存整个网络</span></a>
<a class="sourceLine" id="cb18-2" data-line-number="2">    t.save(net1.state_dict(), <span class="st">&#39;./Try_ModelSaved/net_params.pkl&#39;</span>)  <span class="co"># 只保存参数,速度快</span></a></code></pre></div></li>
<li><p>恢复整个网络</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb19-1" data-line-number="1">    net2 <span class="op">=</span> t.load(<span class="st">&#39;./Try_ModelSaved/net.pkl&#39;</span>)  <span class="co"># restore entire net1 to net2</span></a>
<a class="sourceLine" id="cb19-2" data-line-number="2">    prediction <span class="op">=</span> net2(x)</a></code></pre></div></li>
<li><p>恢复网络参数</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb20-1" data-line-number="1">    <span class="co">&quot;&quot;&quot;新建 net3&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb20-2" data-line-number="2">    net3 <span class="op">=</span> t.nn.Sequential(</a>
<a class="sourceLine" id="cb20-3" data-line-number="3">        nn.Linear(<span class="dv">1</span>, <span class="dv">10</span>),</a>
<a class="sourceLine" id="cb20-4" data-line-number="4">        nn.ReLU(),</a>
<a class="sourceLine" id="cb20-5" data-line-number="5">        nn.Linear(<span class="dv">10</span>, <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb20-6" data-line-number="6">    )</a>
<a class="sourceLine" id="cb20-7" data-line-number="7">    <span class="co">&quot;&quot;&quot;将参数复制到net3中&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb20-8" data-line-number="8">    net3.load_state_dict(t.load(<span class="st">&#39;./Try_ModelSaved/net_params.pkl&#39;</span>))</a>
<a class="sourceLine" id="cb20-9" data-line-number="9">    prediction <span class="op">=</span> net3(x) </a></code></pre></div></li>
</ol>
<p>由图可以看出3个net得到的回归曲线完全相同, 因为它们的内部参数都是完全相同的.</p>
<figure>
<img src="/img/in-post/20_07/ModelSaveUse_10hidden.png" alt="ModelSaveUse_10hidden" /><figcaption>ModelSaveUse_10hidden</figcaption>
</figure>
<blockquote>
<p>hidden layer nodes == 10</p>
</blockquote>
<figure>
<img src="/img/in-post/20_07/ModelSaveUse_100hidden.png" alt="ModelSaveUse_100hidden" /><figcaption>ModelSaveUse_100hidden</figcaption>
</figure>
<blockquote>
<p>hidden layer nodes == 100, 增加宽度后拟合效果明显好与hidden layer nodes == 10的网络.</p>
</blockquote>
<p>完整代码:</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb21-1" data-line-number="1"><span class="im">import</span> os</a>
<a class="sourceLine" id="cb21-2" data-line-number="2">os.environ[<span class="st">&#39;KMP_DUPLICATE_LIB_OK&#39;</span>]<span class="op">=</span><span class="st">&#39;True&#39;</span></a>
<a class="sourceLine" id="cb21-3" data-line-number="3"><span class="im">import</span> imageio</a>
<a class="sourceLine" id="cb21-4" data-line-number="4"><span class="im">from</span> tqdm <span class="im">import</span> tqdm</a>
<a class="sourceLine" id="cb21-5" data-line-number="5"><span class="im">import</span> torch <span class="im">as</span> t</a>
<a class="sourceLine" id="cb21-6" data-line-number="6"><span class="im">import</span> torch.nn <span class="im">as</span> nn</a>
<a class="sourceLine" id="cb21-7" data-line-number="7"><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</a>
<a class="sourceLine" id="cb21-8" data-line-number="8">plt.style.use(<span class="st">&#39;seaborn&#39;</span>)  <span class="co"># 设置使用的样式</span></a>
<a class="sourceLine" id="cb21-9" data-line-number="9"></a>
<a class="sourceLine" id="cb21-10" data-line-number="10">t.manual_seed(<span class="dv">1</span>)    <span class="co"># reproducible</span></a>
<a class="sourceLine" id="cb21-11" data-line-number="11"></a>
<a class="sourceLine" id="cb21-12" data-line-number="12"><span class="co"># 假数据</span></a>
<a class="sourceLine" id="cb21-13" data-line-number="13">x <span class="op">=</span> t.unsqueeze(t.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">100</span>), dim<span class="op">=</span><span class="dv">1</span>)  <span class="co"># x data (tensor), shape=(100, 1)</span></a>
<a class="sourceLine" id="cb21-14" data-line-number="14">y <span class="op">=</span> x.<span class="bu">pow</span>(<span class="dv">2</span>) <span class="op">+</span> <span class="fl">0.2</span><span class="op">*</span>t.rand(x.size())  <span class="co"># noisy y data (tensor), shape=(100, 1)</span></a>
<a class="sourceLine" id="cb21-15" data-line-number="15"></a>
<a class="sourceLine" id="cb21-16" data-line-number="16"></a>
<a class="sourceLine" id="cb21-17" data-line-number="17"><span class="kw">def</span> savemodel():</a>
<a class="sourceLine" id="cb21-18" data-line-number="18">    <span class="co">&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb21-19" data-line-number="19"><span class="co">    保存网络</span></a>
<a class="sourceLine" id="cb21-20" data-line-number="20"><span class="co">    &quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb21-21" data-line-number="21">    <span class="co"># Build Net</span></a>
<a class="sourceLine" id="cb21-22" data-line-number="22">    net1 <span class="op">=</span> nn.Sequential(</a>
<a class="sourceLine" id="cb21-23" data-line-number="23">        nn.Linear(<span class="dv">1</span>, <span class="dv">10</span>),</a>
<a class="sourceLine" id="cb21-24" data-line-number="24">        nn.ReLU(),</a>
<a class="sourceLine" id="cb21-25" data-line-number="25">        nn.Linear(<span class="dv">10</span>, <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb21-26" data-line-number="26">    )</a>
<a class="sourceLine" id="cb21-27" data-line-number="27">    optimizer <span class="op">=</span> t.optim.SGD(net1.parameters(), lr<span class="op">=</span><span class="fl">0.5</span>)</a>
<a class="sourceLine" id="cb21-28" data-line-number="28">    loss_func <span class="op">=</span> t.nn.MSELoss()</a>
<a class="sourceLine" id="cb21-29" data-line-number="29"></a>
<a class="sourceLine" id="cb21-30" data-line-number="30">    <span class="co"># Train</span></a>
<a class="sourceLine" id="cb21-31" data-line-number="31">    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</a>
<a class="sourceLine" id="cb21-32" data-line-number="32">        prediction <span class="op">=</span> net1(x)</a>
<a class="sourceLine" id="cb21-33" data-line-number="33">        loss <span class="op">=</span> loss_func(prediction, y)</a>
<a class="sourceLine" id="cb21-34" data-line-number="34"></a>
<a class="sourceLine" id="cb21-35" data-line-number="35">        optimizer.zero_grad()</a>
<a class="sourceLine" id="cb21-36" data-line-number="36">        loss.backward()</a>
<a class="sourceLine" id="cb21-37" data-line-number="37">        optimizer.step()</a>
<a class="sourceLine" id="cb21-38" data-line-number="38"></a>
<a class="sourceLine" id="cb21-39" data-line-number="39">    <span class="co">&quot;&quot;&quot;Save model&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb21-40" data-line-number="40">    t.save(net1, <span class="st">&#39;./Try_ModelSaved/net.pkl&#39;</span>)  <span class="co"># 保存整个网络</span></a>
<a class="sourceLine" id="cb21-41" data-line-number="41">    t.save(net1.state_dict(), <span class="st">&#39;./Try_ModelSaved/net_params.pkl&#39;</span>)  <span class="co"># 只保存参数,速度快</span></a>
<a class="sourceLine" id="cb21-42" data-line-number="42"></a>
<a class="sourceLine" id="cb21-43" data-line-number="43"></a>
<a class="sourceLine" id="cb21-44" data-line-number="44"><span class="kw">def</span> restore_net():</a>
<a class="sourceLine" id="cb21-45" data-line-number="45">    <span class="co">&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb21-46" data-line-number="46"><span class="co">    提取整个网络</span></a>
<a class="sourceLine" id="cb21-47" data-line-number="47"><span class="co">        网络大时可能会慢</span></a>
<a class="sourceLine" id="cb21-48" data-line-number="48"><span class="co">    &quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb21-49" data-line-number="49">    net2 <span class="op">=</span> t.load(<span class="st">&#39;./Try_ModelSaved/net.pkl&#39;</span>)  <span class="co"># restore entire net1 to net2</span></a>
<a class="sourceLine" id="cb21-50" data-line-number="50">    prediction <span class="op">=</span> net2(x)</a>
<a class="sourceLine" id="cb21-51" data-line-number="51"></a>
<a class="sourceLine" id="cb21-52" data-line-number="52"><span class="kw">def</span> restore_params():</a>
<a class="sourceLine" id="cb21-53" data-line-number="53">    <span class="co">&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb21-54" data-line-number="54"><span class="co">    提取网络参数</span></a>
<a class="sourceLine" id="cb21-55" data-line-number="55"><span class="co">        提取参数, 放入新建的网络中</span></a>
<a class="sourceLine" id="cb21-56" data-line-number="56"><span class="co">    &quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb21-57" data-line-number="57">    <span class="co">&quot;&quot;&quot;新建 net3&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb21-58" data-line-number="58">    net3 <span class="op">=</span> t.nn.Sequential(</a>
<a class="sourceLine" id="cb21-59" data-line-number="59">        nn.Linear(<span class="dv">1</span>, <span class="dv">10</span>),</a>
<a class="sourceLine" id="cb21-60" data-line-number="60">        nn.ReLU(),</a>
<a class="sourceLine" id="cb21-61" data-line-number="61">        nn.Linear(<span class="dv">10</span>, <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb21-62" data-line-number="62">    )</a>
<a class="sourceLine" id="cb21-63" data-line-number="63">    <span class="co">&quot;&quot;&quot;将参数复制到net3中&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb21-64" data-line-number="64">    net3.load_state_dict(t.load(<span class="st">&#39;./Try_ModelSaved/net_params.pkl&#39;</span>))</a>
<a class="sourceLine" id="cb21-65" data-line-number="65">    prediction <span class="op">=</span> net3(x)</a>
<a class="sourceLine" id="cb21-66" data-line-number="66"></a>
<a class="sourceLine" id="cb21-67" data-line-number="67"></a>
<a class="sourceLine" id="cb21-68" data-line-number="68">savemodel()</a>
<a class="sourceLine" id="cb21-69" data-line-number="69">restore_net()</a>
<a class="sourceLine" id="cb21-70" data-line-number="70">restore_params()</a></code></pre></div>
<h1 id="批训练-dataloader">批训练 — DataLoader</h1>
<ol type="1">
<li>批训练可以有很多种途径, 详情请见 <a href="https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/3-06-speed-up-learning/">我制作的 训练优化器 动画简介</a>.</li>
<li>Torch 中提供了一种帮你<strong>整理你的数据结构</strong>的好东西, <code>DataLoader</code> 是 torch 给你用来包装你的数据的工具.
<ol type="1">
<li>作用: <strong>包装自己的数据, 进行批训练</strong>.</li>
<li>过程
<ol type="1">
<li><strong>将自己的 (numpy array 或其他) 数据形式装换成 Tensor </strong></li>
<li><strong>然后再放进这个包装器中</strong>.</li>
</ol></li>
<li>好处: <strong>他们帮你有效地迭代数据</strong></li>
</ol></li>
<li><p><code>DataLoader</code>核心代码</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb22-1" data-line-number="1"><span class="im">import</span> torch,utils.data <span class="im">as</span> Data</a>
<a class="sourceLine" id="cb22-2" data-line-number="2"><span class="co"># 把 dataset 放入 DataLoader</span></a>
<a class="sourceLine" id="cb22-3" data-line-number="3">loader <span class="op">=</span> Data.DataLoader(</a>
<a class="sourceLine" id="cb22-4" data-line-number="4">    dataset<span class="op">=</span>torch_dataset,      <span class="co"># torch TensorDataset format</span></a>
<a class="sourceLine" id="cb22-5" data-line-number="5">    batch_size<span class="op">=</span>BATCH_SIZE,      <span class="co"># mini batch size</span></a>
<a class="sourceLine" id="cb22-6" data-line-number="6">    shuffle<span class="op">=</span><span class="va">True</span>,               <span class="co"># 要不要打乱数据 (打乱比较好)</span></a>
<a class="sourceLine" id="cb22-7" data-line-number="7">    num_workers<span class="op">=</span><span class="dv">2</span>,              <span class="co"># 多线程来读数据</span></a>
<a class="sourceLine" id="cb22-8" data-line-number="8">)                </a></code></pre></div></li>
<li><p>细节:</p>
<ol type="1">
<li>如果最后的<code>step</code>里数据不够一个<code>batch_size</code>时, 最后一个<code>step</code>会输出剩下的数据</li>
</ol></li>
</ol>
<p>完整示例代码:</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb23-1" data-line-number="1"><span class="im">import</span> os</a>
<a class="sourceLine" id="cb23-2" data-line-number="2">os.environ[<span class="st">&#39;KMP_DUPLICATE_LIB_OK&#39;</span>]<span class="op">=</span><span class="st">&#39;True&#39;</span></a>
<a class="sourceLine" id="cb23-3" data-line-number="3"><span class="im">import</span> imageio</a>
<a class="sourceLine" id="cb23-4" data-line-number="4"><span class="im">from</span> tqdm <span class="im">import</span> tqdm</a>
<a class="sourceLine" id="cb23-5" data-line-number="5"><span class="im">import</span> torch <span class="im">as</span> t</a>
<a class="sourceLine" id="cb23-6" data-line-number="6"><span class="im">import</span> torch.nn <span class="im">as</span> nn</a>
<a class="sourceLine" id="cb23-7" data-line-number="7"><span class="im">import</span> torch.utils.data <span class="im">as</span> Data</a>
<a class="sourceLine" id="cb23-8" data-line-number="8"><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</a>
<a class="sourceLine" id="cb23-9" data-line-number="9">plt.style.use(<span class="st">&#39;seaborn&#39;</span>)  <span class="co"># 设置使用的样式</span></a>
<a class="sourceLine" id="cb23-10" data-line-number="10"></a>
<a class="sourceLine" id="cb23-11" data-line-number="11">t.manual_seed(<span class="dv">1</span>)    <span class="co"># reproducible</span></a>
<a class="sourceLine" id="cb23-12" data-line-number="12"></a>
<a class="sourceLine" id="cb23-13" data-line-number="13">BATCH_SIZE <span class="op">=</span> <span class="dv">5</span>      <span class="co"># 批训练的数据个数</span></a>
<a class="sourceLine" id="cb23-14" data-line-number="14"></a>
<a class="sourceLine" id="cb23-15" data-line-number="15">x <span class="op">=</span> t.linspace(<span class="dv">1</span>, <span class="dv">10</span>, <span class="dv">10</span>)       <span class="co"># x data (torch tensor)</span></a>
<a class="sourceLine" id="cb23-16" data-line-number="16">y <span class="op">=</span> t.linspace(<span class="dv">10</span>, <span class="dv">1</span>, <span class="dv">10</span>)       <span class="co"># y data (torch tensor)</span></a>
<a class="sourceLine" id="cb23-17" data-line-number="17"></a>
<a class="sourceLine" id="cb23-18" data-line-number="18"><span class="co"># 先转换成 torch 能识别的 Dataset</span></a>
<a class="sourceLine" id="cb23-19" data-line-number="19">torch_dataset <span class="op">=</span> Data.TensorDataset(x, y)</a>
<a class="sourceLine" id="cb23-20" data-line-number="20"></a>
<a class="sourceLine" id="cb23-21" data-line-number="21"><span class="co"># 把 dataset 放入 DataLoader</span></a>
<a class="sourceLine" id="cb23-22" data-line-number="22">loader <span class="op">=</span> Data.DataLoader(</a>
<a class="sourceLine" id="cb23-23" data-line-number="23">    dataset<span class="op">=</span>torch_dataset,      <span class="co"># torch TensorDataset format</span></a>
<a class="sourceLine" id="cb23-24" data-line-number="24">    batch_size<span class="op">=</span>BATCH_SIZE,      <span class="co"># mini batch size</span></a>
<a class="sourceLine" id="cb23-25" data-line-number="25">    shuffle<span class="op">=</span><span class="va">True</span>,               <span class="co"># 要不要打乱数据 (打乱比较好)</span></a>
<a class="sourceLine" id="cb23-26" data-line-number="26">    num_workers<span class="op">=</span><span class="dv">2</span>,              <span class="co"># 多线程来读数据</span></a>
<a class="sourceLine" id="cb23-27" data-line-number="27">)</a>
<a class="sourceLine" id="cb23-28" data-line-number="28"></a>
<a class="sourceLine" id="cb23-29" data-line-number="29"><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>):</a>
<a class="sourceLine" id="cb23-30" data-line-number="30">    <span class="cf">for</span> step, (batch_x, batch_y) <span class="kw">in</span> <span class="bu">enumerate</span>(loader):</a>
<a class="sourceLine" id="cb23-31" data-line-number="31">        <span class="co"># 假设这里就是你训练的地方...</span></a>
<a class="sourceLine" id="cb23-32" data-line-number="32"></a>
<a class="sourceLine" id="cb23-33" data-line-number="33">        <span class="co"># 打出来一些数据</span></a>
<a class="sourceLine" id="cb23-34" data-line-number="34">        <span class="bu">print</span>(<span class="st">&#39;Epoch:&#39;</span>, epoch, <span class="st">&#39;| Step:&#39;</span>, step, <span class="st">&#39;| batch x:&#39;</span>,</a>
<a class="sourceLine" id="cb23-35" data-line-number="35">              batch_x.numpy(), <span class="st">&#39;| batch y:&#39;</span>, batch_y.numpy())</a></code></pre></div>
<p>输出:</p>
<pre><code>Epoch: 0 | Step: 0 | batch x: [ 5.  7. 10.  3.  4.] | batch y: [6. 4. 1. 8. 7.]
Epoch: 0 | Step: 1 | batch x: [2. 1. 8. 9. 6.] | batch y: [ 9. 10.  3.  2.  5.]
Epoch: 1 | Step: 0 | batch x: [ 4.  6.  7. 10.  8.] | batch y: [7. 5. 4. 1. 3.]
Epoch: 1 | Step: 1 | batch x: [5. 3. 2. 1. 9.] | batch y: [ 6.  8.  9. 10.  2.]
Epoch: 2 | Step: 0 | batch x: [ 4.  2.  5.  6. 10.] | batch y: [7. 9. 6. 5. 1.]
Epoch: 2 | Step: 1 | batch x: [3. 9. 1. 8. 7.] | batch y: [ 8.  2. 10.  3.  4.]</code></pre>
<h1 id="不同的优化器-加速神经网络训练">不同的优化器 — 加速神经网络训练</h1>
<ol type="1">
<li><p>主要有以下几种模式</p>
<ol type="1">
<li>Stochastic Gradient Descent(SGD)</li>
<li>Momentum</li>
<li>AdaGrad</li>
<li>RMSProp</li>
<li><strong>Adam</strong></li>
</ol></li>
<li><p>图示:</p>
<p><img src="/img/in-post/20_07/speedup3.png" alt="speedup3" style="zoom:67%;" /></p>
<p>原始形式:</p>
<p><img src="/img/in-post/20_07/speedup4.png" alt="加速神经网络训练 (Speed Up Training)" style="zoom: 50%;" /></p>
<p>Momentum:</p>
<p><img src="/img/in-post/20_07/speedup5.png" alt="加速神经网络训练 (Speed Up Training)" style="zoom:50%;" /></p>
<p>Adam:</p>
<p><img src="/img/in-post/20_07/speedup8.png" alt="加速神经网络训练 (Speed Up Training)" style="zoom:50%;" /></p>
<blockquote>
<p><code>SGD</code> 是最普通的优化器, 也可以说没有加速效果, 而 <code>Momentum</code> 是 <code>SGD</code> 的改良版, 它加入了动量原则. 后面的 <code>RMSprop</code> 又是 <code>Momentum</code> 的升级版. 而 <code>Adam</code> 又是 <code>RMSprop</code> 的升级版. 不过从这个结果中我们看到, <code>Adam</code> 的效果似乎比 <code>RMSprop</code> 要差一点. 所以说并不是越先进的优化器, 结果越佳. 我们在自己的试验中可以尝试不同的优化器, 找到那个最适合你数据/网络的优化器.</p>
</blockquote></li>
<li><p>注意画不同的图的时候要建立不同的<code>plt.figure()</code></p></li>
<li><p>待分类数据:</p>
<p><img src="/img/in-post/20_07/Optimizer_data.png" alt="Optimizer_data" /> 不同优化器对比结果: <img src="/img/in-post/20_07/Optimizer-6504925.png" alt="Optimizer" /></p></li>
</ol>
<p>完整代码:</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb25-1" data-line-number="1"><span class="im">import</span> os</a>
<a class="sourceLine" id="cb25-2" data-line-number="2">os.environ[<span class="st">&#39;KMP_DUPLICATE_LIB_OK&#39;</span>]<span class="op">=</span><span class="st">&#39;True&#39;</span></a>
<a class="sourceLine" id="cb25-3" data-line-number="3"><span class="im">import</span> imageio</a>
<a class="sourceLine" id="cb25-4" data-line-number="4"><span class="im">from</span> tqdm <span class="im">import</span> tqdm</a>
<a class="sourceLine" id="cb25-5" data-line-number="5"><span class="im">import</span> torch <span class="im">as</span> t</a>
<a class="sourceLine" id="cb25-6" data-line-number="6"><span class="im">import</span> torch.nn <span class="im">as</span> nn</a>
<a class="sourceLine" id="cb25-7" data-line-number="7"><span class="im">import</span> torch.utils.data <span class="im">as</span> Data</a>
<a class="sourceLine" id="cb25-8" data-line-number="8"><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</a>
<a class="sourceLine" id="cb25-9" data-line-number="9"><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</a>
<a class="sourceLine" id="cb25-10" data-line-number="10">plt.style.use(<span class="st">&#39;seaborn&#39;</span>)  <span class="co"># 设置使用的样式</span></a>
<a class="sourceLine" id="cb25-11" data-line-number="11"></a>
<a class="sourceLine" id="cb25-12" data-line-number="12">t.manual_seed(<span class="dv">1</span>)    <span class="co"># reproducible</span></a>
<a class="sourceLine" id="cb25-13" data-line-number="13"></a>
<a class="sourceLine" id="cb25-14" data-line-number="14">LR <span class="op">=</span> <span class="fl">0.01</span></a>
<a class="sourceLine" id="cb25-15" data-line-number="15">BATCH_SIZE <span class="op">=</span> <span class="dv">32</span>      <span class="co"># 批训练的数据个数</span></a>
<a class="sourceLine" id="cb25-16" data-line-number="16">EPOCH <span class="op">=</span> <span class="dv">12</span></a>
<a class="sourceLine" id="cb25-17" data-line-number="17"></a>
<a class="sourceLine" id="cb25-18" data-line-number="18"><span class="co"># fake dataset</span></a>
<a class="sourceLine" id="cb25-19" data-line-number="19">x <span class="op">=</span> t.unsqueeze(t.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1000</span>), dim<span class="op">=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb25-20" data-line-number="20">y <span class="op">=</span> x.<span class="bu">pow</span>(<span class="dv">2</span>) <span class="op">+</span> <span class="fl">0.1</span><span class="op">*</span>t.normal(t.zeros(<span class="op">*</span>x.size()))</a>
<a class="sourceLine" id="cb25-21" data-line-number="21"></a>
<a class="sourceLine" id="cb25-22" data-line-number="22"><span class="co"># 注意画不同的图的时候要建立不同的`plt.figure()`</span></a>
<a class="sourceLine" id="cb25-23" data-line-number="23">plt.figure()</a>
<a class="sourceLine" id="cb25-24" data-line-number="24">plt.scatter(x.numpy(), y.numpy())</a>
<a class="sourceLine" id="cb25-25" data-line-number="25">plt.show()</a>
<a class="sourceLine" id="cb25-26" data-line-number="26"></a>
<a class="sourceLine" id="cb25-27" data-line-number="27"><span class="co"># 使用上节内容提到的 data loader</span></a>
<a class="sourceLine" id="cb25-28" data-line-number="28">torch_dataset <span class="op">=</span> Data.TensorDataset(x, y)</a>
<a class="sourceLine" id="cb25-29" data-line-number="29">loader <span class="op">=</span> Data.DataLoader(dataset<span class="op">=</span>torch_dataset, batch_size<span class="op">=</span>BATCH_SIZE, shuffle<span class="op">=</span><span class="va">True</span>, num_workers<span class="op">=</span><span class="dv">2</span>,)</a>
<a class="sourceLine" id="cb25-30" data-line-number="30"></a>
<a class="sourceLine" id="cb25-31" data-line-number="31"></a>
<a class="sourceLine" id="cb25-32" data-line-number="32"><span class="co"># 为了对比每一种优化器, 我们给他们各自创建一个神经网络, 但这个神经网络都来自同一个 Net 形式.</span></a>
<a class="sourceLine" id="cb25-33" data-line-number="33"><span class="kw">class</span> Net(nn.Module):</a>
<a class="sourceLine" id="cb25-34" data-line-number="34">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</a>
<a class="sourceLine" id="cb25-35" data-line-number="35">        <span class="bu">super</span>(Net, <span class="va">self</span>).<span class="fu">__init__</span>()</a>
<a class="sourceLine" id="cb25-36" data-line-number="36">        <span class="va">self</span>.hidden <span class="op">=</span> nn.Linear(<span class="dv">1</span>, <span class="dv">10</span>)</a>
<a class="sourceLine" id="cb25-37" data-line-number="37">        <span class="va">self</span>.predict <span class="op">=</span> nn.Linear(<span class="dv">10</span>, <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb25-38" data-line-number="38"></a>
<a class="sourceLine" id="cb25-39" data-line-number="39">    <span class="kw">def</span> forward(<span class="va">self</span>, x):</a>
<a class="sourceLine" id="cb25-40" data-line-number="40">        x <span class="op">=</span> <span class="va">self</span>.hidden(x)</a>
<a class="sourceLine" id="cb25-41" data-line-number="41">        x <span class="op">=</span> F.relu(x)</a>
<a class="sourceLine" id="cb25-42" data-line-number="42">        x <span class="op">=</span> <span class="va">self</span>.predict(x)</a>
<a class="sourceLine" id="cb25-43" data-line-number="43">        <span class="cf">return</span> x</a>
<a class="sourceLine" id="cb25-44" data-line-number="44"></a>
<a class="sourceLine" id="cb25-45" data-line-number="45">net_SGD         <span class="op">=</span> Net()</a>
<a class="sourceLine" id="cb25-46" data-line-number="46">net_Momentum    <span class="op">=</span> Net()</a>
<a class="sourceLine" id="cb25-47" data-line-number="47">net_RMSprop     <span class="op">=</span> Net()</a>
<a class="sourceLine" id="cb25-48" data-line-number="48">net_Adam        <span class="op">=</span> Net()</a>
<a class="sourceLine" id="cb25-49" data-line-number="49">nets <span class="op">=</span> [net_SGD, net_Momentum, net_RMSprop, net_Adam]</a>
<a class="sourceLine" id="cb25-50" data-line-number="50"></a>
<a class="sourceLine" id="cb25-51" data-line-number="51">opt_SGD         <span class="op">=</span> t.optim.SGD(net_SGD.parameters(), lr<span class="op">=</span>LR)</a>
<a class="sourceLine" id="cb25-52" data-line-number="52">opt_Momentum    <span class="op">=</span> t.optim.SGD(net_Momentum.parameters(), lr<span class="op">=</span>LR, momentum<span class="op">=</span><span class="fl">0.8</span>)</a>
<a class="sourceLine" id="cb25-53" data-line-number="53">opt_RMSprop     <span class="op">=</span> t.optim.RMSprop(net_RMSprop.parameters(), lr<span class="op">=</span>LR, alpha<span class="op">=</span><span class="fl">0.9</span>)</a>
<a class="sourceLine" id="cb25-54" data-line-number="54">opt_Adam        <span class="op">=</span> t.optim.Adam(net_Adam.parameters(), lr<span class="op">=</span>LR, betas<span class="op">=</span>(<span class="fl">0.9</span>, <span class="fl">0.99</span>))</a>
<a class="sourceLine" id="cb25-55" data-line-number="55">optimizers <span class="op">=</span> [opt_SGD, opt_Momentum, opt_RMSprop, opt_Adam]</a>
<a class="sourceLine" id="cb25-56" data-line-number="56"></a>
<a class="sourceLine" id="cb25-57" data-line-number="57">loss_func <span class="op">=</span> nn.MSELoss()</a>
<a class="sourceLine" id="cb25-58" data-line-number="58">losses_his <span class="op">=</span> [[], [], [], []]   <span class="co"># record the loss of 4 net</span></a>
<a class="sourceLine" id="cb25-59" data-line-number="59"></a>
<a class="sourceLine" id="cb25-60" data-line-number="60"><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(EPOCH):</a>
<a class="sourceLine" id="cb25-61" data-line-number="61">    <span class="bu">print</span>(<span class="st">&#39;Epoch:&#39;</span>, epoch)</a>
<a class="sourceLine" id="cb25-62" data-line-number="62">    <span class="cf">for</span> step, (b_x, b_y) <span class="kw">in</span> <span class="bu">enumerate</span>(loader):</a>
<a class="sourceLine" id="cb25-63" data-line-number="63"></a>
<a class="sourceLine" id="cb25-64" data-line-number="64">        <span class="co"># 通过数组的形式, 迭代4次</span></a>
<a class="sourceLine" id="cb25-65" data-line-number="65">        <span class="cf">for</span> net, opt, l_his <span class="kw">in</span> <span class="bu">zip</span>(nets, optimizers, losses_his):</a>
<a class="sourceLine" id="cb25-66" data-line-number="66">            output <span class="op">=</span> net(b_x)</a>
<a class="sourceLine" id="cb25-67" data-line-number="67">            loss <span class="op">=</span> loss_func(output, b_y)</a>
<a class="sourceLine" id="cb25-68" data-line-number="68"></a>
<a class="sourceLine" id="cb25-69" data-line-number="69">            opt.zero_grad()</a>
<a class="sourceLine" id="cb25-70" data-line-number="70">            loss.backward()</a>
<a class="sourceLine" id="cb25-71" data-line-number="71">            opt.step()      <span class="co"># 为每个节点重设梯度</span></a>
<a class="sourceLine" id="cb25-72" data-line-number="72">            l_his.append(loss.data.numpy())</a>
<a class="sourceLine" id="cb25-73" data-line-number="73"></a>
<a class="sourceLine" id="cb25-74" data-line-number="74"><span class="co"># 注意画不同的图的时候要建立不同的`plt.figure()`</span></a>
<a class="sourceLine" id="cb25-75" data-line-number="75">plt.figure()       </a>
<a class="sourceLine" id="cb25-76" data-line-number="76">labels <span class="op">=</span> [<span class="st">&#39;SGD&#39;</span>, <span class="st">&#39;Momentum&#39;</span>, <span class="st">&#39;RMSprop&#39;</span>, <span class="st">&#39;Adam&#39;</span>]</a>
<a class="sourceLine" id="cb25-77" data-line-number="77"><span class="cf">for</span> i, l_his <span class="kw">in</span> <span class="bu">enumerate</span>(losses_his):</a>
<a class="sourceLine" id="cb25-78" data-line-number="78">    plt.plot(l_his, label<span class="op">=</span>labels[i])</a>
<a class="sourceLine" id="cb25-79" data-line-number="79">plt.legend(loc<span class="op">=</span><span class="st">&#39;best&#39;</span>)</a>
<a class="sourceLine" id="cb25-80" data-line-number="80">plt.xlabel(<span class="st">&#39;Steps&#39;</span>)</a>
<a class="sourceLine" id="cb25-81" data-line-number="81">plt.ylabel(<span class="st">&#39;Loss&#39;</span>)</a>
<a class="sourceLine" id="cb25-82" data-line-number="82">plt.ylim((<span class="dv">0</span>, <span class="fl">0.2</span>))</a>
<a class="sourceLine" id="cb25-83" data-line-number="83">plt.show()</a></code></pre></div>
<h1 id="cnn-mnidt分类">CNN — MNIDT分类</h1>
<ol type="1">
<li>加载数据
<ol type="1">
<li>选一个样本进行输出 <img src="/img/in-post/20_07/image-20200804154520873.png" alt="image-20200804154520873" style="zoom: 25%;" /></li>
</ol></li>
<li><p>下载数据集方法</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb26-1" data-line-number="1">train_data <span class="op">=</span> torchvision.datasets.MNIST(</a>
<a class="sourceLine" id="cb26-2" data-line-number="2">    root<span class="op">=</span><span class="st">&#39;Volumes/ArcFile/mnist/&#39;</span>,</a>
<a class="sourceLine" id="cb26-3" data-line-number="3">    train<span class="op">=</span><span class="va">True</span>,  <span class="co"># this is training data</span></a>
<a class="sourceLine" id="cb26-4" data-line-number="4">    <span class="co"># PIL.Image or numpy.ndarray to torch.FloatTensor(C * H * W), and normalize in the range [0.0, 1.0]</span></a>
<a class="sourceLine" id="cb26-5" data-line-number="5">    transform<span class="op">=</span>torchvision.transforms.ToTensor(),</a>
<a class="sourceLine" id="cb26-6" data-line-number="6"></a>
<a class="sourceLine" id="cb26-7" data-line-number="7">    download<span class="op">=</span>DOWNLOAD_MNIST</a>
<a class="sourceLine" id="cb26-8" data-line-number="8">)</a></code></pre></div></li>
<li><p>t-SNE聚类及可视化</p>
<p><code>low_dim_embs = tsne.fit_transform(last_layer.data.numpy()[:plot_only, :])</code>, ==<strong>通过最后一层的pixel进行t-SNE分类可视化</strong>==</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb27-1" data-line-number="1">test_output, last_layer <span class="op">=</span> cnn(test_x)  <span class="co"># x 赋值给 last_layer</span></a>
<a class="sourceLine" id="cb27-2" data-line-number="2"></a>
<a class="sourceLine" id="cb27-3" data-line-number="3"><span class="co"># Visualization of trained flatten layer (T-SNE)</span></a>
<a class="sourceLine" id="cb27-4" data-line-number="4">tsne <span class="op">=</span> TSNE(perplexity<span class="op">=</span><span class="dv">30</span>, n_components<span class="op">=</span><span class="dv">2</span>, init<span class="op">=</span><span class="st">&#39;pca&#39;</span>, n_iter<span class="op">=</span><span class="dv">5000</span>)</a>
<a class="sourceLine" id="cb27-5" data-line-number="5">plot_only <span class="op">=</span> <span class="dv">500</span></a>
<a class="sourceLine" id="cb27-6" data-line-number="6">low_dim_embs <span class="op">=</span> tsne.fit_transform(last_layer.data.numpy()[:plot_only, :])  <span class="co"># 通过最后一层的pixel进行t-SNE分类可视化</span></a></code></pre></div></li>
<li><p>CNN模型</p>
<ol type="1">
<li>通过padding, 经过卷积层图片大小不变</li>
<li>注意全连接层输入是batch_size个一维向量 — (32*7*7, 10)</li>
<li>可视化时需要 return x 来返回</li>
</ol>
<div class="sourceCode" id="cb28"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb28-1" data-line-number="1"><span class="kw">class</span> CNN(t.nn.Module):</a>
<a class="sourceLine" id="cb28-2" data-line-number="2">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</a>
<a class="sourceLine" id="cb28-3" data-line-number="3">        <span class="bu">super</span>(CNN, <span class="va">self</span>).<span class="fu">__init__</span>()</a>
<a class="sourceLine" id="cb28-4" data-line-number="4">        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Sequential(</a>
<a class="sourceLine" id="cb28-5" data-line-number="5">            <span class="co"># input shape (1, 28, 28) -&gt; (16, 28, 28) -&gt; (16, 14, 14)</span></a>
<a class="sourceLine" id="cb28-6" data-line-number="6">            nn.Conv2d(in_channels<span class="op">=</span><span class="dv">1</span>, out_channels<span class="op">=</span><span class="dv">16</span>, kernel_size<span class="op">=</span><span class="dv">5</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">2</span>),  <span class="co"># conv后的图片大小不变</span></a>
<a class="sourceLine" id="cb28-7" data-line-number="7">            nn.ReLU(),</a>
<a class="sourceLine" id="cb28-8" data-line-number="8">            nn.MaxPool2d(kernel_size<span class="op">=</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb28-9" data-line-number="9">        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Sequential(</a>
<a class="sourceLine" id="cb28-10" data-line-number="10">            <span class="co"># input shape (16, 14, 14) -&gt; (32, 14, 14) -&gt; (32, 7, 7)</span></a>
<a class="sourceLine" id="cb28-11" data-line-number="11">            nn.Conv2d(in_channels<span class="op">=</span><span class="dv">16</span>, out_channels<span class="op">=</span><span class="dv">32</span>, kernel_size<span class="op">=</span><span class="dv">5</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">2</span>),</a>
<a class="sourceLine" id="cb28-12" data-line-number="12">            nn.ReLU(),</a>
<a class="sourceLine" id="cb28-13" data-line-number="13">            nn.MaxPool2d(kernel_size<span class="op">=</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb28-14" data-line-number="14">        <span class="va">self</span>.out <span class="op">=</span> nn.Linear(<span class="dv">32</span> <span class="op">*</span> <span class="dv">7</span> <span class="op">*</span> <span class="dv">7</span>, <span class="dv">10</span>)  <span class="co"># 注意:(32*7*7, 10)</span></a>
<a class="sourceLine" id="cb28-15" data-line-number="15"></a>
<a class="sourceLine" id="cb28-16" data-line-number="16">    <span class="kw">def</span> forward(<span class="va">self</span>, x):</a>
<a class="sourceLine" id="cb28-17" data-line-number="17">        x <span class="op">=</span> <span class="va">self</span>.conv1(x)</a>
<a class="sourceLine" id="cb28-18" data-line-number="18">        x <span class="op">=</span> <span class="va">self</span>.conv2(x)</a>
<a class="sourceLine" id="cb28-19" data-line-number="19">        x <span class="op">=</span> x.view(x.size(<span class="dv">0</span>), <span class="dv">-1</span>)  <span class="co"># 注意要将输入展开成一维向量(batch_size, 32*7*7)</span></a>
<a class="sourceLine" id="cb28-20" data-line-number="20">        output <span class="op">=</span> <span class="va">self</span>.out(x)</a>
<a class="sourceLine" id="cb28-21" data-line-number="21">        <span class="cf">return</span> output, x        <span class="co"># 一定注意: 可视化时 return x for visualization</span></a></code></pre></div>
<p><img src="/img/in-post/20_07/CNN_MNIST.gif" alt="CNN_MNIST" style="zoom:50%;" />CNN结构输出:</p>
<pre><code>CNN(
  (conv1): Sequential(
    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): ReLU()
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): ReLU()
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (out): Linear(in_features=1568, out_features=10, bias=True)
)</code></pre>
<p>测试集前10个样本输出预测</p>
<pre><code>[7 2 1 0 4 1 4 9 5 9] prediction number
[7 2 1 0 4 1 4 9 5 9] real number</code></pre>
<p>完整代码:</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb31-1" data-line-number="1"><span class="im">import</span> os</a>
<a class="sourceLine" id="cb31-2" data-line-number="2"></a>
<a class="sourceLine" id="cb31-3" data-line-number="3">os.environ[<span class="st">&#39;KMP_DUPLICATE_LIB_OK&#39;</span>] <span class="op">=</span> <span class="st">&#39;True&#39;</span></a>
<a class="sourceLine" id="cb31-4" data-line-number="4"><span class="im">import</span> imageio</a>
<a class="sourceLine" id="cb31-5" data-line-number="5"><span class="im">from</span> tqdm <span class="im">import</span> tqdm</a>
<a class="sourceLine" id="cb31-6" data-line-number="6"><span class="im">import</span> torch <span class="im">as</span> t</a>
<a class="sourceLine" id="cb31-7" data-line-number="7"><span class="im">import</span> torch.nn <span class="im">as</span> nn</a>
<a class="sourceLine" id="cb31-8" data-line-number="8"><span class="im">import</span> torch.utils.data <span class="im">as</span> Data</a>
<a class="sourceLine" id="cb31-9" data-line-number="9"><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</a>
<a class="sourceLine" id="cb31-10" data-line-number="10"><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</a>
<a class="sourceLine" id="cb31-11" data-line-number="11"><span class="im">import</span> torchvision</a>
<a class="sourceLine" id="cb31-12" data-line-number="12"></a>
<a class="sourceLine" id="cb31-13" data-line-number="13">plt.style.use(<span class="st">&#39;seaborn&#39;</span>)  <span class="co"># 设置使用的样式</span></a>
<a class="sourceLine" id="cb31-14" data-line-number="14"></a>
<a class="sourceLine" id="cb31-15" data-line-number="15"><span class="co"># for visualization</span></a>
<a class="sourceLine" id="cb31-16" data-line-number="16"><span class="im">from</span> matplotlib <span class="im">import</span> cm</a>
<a class="sourceLine" id="cb31-17" data-line-number="17"><span class="cf">try</span>:</a>
<a class="sourceLine" id="cb31-18" data-line-number="18">    <span class="im">from</span> sklearn.manifold <span class="im">import</span> TSNE<span class="op">;</span> HAS_SK <span class="op">=</span> <span class="va">True</span></a>
<a class="sourceLine" id="cb31-19" data-line-number="19"><span class="cf">except</span>:</a>
<a class="sourceLine" id="cb31-20" data-line-number="20">    HAS_SK <span class="op">=</span> <span class="va">False</span><span class="op">;</span> <span class="bu">print</span>(<span class="st">&#39;Please install sklearn for layer visualization&#39;</span>)</a>
<a class="sourceLine" id="cb31-21" data-line-number="21"></a>
<a class="sourceLine" id="cb31-22" data-line-number="22">LR <span class="op">=</span> <span class="fl">0.001</span></a>
<a class="sourceLine" id="cb31-23" data-line-number="23">BATCH_SIZE <span class="op">=</span> <span class="dv">50</span>  <span class="co"># 批训练的数据个数</span></a>
<a class="sourceLine" id="cb31-24" data-line-number="24">EPOCH <span class="op">=</span> <span class="dv">8</span></a>
<a class="sourceLine" id="cb31-25" data-line-number="25">DOWNLOAD_MNIST <span class="op">=</span> <span class="va">False</span></a>
<a class="sourceLine" id="cb31-26" data-line-number="26"></a>
<a class="sourceLine" id="cb31-27" data-line-number="27"><span class="co">&quot;&quot;&quot;Mnist digits dataset&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb31-28" data-line-number="28"><span class="cf">if</span> <span class="kw">not</span>(os.path.exists(<span class="st">&#39;Volumes/ArcFile/mnist/&#39;</span>)) <span class="kw">or</span> <span class="kw">not</span> os.listdir(<span class="st">&#39;Volumes/ArcFile/mnist/&#39;</span>):</a>
<a class="sourceLine" id="cb31-29" data-line-number="29">    DOWNLOAD_MNIST <span class="op">=</span> <span class="va">True</span></a>
<a class="sourceLine" id="cb31-30" data-line-number="30"></a>
<a class="sourceLine" id="cb31-31" data-line-number="31">train_data <span class="op">=</span> torchvision.datasets.MNIST(</a>
<a class="sourceLine" id="cb31-32" data-line-number="32">    root<span class="op">=</span><span class="st">&#39;Volumes/ArcFile/mnist/&#39;</span>,</a>
<a class="sourceLine" id="cb31-33" data-line-number="33">    train<span class="op">=</span><span class="va">True</span>,  <span class="co"># this is training data</span></a>
<a class="sourceLine" id="cb31-34" data-line-number="34">    <span class="co"># PIL.Image or numpy.ndarray to torch.FloatTensor(C * H * W), and normalize in the range [0.0, 1.0]</span></a>
<a class="sourceLine" id="cb31-35" data-line-number="35">    transform<span class="op">=</span>torchvision.transforms.ToTensor(),</a>
<a class="sourceLine" id="cb31-36" data-line-number="36"></a>
<a class="sourceLine" id="cb31-37" data-line-number="37">    download<span class="op">=</span>DOWNLOAD_MNIST</a>
<a class="sourceLine" id="cb31-38" data-line-number="38">)</a>
<a class="sourceLine" id="cb31-39" data-line-number="39"><span class="co"># 批训练 50samples, 1 channel, 28x28 (50, 1, 28, 28)</span></a>
<a class="sourceLine" id="cb31-40" data-line-number="40">train_loader <span class="op">=</span> Data.DataLoader(dataset<span class="op">=</span>train_data, batch_size<span class="op">=</span>BATCH_SIZE, shuffle<span class="op">=</span><span class="va">True</span>)</a>
<a class="sourceLine" id="cb31-41" data-line-number="41"><span class="co"># plot one example</span></a>
<a class="sourceLine" id="cb31-42" data-line-number="42"><span class="bu">print</span>(train_data.train_data.size())                 <span class="co"># (60000, 28, 28)</span></a>
<a class="sourceLine" id="cb31-43" data-line-number="43"><span class="bu">print</span>(train_data.train_labels.size())               <span class="co"># (60000)</span></a>
<a class="sourceLine" id="cb31-44" data-line-number="44">plt.imshow(train_data.train_data[<span class="dv">0</span>].numpy(), cmap<span class="op">=</span><span class="st">&#39;gray&#39;</span>)</a>
<a class="sourceLine" id="cb31-45" data-line-number="45">plt.title(<span class="st">&#39;</span><span class="sc">%i</span><span class="st">&#39;</span> <span class="op">%</span> train_data.train_labels[<span class="dv">0</span>])</a>
<a class="sourceLine" id="cb31-46" data-line-number="46">plt.show()</a>
<a class="sourceLine" id="cb31-47" data-line-number="47"></a>
<a class="sourceLine" id="cb31-48" data-line-number="48"><span class="co"># pick 2000 samples to speed up testing</span></a>
<a class="sourceLine" id="cb31-49" data-line-number="49">test_data <span class="op">=</span> torchvision.datasets.MNIST(root<span class="op">=</span><span class="st">&#39;Volumes/ArcFile/mnist/&#39;</span>, train<span class="op">=</span><span class="va">False</span>)</a>
<a class="sourceLine" id="cb31-50" data-line-number="50"><span class="co"># shape from (2000, 28, 28) to (2000, 1, 28, 28), value in range(0,1)</span></a>
<a class="sourceLine" id="cb31-51" data-line-number="51">test_x <span class="op">=</span> t.unsqueeze(test_data.test_data, dim<span class="op">=</span><span class="dv">1</span>).<span class="bu">type</span>(t.FloatTensor)[:<span class="dv">2000</span>] <span class="op">/</span> <span class="fl">255.</span></a>
<a class="sourceLine" id="cb31-52" data-line-number="52">test_y <span class="op">=</span> test_data.test_labels[:<span class="dv">2000</span>]</a>
<a class="sourceLine" id="cb31-53" data-line-number="53"></a>
<a class="sourceLine" id="cb31-54" data-line-number="54"></a>
<a class="sourceLine" id="cb31-55" data-line-number="55"><span class="kw">class</span> CNN(t.nn.Module):</a>
<a class="sourceLine" id="cb31-56" data-line-number="56">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</a>
<a class="sourceLine" id="cb31-57" data-line-number="57">        <span class="bu">super</span>(CNN, <span class="va">self</span>).<span class="fu">__init__</span>()</a>
<a class="sourceLine" id="cb31-58" data-line-number="58">        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Sequential(</a>
<a class="sourceLine" id="cb31-59" data-line-number="59">            <span class="co"># input shape (1, 28, 28) -&gt; (16, 28, 28) -&gt; (16, 14, 14)</span></a>
<a class="sourceLine" id="cb31-60" data-line-number="60">            nn.Conv2d(in_channels<span class="op">=</span><span class="dv">1</span>, out_channels<span class="op">=</span><span class="dv">16</span>, kernel_size<span class="op">=</span><span class="dv">5</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">2</span>),  <span class="co"># conv后的图片大小不变</span></a>
<a class="sourceLine" id="cb31-61" data-line-number="61">            nn.ReLU(),</a>
<a class="sourceLine" id="cb31-62" data-line-number="62">            nn.MaxPool2d(kernel_size<span class="op">=</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb31-63" data-line-number="63">        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Sequential(</a>
<a class="sourceLine" id="cb31-64" data-line-number="64">            <span class="co"># input shape (16, 14, 14) -&gt; (32, 14, 14) -&gt; (32, 7, 7)</span></a>
<a class="sourceLine" id="cb31-65" data-line-number="65">            nn.Conv2d(in_channels<span class="op">=</span><span class="dv">16</span>, out_channels<span class="op">=</span><span class="dv">32</span>, kernel_size<span class="op">=</span><span class="dv">5</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">2</span>),</a>
<a class="sourceLine" id="cb31-66" data-line-number="66">            nn.ReLU(),</a>
<a class="sourceLine" id="cb31-67" data-line-number="67">            nn.MaxPool2d(kernel_size<span class="op">=</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb31-68" data-line-number="68">        <span class="va">self</span>.out <span class="op">=</span> nn.Linear(<span class="dv">32</span> <span class="op">*</span> <span class="dv">7</span> <span class="op">*</span> <span class="dv">7</span>, <span class="dv">10</span>)  <span class="co"># 注意:(32*7*7, 10)</span></a>
<a class="sourceLine" id="cb31-69" data-line-number="69"></a>
<a class="sourceLine" id="cb31-70" data-line-number="70">    <span class="kw">def</span> forward(<span class="va">self</span>, x):</a>
<a class="sourceLine" id="cb31-71" data-line-number="71">        x <span class="op">=</span> <span class="va">self</span>.conv1(x)</a>
<a class="sourceLine" id="cb31-72" data-line-number="72">        x <span class="op">=</span> <span class="va">self</span>.conv2(x)</a>
<a class="sourceLine" id="cb31-73" data-line-number="73">        x <span class="op">=</span> x.view(x.size(<span class="dv">0</span>), <span class="dv">-1</span>)  <span class="co"># 注意要将输入展开成一维向量(batch_size, 32*7*7)</span></a>
<a class="sourceLine" id="cb31-74" data-line-number="74">        output <span class="op">=</span> <span class="va">self</span>.out(x)</a>
<a class="sourceLine" id="cb31-75" data-line-number="75">        <span class="cf">return</span> output, x        <span class="co"># 一定注意: 可视化时 return x for visualization</span></a>
<a class="sourceLine" id="cb31-76" data-line-number="76"></a>
<a class="sourceLine" id="cb31-77" data-line-number="77"></a>
<a class="sourceLine" id="cb31-78" data-line-number="78">cnn <span class="op">=</span> CNN()</a>
<a class="sourceLine" id="cb31-79" data-line-number="79"><span class="bu">print</span>(cnn)</a>
<a class="sourceLine" id="cb31-80" data-line-number="80"></a>
<a class="sourceLine" id="cb31-81" data-line-number="81">optimizer <span class="op">=</span> t.optim.Adam(cnn.parameters(), lr<span class="op">=</span>LR, betas<span class="op">=</span>(<span class="fl">0.9</span>, <span class="fl">0.99</span>))</a>
<a class="sourceLine" id="cb31-82" data-line-number="82">loss_func <span class="op">=</span> nn.CrossEntropyLoss()</a>
<a class="sourceLine" id="cb31-83" data-line-number="83"></a>
<a class="sourceLine" id="cb31-84" data-line-number="84"></a>
<a class="sourceLine" id="cb31-85" data-line-number="85"><span class="kw">def</span> plot_with_labels(lowDWeights, labels, image_list, <span class="op">*</span>param):</a>
<a class="sourceLine" id="cb31-86" data-line-number="86">    plt.cla()</a>
<a class="sourceLine" id="cb31-87" data-line-number="87">    X, Y <span class="op">=</span> lowDWeights[:, <span class="dv">0</span>], lowDWeights[:, <span class="dv">1</span>]</a>
<a class="sourceLine" id="cb31-88" data-line-number="88">    epoch, loss, accuracy <span class="op">=</span> param</a>
<a class="sourceLine" id="cb31-89" data-line-number="89">    <span class="cf">for</span> x, y, s <span class="kw">in</span> <span class="bu">zip</span>(X, Y, labels):</a>
<a class="sourceLine" id="cb31-90" data-line-number="90">        c <span class="op">=</span> cm.rainbow(<span class="bu">int</span>(<span class="dv">255</span> <span class="op">*</span> s <span class="op">/</span> <span class="dv">9</span>))</a>
<a class="sourceLine" id="cb31-91" data-line-number="91">        plt.text(x, y, s, backgroundcolor<span class="op">=</span>c, fontsize<span class="op">=</span><span class="dv">9</span>)</a>
<a class="sourceLine" id="cb31-92" data-line-number="92">    <span class="co"># plt.xlim(X.min(), X.max())</span></a>
<a class="sourceLine" id="cb31-93" data-line-number="93">    <span class="co"># plt.ylim(Y.min(), Y.max())</span></a>
<a class="sourceLine" id="cb31-94" data-line-number="94">    plt.xlim(<span class="op">-</span><span class="dv">50</span>, <span class="dv">50</span>)</a>
<a class="sourceLine" id="cb31-95" data-line-number="95">    plt.ylim(<span class="op">-</span><span class="dv">40</span>, <span class="dv">40</span>)</a>
<a class="sourceLine" id="cb31-96" data-line-number="96">    plt.title(<span class="st">&#39;Visualize last layer&#39;</span>)</a>
<a class="sourceLine" id="cb31-97" data-line-number="97">    text <span class="op">=</span> (<span class="st">&#39;Epoch: </span><span class="sc">%d</span><span class="st">,&#39;</span> <span class="op">%</span> epoch <span class="op">+</span> <span class="st">&#39; train loss: </span><span class="sc">%.4f</span><span class="st">,&#39;</span> <span class="op">%</span> loss.data.numpy() <span class="op">+</span> <span class="st">&#39; test accuracy: </span><span class="sc">%.3f</span><span class="st">&#39;</span> <span class="op">%</span> accuracy)</a>
<a class="sourceLine" id="cb31-98" data-line-number="98">    plt.text(<span class="op">-</span><span class="dv">30</span>, <span class="dv">-35</span>, text, fontdict<span class="op">=</span>{<span class="st">&#39;size&#39;</span>: <span class="dv">13</span>, <span class="st">&#39;color&#39;</span>: <span class="st">&#39;red&#39;</span>})</a>
<a class="sourceLine" id="cb31-99" data-line-number="99">    plt.show()</a>
<a class="sourceLine" id="cb31-100" data-line-number="100">    plt.savefig(<span class="st">&quot;temp.jpg&quot;</span>)</a>
<a class="sourceLine" id="cb31-101" data-line-number="101">    plt.pause(<span class="fl">0.02</span>)</a>
<a class="sourceLine" id="cb31-102" data-line-number="102">    image_list.append(imageio.imread(<span class="st">&quot;temp.jpg&quot;</span>))  <span class="co"># 可以不用循环i，直接用列表形式</span></a>
<a class="sourceLine" id="cb31-103" data-line-number="103"></a>
<a class="sourceLine" id="cb31-104" data-line-number="104"></a>
<a class="sourceLine" id="cb31-105" data-line-number="105"><span class="kw">def</span> Train():</a>
<a class="sourceLine" id="cb31-106" data-line-number="106">    plt.ion()</a>
<a class="sourceLine" id="cb31-107" data-line-number="107">    image_list <span class="op">=</span> []</a>
<a class="sourceLine" id="cb31-108" data-line-number="108">    <span class="cf">for</span> epoch <span class="kw">in</span> tqdm(<span class="bu">range</span>(EPOCH)):</a>
<a class="sourceLine" id="cb31-109" data-line-number="109">        <span class="cf">for</span> step, (b_x, b_y) <span class="kw">in</span> <span class="bu">enumerate</span>(train_loader):</a>
<a class="sourceLine" id="cb31-110" data-line-number="110">            output <span class="op">=</span> cnn(b_x)[<span class="dv">0</span>]  <span class="co"># 注意如果需要visualization时会有2个返回值,故要选择</span></a>
<a class="sourceLine" id="cb31-111" data-line-number="111"></a>
<a class="sourceLine" id="cb31-112" data-line-number="112">            loss <span class="op">=</span> loss_func(output, b_y)</a>
<a class="sourceLine" id="cb31-113" data-line-number="113">            optimizer.zero_grad()  <span class="co"># 计算完损失就可以清除梯度了</span></a>
<a class="sourceLine" id="cb31-114" data-line-number="114">            loss.backward()</a>
<a class="sourceLine" id="cb31-115" data-line-number="115">            optimizer.step()</a>
<a class="sourceLine" id="cb31-116" data-line-number="116"></a>
<a class="sourceLine" id="cb31-117" data-line-number="117">            <span class="co"># visualization use t-SNE</span></a>
<a class="sourceLine" id="cb31-118" data-line-number="118">            <span class="cf">if</span> step <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span>:</a>
<a class="sourceLine" id="cb31-119" data-line-number="119">                test_output, last_layer <span class="op">=</span> cnn(test_x)</a>
<a class="sourceLine" id="cb31-120" data-line-number="120">                pred_y <span class="op">=</span> t.<span class="bu">max</span>(test_output, <span class="dv">1</span>)[<span class="dv">1</span>].data.numpy()</a>
<a class="sourceLine" id="cb31-121" data-line-number="121">                accuracy <span class="op">=</span> <span class="bu">float</span>((pred_y <span class="op">==</span> test_y.data.numpy()).astype(<span class="bu">int</span>).<span class="bu">sum</span>()) <span class="op">/</span> <span class="bu">float</span>(test_y.size(<span class="dv">0</span>))</a>
<a class="sourceLine" id="cb31-122" data-line-number="122">                <span class="bu">print</span>(<span class="st">&#39;Epoch: &#39;</span>, epoch, <span class="st">&#39;| train loss: </span><span class="sc">%.4f</span><span class="st">&#39;</span> <span class="op">%</span> loss.data.numpy(), <span class="st">&#39;| test accuracy: </span><span class="sc">%.3f</span><span class="st">&#39;</span> <span class="op">%</span> accuracy)</a>
<a class="sourceLine" id="cb31-123" data-line-number="123">                <span class="cf">if</span> HAS_SK:</a>
<a class="sourceLine" id="cb31-124" data-line-number="124">                    <span class="co"># Visualization of trained flatten layer (T-SNE)</span></a>
<a class="sourceLine" id="cb31-125" data-line-number="125">                    tsne <span class="op">=</span> TSNE(perplexity<span class="op">=</span><span class="dv">30</span>, n_components<span class="op">=</span><span class="dv">2</span>, init<span class="op">=</span><span class="st">&#39;pca&#39;</span>, n_iter<span class="op">=</span><span class="dv">5000</span>)</a>
<a class="sourceLine" id="cb31-126" data-line-number="126">                    plot_only <span class="op">=</span> <span class="dv">500</span></a>
<a class="sourceLine" id="cb31-127" data-line-number="127">                    low_dim_embs <span class="op">=</span> tsne.fit_transform(last_layer.data.numpy()[:plot_only, :])</a>
<a class="sourceLine" id="cb31-128" data-line-number="128">                    labels <span class="op">=</span> test_y.numpy()[:plot_only]</a>
<a class="sourceLine" id="cb31-129" data-line-number="129">                    plot_with_labels(low_dim_embs, labels, image_list, epoch, loss, accuracy)  <span class="co"># lowDWeights, labels</span></a>
<a class="sourceLine" id="cb31-130" data-line-number="130">    plt.ioff()</a>
<a class="sourceLine" id="cb31-131" data-line-number="131">    imageio.mimsave(<span class="st">&#39;CNN_MNIST.gif&#39;</span>, image_list, <span class="st">&#39;GIF&#39;</span>, duration<span class="op">=</span><span class="fl">0.2</span>)</a>
<a class="sourceLine" id="cb31-132" data-line-number="132"></a>
<a class="sourceLine" id="cb31-133" data-line-number="133">    t.save(cnn, <span class="st">&#39;./Try_ModelSaved/cnn_mnist.pkl&#39;</span>)  <span class="co"># 保存整个网络</span></a>
<a class="sourceLine" id="cb31-134" data-line-number="134"></a>
<a class="sourceLine" id="cb31-135" data-line-number="135"></a>
<a class="sourceLine" id="cb31-136" data-line-number="136"><span class="kw">def</span> Test():</a>
<a class="sourceLine" id="cb31-137" data-line-number="137">    net2 <span class="op">=</span> t.load(<span class="st">&#39;./Try_ModelSaved/cnn_mnist.pkl&#39;</span>)  <span class="co"># restore entire net1 to net2</span></a>
<a class="sourceLine" id="cb31-138" data-line-number="138">    test_output <span class="op">=</span> net2(test_x[:<span class="dv">10</span>])</a>
<a class="sourceLine" id="cb31-139" data-line-number="139">    pred_y <span class="op">=</span> t.<span class="bu">max</span>(test_output[<span class="dv">0</span>], <span class="dv">1</span>)[<span class="dv">1</span>].data.numpy().squeeze()</a>
<a class="sourceLine" id="cb31-140" data-line-number="140">    <span class="bu">print</span>(pred_y, <span class="st">&#39;prediction number&#39;</span>)</a>
<a class="sourceLine" id="cb31-141" data-line-number="141">    <span class="bu">print</span>(test_y[:<span class="dv">10</span>].numpy(), <span class="st">&#39;real number&#39;</span>)</a>
<a class="sourceLine" id="cb31-142" data-line-number="142"></a>
<a class="sourceLine" id="cb31-143" data-line-number="143"></a>
<a class="sourceLine" id="cb31-144" data-line-number="144">Train()</a>
<a class="sourceLine" id="cb31-145" data-line-number="145">Test()</a></code></pre></div></li>
</ol>
<h1 id="rnn-序列化分析">RNN — 序列化分析</h1>
<h2 id="介绍">介绍</h2>
<ol type="1">
<li>具有记忆之前知识的能力</li>
</ol>
<p><img src="/img/in-post/20_07/rnn4.png" alt="rnn4.png" style="zoom:50%;" /></p>
<ol start="2" type="1">
<li><p>梯度消失或者梯度爆炸的问题(取决于w大于1还是小于1)</p>
<p><img src="/img/in-post/20_07/lstm3.png" alt="lstm3.png" style="zoom:50%;" /></p>
<p><img src="/img/in-post/20_07/lstm4.png" alt="lstm4.png" style="zoom:50%;" /></p></li>
</ol>
<h1 id="lstm分类-延缓记忆衰退">LSTM分类 — 延缓记忆衰退</h1>
<p>带控制器 — 输入控制,输出控制,忘记控制</p>
<p><img src="/img/in-post/20_07/lstm5.png" alt="lstm5.png" style="zoom:50%;" /></p>
<ol type="1">
<li>参数相比CNN增加了2个, 横着读, 每次读一行(28个point), 读28次.</li>
</ol>
<div class="sourceCode" id="cb32"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb32-1" data-line-number="1">TIME_STEP <span class="op">=</span> <span class="dv">28</span>      <span class="co"># rnn 时间步数 / 图片高度</span></a>
<a class="sourceLine" id="cb32-2" data-line-number="2">INPUT_SIZE <span class="op">=</span> <span class="dv">28</span>     <span class="co"># rnn 每步输入值 / 图片每行像素</span></a></code></pre></div>
<p><img src="/img/in-post/20_07/4-1-1-20200805001304803.png" alt="4-1-1.png" style="zoom:50%;" /></p>
<ol start="2" type="1">
<li><p>RNN模型: 和以前一样, 我们用一个 class 来建立 RNN 模型. 这个 RNN 整体流程是</p></li>
<li><code>(input0, state0)</code> -&gt; <code>LSTM</code> -&gt; <code>(output0, state1)</code>;</li>
<li><code>(input1, state1)</code> -&gt; <code>LSTM</code> -&gt; <code>(output1, state2)</code>;</li>
<li>…</li>
<li><code>(inputN, stateN)</code>-&gt; <code>LSTM</code> -&gt; <code>(outputN, stateN+1)</code>;</li>
<li><p><code>outputN</code> -&gt; <code>Linear</code> -&gt; <code>prediction</code>. 通过<code>LSTM</code><strong>分析每一时刻的值</strong>, 并且将这一时刻和前面时刻的理解合并在一起, 生成当前时刻对前面数据的理解或记忆. 传递这种理解给下一时刻分析.</p></li>
</ol>
<p>输出:</p>
<pre><code>torch.Size([60000, 28, 28])
torch.Size([60000])
RNN(
  (rnn): LSTM(28, 64, batch_first=True)
  (out): Linear(in_features=64, out_features=10, bias=True)
)
Epoch:  0 | train loss: 2.2925 | test accuracy: 0.10
Epoch:  0 | train loss: 1.1347 | test accuracy: 0.55
Epoch:  0 | train loss: 1.0238 | test accuracy: 0.68
Epoch:  0 | train loss: 0.4887 | test accuracy: 0.81
Epoch:  0 | train loss: 0.4760 | test accuracy: 0.85
Epoch:  0 | train loss: 0.2999 | test accuracy: 0.86
Epoch:  0 | train loss: 0.3342 | test accuracy: 0.89
Epoch:  0 | train loss: 0.4210 | test accuracy: 0.93
Epoch:  0 | train loss: 0.1264 | test accuracy: 0.93
Epoch:  0 | train loss: 0.3789 | test accuracy: 0.93
Epoch:  0 | train loss: 0.2171 | test accuracy: 0.92
Epoch:  0 | train loss: 0.2132 | test accuracy: 0.94
Epoch:  0 | train loss: 0.2326 | test accuracy: 0.93
Epoch:  0 | train loss: 0.0600 | test accuracy: 0.94
Epoch:  0 | train loss: 0.2595 | test accuracy: 0.94
Epoch:  0 | train loss: 0.0987 | test accuracy: 0.95
Epoch:  0 | train loss: 0.1818 | test accuracy: 0.94
Epoch:  0 | train loss: 0.3106 | test accuracy: 0.96
Epoch:  0 | train loss: 0.1822 | test accuracy: 0.95
[7 2 1 0 4 1 4 9 5 9] prediction number
[7 2 1 0 4 1 4 9 5 9] real number</code></pre>
<p>完整程序:</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb34-1" data-line-number="1"><span class="im">import</span> os</a>
<a class="sourceLine" id="cb34-2" data-line-number="2"></a>
<a class="sourceLine" id="cb34-3" data-line-number="3">os.environ[<span class="st">&#39;KMP_DUPLICATE_LIB_OK&#39;</span>] <span class="op">=</span> <span class="st">&#39;True&#39;</span></a>
<a class="sourceLine" id="cb34-4" data-line-number="4"><span class="im">import</span> imageio</a>
<a class="sourceLine" id="cb34-5" data-line-number="5"><span class="im">from</span> tqdm <span class="im">import</span> tqdm</a>
<a class="sourceLine" id="cb34-6" data-line-number="6"><span class="im">import</span> torch <span class="im">as</span> t</a>
<a class="sourceLine" id="cb34-7" data-line-number="7"><span class="im">import</span> torch.nn <span class="im">as</span> nn</a>
<a class="sourceLine" id="cb34-8" data-line-number="8"><span class="im">import</span> torch.utils.data <span class="im">as</span> Data</a>
<a class="sourceLine" id="cb34-9" data-line-number="9"><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</a>
<a class="sourceLine" id="cb34-10" data-line-number="10"><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</a>
<a class="sourceLine" id="cb34-11" data-line-number="11"><span class="im">import</span> torchvision</a>
<a class="sourceLine" id="cb34-12" data-line-number="12"><span class="im">import</span> torchvision.transforms <span class="im">as</span> transforms</a>
<a class="sourceLine" id="cb34-13" data-line-number="13"></a>
<a class="sourceLine" id="cb34-14" data-line-number="14">plt.style.use(<span class="st">&#39;seaborn&#39;</span>)  <span class="co"># 设置使用的样式</span></a>
<a class="sourceLine" id="cb34-15" data-line-number="15"></a>
<a class="sourceLine" id="cb34-16" data-line-number="16"><span class="co"># for visualization</span></a>
<a class="sourceLine" id="cb34-17" data-line-number="17"><span class="im">from</span> matplotlib <span class="im">import</span> cm</a>
<a class="sourceLine" id="cb34-18" data-line-number="18"><span class="cf">try</span>:</a>
<a class="sourceLine" id="cb34-19" data-line-number="19">    <span class="im">from</span> sklearn.manifold <span class="im">import</span> TSNE<span class="op">;</span> HAS_SK <span class="op">=</span> <span class="va">True</span></a>
<a class="sourceLine" id="cb34-20" data-line-number="20"><span class="cf">except</span>:</a>
<a class="sourceLine" id="cb34-21" data-line-number="21">    HAS_SK <span class="op">=</span> <span class="va">False</span><span class="op">;</span> <span class="bu">print</span>(<span class="st">&#39;Please install sklearn for layer visualization&#39;</span>)</a>
<a class="sourceLine" id="cb34-22" data-line-number="22"></a>
<a class="sourceLine" id="cb34-23" data-line-number="23">LR <span class="op">=</span> <span class="fl">0.01</span></a>
<a class="sourceLine" id="cb34-24" data-line-number="24">BATCH_SIZE <span class="op">=</span> <span class="dv">64</span></a>
<a class="sourceLine" id="cb34-25" data-line-number="25">EPOCH <span class="op">=</span> <span class="dv">1</span></a>
<a class="sourceLine" id="cb34-26" data-line-number="26">TIME_STEP <span class="op">=</span> <span class="dv">28</span>      <span class="co"># rnn 时间步数 / 图片高度</span></a>
<a class="sourceLine" id="cb34-27" data-line-number="27">INPUT_SIZE <span class="op">=</span> <span class="dv">28</span>     <span class="co"># rnn 每步输入值 / 图片每行像素</span></a>
<a class="sourceLine" id="cb34-28" data-line-number="28"></a>
<a class="sourceLine" id="cb34-29" data-line-number="29">DOWNLOAD_MNIST <span class="op">=</span> <span class="va">False</span></a>
<a class="sourceLine" id="cb34-30" data-line-number="30"></a>
<a class="sourceLine" id="cb34-31" data-line-number="31"><span class="co">&quot;&quot;&quot;Mnist digits dataset&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb34-32" data-line-number="32"><span class="cf">if</span> <span class="kw">not</span>(os.path.exists(<span class="st">&#39;/Volumes/ArcFile/mnist/&#39;</span>)) <span class="kw">or</span> <span class="kw">not</span> os.listdir(<span class="st">&#39;/Volumes/ArcFile/mnist/&#39;</span>):</a>
<a class="sourceLine" id="cb34-33" data-line-number="33">    DOWNLOAD_MNIST <span class="op">=</span> <span class="va">True</span></a>
<a class="sourceLine" id="cb34-34" data-line-number="34"></a>
<a class="sourceLine" id="cb34-35" data-line-number="35">train_data <span class="op">=</span> torchvision.datasets.MNIST(</a>
<a class="sourceLine" id="cb34-36" data-line-number="36">    root<span class="op">=</span><span class="st">&#39;/Volumes/ArcFile/mnist/&#39;</span>,</a>
<a class="sourceLine" id="cb34-37" data-line-number="37">    train<span class="op">=</span><span class="va">True</span>,  <span class="co"># this is training data</span></a>
<a class="sourceLine" id="cb34-38" data-line-number="38">    <span class="co"># PIL.Image or numpy.ndarray to torch.FloatTensor(C * H * W), and normalize in the range [0.0, 1.0]</span></a>
<a class="sourceLine" id="cb34-39" data-line-number="39">    transform<span class="op">=</span>transforms.ToTensor(),</a>
<a class="sourceLine" id="cb34-40" data-line-number="40"></a>
<a class="sourceLine" id="cb34-41" data-line-number="41">    download<span class="op">=</span>DOWNLOAD_MNIST</a>
<a class="sourceLine" id="cb34-42" data-line-number="42">)</a>
<a class="sourceLine" id="cb34-43" data-line-number="43"><span class="co"># 批训练 50samples, 1 channel, 28x28 (50, 1, 28, 28)</span></a>
<a class="sourceLine" id="cb34-44" data-line-number="44">train_loader <span class="op">=</span> Data.DataLoader(dataset<span class="op">=</span>train_data, batch_size<span class="op">=</span>BATCH_SIZE, shuffle<span class="op">=</span><span class="va">True</span>)</a>
<a class="sourceLine" id="cb34-45" data-line-number="45"><span class="co"># plot one example</span></a>
<a class="sourceLine" id="cb34-46" data-line-number="46"><span class="bu">print</span>(train_data.train_data.size())                 <span class="co"># (60000, 28, 28)</span></a>
<a class="sourceLine" id="cb34-47" data-line-number="47"><span class="bu">print</span>(train_data.train_labels.size())               <span class="co"># (60000)</span></a>
<a class="sourceLine" id="cb34-48" data-line-number="48">plt.imshow(train_data.train_data[<span class="dv">0</span>].numpy(), cmap<span class="op">=</span><span class="st">&#39;gray&#39;</span>)</a>
<a class="sourceLine" id="cb34-49" data-line-number="49">plt.title(<span class="st">&#39;</span><span class="sc">%i</span><span class="st">&#39;</span> <span class="op">%</span> train_data.train_labels[<span class="dv">0</span>])</a>
<a class="sourceLine" id="cb34-50" data-line-number="50">plt.show()</a>
<a class="sourceLine" id="cb34-51" data-line-number="51"></a>
<a class="sourceLine" id="cb34-52" data-line-number="52"><span class="co"># pick 2000 samples to speed up testing</span></a>
<a class="sourceLine" id="cb34-53" data-line-number="53">test_data <span class="op">=</span> torchvision.datasets.MNIST(root<span class="op">=</span><span class="st">&#39;/Volumes/ArcFile/mnist/&#39;</span>, train<span class="op">=</span><span class="va">False</span>,  transform<span class="op">=</span>transforms.ToTensor())</a>
<a class="sourceLine" id="cb34-54" data-line-number="54"><span class="co"># shape from (2000, 28, 28) to (2000, 1, 28, 28), value in range(0,1)</span></a>
<a class="sourceLine" id="cb34-55" data-line-number="55">test_x <span class="op">=</span> test_data.test_data.<span class="bu">type</span>(t.FloatTensor)[:<span class="dv">2000</span>] <span class="op">/</span> <span class="fl">255.</span>      <span class="co"># 注意这里不需要unsqueeze()数据了,因为RNN只需要3个??</span></a>
<a class="sourceLine" id="cb34-56" data-line-number="56">test_y <span class="op">=</span> test_data.test_labels.numpy()[:<span class="dv">2000</span>]</a>
<a class="sourceLine" id="cb34-57" data-line-number="57"></a>
<a class="sourceLine" id="cb34-58" data-line-number="58"></a>
<a class="sourceLine" id="cb34-59" data-line-number="59"><span class="kw">class</span> RNN(t.nn.Module):</a>
<a class="sourceLine" id="cb34-60" data-line-number="60">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</a>
<a class="sourceLine" id="cb34-61" data-line-number="61">        <span class="bu">super</span>(RNN, <span class="va">self</span>).<span class="fu">__init__</span>()</a>
<a class="sourceLine" id="cb34-62" data-line-number="62">        <span class="va">self</span>.rnn <span class="op">=</span> nn.LSTM(  <span class="co"># LSTM 效果要比 nn.RNN() 好多了</span></a>
<a class="sourceLine" id="cb34-63" data-line-number="63">            input_size<span class="op">=</span><span class="dv">28</span>,  <span class="co"># 图片每行的数据像素点</span></a>
<a class="sourceLine" id="cb34-64" data-line-number="64">            hidden_size<span class="op">=</span><span class="dv">64</span>,  <span class="co"># rnn hidden unit</span></a>
<a class="sourceLine" id="cb34-65" data-line-number="65">            num_layers<span class="op">=</span><span class="dv">1</span>,  <span class="co"># 有几层 RNN layers</span></a>
<a class="sourceLine" id="cb34-66" data-line-number="66">            batch_first<span class="op">=</span><span class="va">True</span>,  <span class="co"># input &amp; output 会是以 batch size 为第一维度的特征集 e.g. (batch, time_step, input_size)</span></a>
<a class="sourceLine" id="cb34-67" data-line-number="67">        )</a>
<a class="sourceLine" id="cb34-68" data-line-number="68"></a>
<a class="sourceLine" id="cb34-69" data-line-number="69">        <span class="va">self</span>.out <span class="op">=</span> nn.Linear(<span class="dv">64</span>, <span class="dv">10</span>)  <span class="co"># 输出层</span></a>
<a class="sourceLine" id="cb34-70" data-line-number="70"></a>
<a class="sourceLine" id="cb34-71" data-line-number="71">    <span class="kw">def</span> forward(<span class="va">self</span>, x):</a>
<a class="sourceLine" id="cb34-72" data-line-number="72">        <span class="co"># x shape (batch, time_step, input_size)</span></a>
<a class="sourceLine" id="cb34-73" data-line-number="73">        <span class="co"># r_out shape (batch, time_step, output_size)</span></a>
<a class="sourceLine" id="cb34-74" data-line-number="74">        <span class="co"># h_n shape (n_layers, batch, hidden_size)   LSTM 有两个 hidden states, h_n 是分线, h_c 是主线</span></a>
<a class="sourceLine" id="cb34-75" data-line-number="75">        <span class="co"># h_c shape (n_layers, batch, hidden_size)</span></a>
<a class="sourceLine" id="cb34-76" data-line-number="76">        r_out, (h_n, h_c) <span class="op">=</span> <span class="va">self</span>.rnn(x, <span class="va">None</span>)  <span class="co"># None 表示 hidden state 会用全0的 state</span></a>
<a class="sourceLine" id="cb34-77" data-line-number="77"></a>
<a class="sourceLine" id="cb34-78" data-line-number="78">        <span class="co"># 选取最后一个时间点的 r_out 输出</span></a>
<a class="sourceLine" id="cb34-79" data-line-number="79">        <span class="co"># 这里 r_out[:, -1, :] 的值也是 h_n 的值</span></a>
<a class="sourceLine" id="cb34-80" data-line-number="80">        out <span class="op">=</span> <span class="va">self</span>.out(r_out[:, <span class="dv">-1</span>, :])</a>
<a class="sourceLine" id="cb34-81" data-line-number="81">        <span class="cf">return</span> out</a>
<a class="sourceLine" id="cb34-82" data-line-number="82"></a>
<a class="sourceLine" id="cb34-83" data-line-number="83">rnn <span class="op">=</span> RNN()</a>
<a class="sourceLine" id="cb34-84" data-line-number="84"><span class="bu">print</span>(rnn)</a>
<a class="sourceLine" id="cb34-85" data-line-number="85"><span class="co">&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb34-86" data-line-number="86"><span class="co">RNN (</span></a>
<a class="sourceLine" id="cb34-87" data-line-number="87"><span class="co">  (rnn): LSTM(28, 64, batch_first=True)</span></a>
<a class="sourceLine" id="cb34-88" data-line-number="88"><span class="co">  (out): Linear (64 -&gt; 10)</span></a>
<a class="sourceLine" id="cb34-89" data-line-number="89"><span class="co">)</span></a>
<a class="sourceLine" id="cb34-90" data-line-number="90"><span class="co">&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb34-91" data-line-number="91"></a>
<a class="sourceLine" id="cb34-92" data-line-number="92">optimizer <span class="op">=</span> t.optim.Adam(rnn.parameters())</a>
<a class="sourceLine" id="cb34-93" data-line-number="93">loss_func <span class="op">=</span> nn.CrossEntropyLoss()</a>
<a class="sourceLine" id="cb34-94" data-line-number="94"></a>
<a class="sourceLine" id="cb34-95" data-line-number="95"></a>
<a class="sourceLine" id="cb34-96" data-line-number="96"><span class="kw">def</span> plot_with_labels(lowDWeights, labels, image_list, <span class="op">*</span>param):</a>
<a class="sourceLine" id="cb34-97" data-line-number="97">    plt.cla()</a>
<a class="sourceLine" id="cb34-98" data-line-number="98">    X, Y <span class="op">=</span> lowDWeights[:, <span class="dv">0</span>], lowDWeights[:, <span class="dv">1</span>]</a>
<a class="sourceLine" id="cb34-99" data-line-number="99">    epoch, loss, accuracy <span class="op">=</span> param</a>
<a class="sourceLine" id="cb34-100" data-line-number="100">    <span class="cf">for</span> x, y, s <span class="kw">in</span> <span class="bu">zip</span>(X, Y, labels):</a>
<a class="sourceLine" id="cb34-101" data-line-number="101">        c <span class="op">=</span> cm.rainbow(<span class="bu">int</span>(<span class="dv">255</span> <span class="op">*</span> s <span class="op">/</span> <span class="dv">9</span>))</a>
<a class="sourceLine" id="cb34-102" data-line-number="102">        plt.text(x, y, s, backgroundcolor<span class="op">=</span>c, fontsize<span class="op">=</span><span class="dv">9</span>)</a>
<a class="sourceLine" id="cb34-103" data-line-number="103">    <span class="co"># plt.xlim(X.min(), X.max())</span></a>
<a class="sourceLine" id="cb34-104" data-line-number="104">    <span class="co"># plt.ylim(Y.min(), Y.max())</span></a>
<a class="sourceLine" id="cb34-105" data-line-number="105">    plt.xlim(<span class="op">-</span><span class="dv">50</span>, <span class="dv">50</span>)</a>
<a class="sourceLine" id="cb34-106" data-line-number="106">    plt.ylim(<span class="op">-</span><span class="dv">40</span>, <span class="dv">40</span>)</a>
<a class="sourceLine" id="cb34-107" data-line-number="107">    plt.title(<span class="st">&#39;Visualize last layer&#39;</span>)</a>
<a class="sourceLine" id="cb34-108" data-line-number="108">    text <span class="op">=</span> (<span class="st">&#39;Epoch: </span><span class="sc">%d</span><span class="st">,&#39;</span> <span class="op">%</span> epoch <span class="op">+</span> <span class="st">&#39; train loss: </span><span class="sc">%.4f</span><span class="st">,&#39;</span> <span class="op">%</span> loss.data.numpy() <span class="op">+</span> <span class="st">&#39; test accuracy: </span><span class="sc">%.3f</span><span class="st">&#39;</span> <span class="op">%</span> accuracy)</a>
<a class="sourceLine" id="cb34-109" data-line-number="109">    plt.text(<span class="op">-</span><span class="dv">30</span>, <span class="dv">-35</span>, text, fontdict<span class="op">=</span>{<span class="st">&#39;size&#39;</span>: <span class="dv">13</span>, <span class="st">&#39;color&#39;</span>: <span class="st">&#39;red&#39;</span>})</a>
<a class="sourceLine" id="cb34-110" data-line-number="110">    plt.show()</a>
<a class="sourceLine" id="cb34-111" data-line-number="111">    plt.savefig(<span class="st">&quot;temp.jpg&quot;</span>)</a>
<a class="sourceLine" id="cb34-112" data-line-number="112">    plt.pause(<span class="fl">0.02</span>)</a>
<a class="sourceLine" id="cb34-113" data-line-number="113">    image_list.append(imageio.imread(<span class="st">&quot;temp.jpg&quot;</span>))  <span class="co"># 可以不用循环i，直接用列表形式</span></a>
<a class="sourceLine" id="cb34-114" data-line-number="114"></a>
<a class="sourceLine" id="cb34-115" data-line-number="115"></a>
<a class="sourceLine" id="cb34-116" data-line-number="116"><span class="kw">def</span> Train():</a>
<a class="sourceLine" id="cb34-117" data-line-number="117">    plt.ion()</a>
<a class="sourceLine" id="cb34-118" data-line-number="118">    <span class="co"># plt.figure(figsize=(5,5))</span></a>
<a class="sourceLine" id="cb34-119" data-line-number="119">    <span class="co"># image_list = []</span></a>
<a class="sourceLine" id="cb34-120" data-line-number="120">    <span class="cf">for</span> epoch <span class="kw">in</span> tqdm(<span class="bu">range</span>(EPOCH)):</a>
<a class="sourceLine" id="cb34-121" data-line-number="121">        <span class="cf">for</span> step, (b_x, b_y) <span class="kw">in</span> <span class="bu">enumerate</span>(train_loader):</a>
<a class="sourceLine" id="cb34-122" data-line-number="122">            b_x <span class="op">=</span> b_x.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">28</span>, <span class="dv">28</span>)   <span class="co"># reshape x to (batch, time_step, input_size)</span></a>
<a class="sourceLine" id="cb34-123" data-line-number="123"></a>
<a class="sourceLine" id="cb34-124" data-line-number="124">            output <span class="op">=</span> rnn(b_x)  <span class="co"># 注意如果需要visualization时会有2个返回值,故要选择</span></a>
<a class="sourceLine" id="cb34-125" data-line-number="125"></a>
<a class="sourceLine" id="cb34-126" data-line-number="126">            loss <span class="op">=</span> loss_func(output, b_y)</a>
<a class="sourceLine" id="cb34-127" data-line-number="127">            optimizer.zero_grad()  <span class="co"># 计算完损失就可以清除梯度了</span></a>
<a class="sourceLine" id="cb34-128" data-line-number="128">            loss.backward()</a>
<a class="sourceLine" id="cb34-129" data-line-number="129">            optimizer.step()</a>
<a class="sourceLine" id="cb34-130" data-line-number="130"></a>
<a class="sourceLine" id="cb34-131" data-line-number="131">            <span class="co"># visualization use t-SNE</span></a>
<a class="sourceLine" id="cb34-132" data-line-number="132">            <span class="cf">if</span> step <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span>:</a>
<a class="sourceLine" id="cb34-133" data-line-number="133">                test_output <span class="op">=</span> rnn(test_x)  <span class="co"># x 赋值给 last_layer</span></a>
<a class="sourceLine" id="cb34-134" data-line-number="134">                pred_y <span class="op">=</span> t.<span class="bu">max</span>(test_output, <span class="dv">1</span>)[<span class="dv">1</span>].data.numpy()</a>
<a class="sourceLine" id="cb34-135" data-line-number="135">                accuracy <span class="op">=</span> <span class="bu">float</span>((pred_y <span class="op">==</span> test_y).astype(<span class="bu">int</span>).<span class="bu">sum</span>()) <span class="op">/</span> <span class="bu">float</span>(test_y.size)</a>
<a class="sourceLine" id="cb34-136" data-line-number="136">                <span class="bu">print</span>(<span class="st">&#39;Epoch: &#39;</span>, epoch, <span class="st">&#39;| train loss: </span><span class="sc">%.4f</span><span class="st">&#39;</span> <span class="op">%</span> loss.data.numpy(), <span class="st">&#39;| test accuracy: </span><span class="sc">%.3f</span><span class="st">&#39;</span> <span class="op">%</span> accuracy)</a>
<a class="sourceLine" id="cb34-137" data-line-number="137">    <span class="co">#             if HAS_SK:</span></a>
<a class="sourceLine" id="cb34-138" data-line-number="138">    <span class="co">#                 # Visualization of trained flatten layer (T-SNE)</span></a>
<a class="sourceLine" id="cb34-139" data-line-number="139">    <span class="co">#                 tsne = TSNE(perplexity=30, n_components=2, init=&#39;pca&#39;, n_iter=5000)</span></a>
<a class="sourceLine" id="cb34-140" data-line-number="140">    <span class="co">#                 plot_only = 500</span></a>
<a class="sourceLine" id="cb34-141" data-line-number="141">    <span class="co">#                 low_dim_embs = tsne.fit_transform(last_layer.data.numpy()[:plot_only, :])  # 通过最后一层的pixel进行t-SNE分类可视化</span></a>
<a class="sourceLine" id="cb34-142" data-line-number="142">    <span class="co">#                 labels = test_y.numpy()[:plot_only]</span></a>
<a class="sourceLine" id="cb34-143" data-line-number="143">    <span class="co">#                 plot_with_labels(low_dim_embs, labels, image_list, epoch, loss, accuracy)  # lowDWeights, labels</span></a>
<a class="sourceLine" id="cb34-144" data-line-number="144">    <span class="co"># plt.ioff()</span></a>
<a class="sourceLine" id="cb34-145" data-line-number="145">    <span class="co"># imageio.mimsave(&#39;RNN_MNIST.gif&#39;, image_list, &#39;GIF&#39;, duration=0.2)</span></a>
<a class="sourceLine" id="cb34-146" data-line-number="146"></a>
<a class="sourceLine" id="cb34-147" data-line-number="147">    t.save(rnn, <span class="st">&#39;./Try_ModelSaved/rnn_mnist.pkl&#39;</span>)  <span class="co"># 保存整个网络</span></a>
<a class="sourceLine" id="cb34-148" data-line-number="148"></a>
<a class="sourceLine" id="cb34-149" data-line-number="149"></a>
<a class="sourceLine" id="cb34-150" data-line-number="150"><span class="kw">def</span> Test():</a>
<a class="sourceLine" id="cb34-151" data-line-number="151">    net2 <span class="op">=</span> t.load(<span class="st">&#39;./Try_ModelSaved/rnn_mnist.pkl&#39;</span>)  <span class="co"># restore entire net1 to net2</span></a>
<a class="sourceLine" id="cb34-152" data-line-number="152">    test_output <span class="op">=</span> net2(test_x[:<span class="dv">10</span>])</a>
<a class="sourceLine" id="cb34-153" data-line-number="153">    pred_y <span class="op">=</span> t.<span class="bu">max</span>(test_output, <span class="dv">1</span>)[<span class="dv">1</span>].data.numpy().squeeze()</a>
<a class="sourceLine" id="cb34-154" data-line-number="154">    <span class="bu">print</span>(pred_y, <span class="st">&#39;prediction number&#39;</span>)</a>
<a class="sourceLine" id="cb34-155" data-line-number="155">    <span class="bu">print</span>(test_y[:<span class="dv">10</span>], <span class="st">&#39;real number&#39;</span>)</a>
<a class="sourceLine" id="cb34-156" data-line-number="156"></a>
<a class="sourceLine" id="cb34-157" data-line-number="157"></a>
<a class="sourceLine" id="cb34-158" data-line-number="158">Train()</a>
<a class="sourceLine" id="cb34-159" data-line-number="159">Test()</a>
<a class="sourceLine" id="cb34-160" data-line-number="160"></a>
<a class="sourceLine" id="cb34-161" data-line-number="161"></a>
<a class="sourceLine" id="cb34-162" data-line-number="162"><span class="im">import</span> torch</a>
<a class="sourceLine" id="cb34-163" data-line-number="163"><span class="im">from</span> torch <span class="im">import</span> nn</a>
<a class="sourceLine" id="cb34-164" data-line-number="164"><span class="im">import</span> torchvision.datasets <span class="im">as</span> dsets</a>
<a class="sourceLine" id="cb34-165" data-line-number="165"><span class="im">import</span> torchvision.transforms <span class="im">as</span> transforms</a>
<a class="sourceLine" id="cb34-166" data-line-number="166"><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</a>
<a class="sourceLine" id="cb34-167" data-line-number="167"></a>
<a class="sourceLine" id="cb34-168" data-line-number="168"></a>
<a class="sourceLine" id="cb34-169" data-line-number="169"><span class="co"># torch.manual_seed(1)    # reproducible</span></a>
<a class="sourceLine" id="cb34-170" data-line-number="170"></a>
<a class="sourceLine" id="cb34-171" data-line-number="171"><span class="co"># Hyper Parameters</span></a>
<a class="sourceLine" id="cb34-172" data-line-number="172">EPOCH <span class="op">=</span> <span class="dv">1</span>               <span class="co"># train the training data n times, to save time, we just train 1 epoch</span></a>
<a class="sourceLine" id="cb34-173" data-line-number="173">BATCH_SIZE <span class="op">=</span> <span class="dv">64</span></a>
<a class="sourceLine" id="cb34-174" data-line-number="174">TIME_STEP <span class="op">=</span> <span class="dv">28</span>          <span class="co"># rnn time step / image height</span></a>
<a class="sourceLine" id="cb34-175" data-line-number="175">INPUT_SIZE <span class="op">=</span> <span class="dv">28</span>         <span class="co"># rnn input size / image width</span></a>
<a class="sourceLine" id="cb34-176" data-line-number="176">LR <span class="op">=</span> <span class="fl">0.01</span>               <span class="co"># learning rate</span></a>
<a class="sourceLine" id="cb34-177" data-line-number="177">DOWNLOAD_MNIST <span class="op">=</span> <span class="va">True</span>   <span class="co"># set to True if haven&#39;t download the data</span></a>
<a class="sourceLine" id="cb34-178" data-line-number="178"></a>
<a class="sourceLine" id="cb34-179" data-line-number="179"></a>
<a class="sourceLine" id="cb34-180" data-line-number="180"><span class="co"># Mnist digital dataset</span></a>
<a class="sourceLine" id="cb34-181" data-line-number="181">train_data <span class="op">=</span> dsets.MNIST(</a>
<a class="sourceLine" id="cb34-182" data-line-number="182">    root<span class="op">=</span><span class="st">&#39;./mnist/&#39;</span>,</a>
<a class="sourceLine" id="cb34-183" data-line-number="183">    train<span class="op">=</span><span class="va">True</span>,                         <span class="co"># this is training data</span></a>
<a class="sourceLine" id="cb34-184" data-line-number="184">    transform<span class="op">=</span>transforms.ToTensor(),    <span class="co"># Converts a PIL.Image or numpy.ndarray to</span></a>
<a class="sourceLine" id="cb34-185" data-line-number="185">                                        <span class="co"># torch.FloatTensor of shape (C x H x W) and normalize in the range [0.0, 1.0]</span></a>
<a class="sourceLine" id="cb34-186" data-line-number="186">    download<span class="op">=</span>DOWNLOAD_MNIST,            <span class="co"># download it if you don&#39;t have it</span></a>
<a class="sourceLine" id="cb34-187" data-line-number="187">)</a>
<a class="sourceLine" id="cb34-188" data-line-number="188"></a>
<a class="sourceLine" id="cb34-189" data-line-number="189"><span class="co"># plot one example</span></a>
<a class="sourceLine" id="cb34-190" data-line-number="190"><span class="bu">print</span>(train_data.train_data.size())     <span class="co"># (60000, 28, 28)</span></a>
<a class="sourceLine" id="cb34-191" data-line-number="191"><span class="bu">print</span>(train_data.train_labels.size())   <span class="co"># (60000)</span></a>
<a class="sourceLine" id="cb34-192" data-line-number="192">plt.imshow(train_data.train_data[<span class="dv">0</span>].numpy(), cmap<span class="op">=</span><span class="st">&#39;gray&#39;</span>)</a>
<a class="sourceLine" id="cb34-193" data-line-number="193">plt.title(<span class="st">&#39;</span><span class="sc">%i</span><span class="st">&#39;</span> <span class="op">%</span> train_data.train_labels[<span class="dv">0</span>])</a>
<a class="sourceLine" id="cb34-194" data-line-number="194">plt.show()</a>
<a class="sourceLine" id="cb34-195" data-line-number="195"></a>
<a class="sourceLine" id="cb34-196" data-line-number="196"><span class="co"># Data Loader for easy mini-batch return in training</span></a>
<a class="sourceLine" id="cb34-197" data-line-number="197">train_loader <span class="op">=</span> torch.utils.data.DataLoader(dataset<span class="op">=</span>train_data, batch_size<span class="op">=</span>BATCH_SIZE, shuffle<span class="op">=</span><span class="va">True</span>)</a>
<a class="sourceLine" id="cb34-198" data-line-number="198"></a>
<a class="sourceLine" id="cb34-199" data-line-number="199"><span class="co"># convert test data into Variable, pick 2000 samples to speed up testing</span></a>
<a class="sourceLine" id="cb34-200" data-line-number="200">test_data <span class="op">=</span> dsets.MNIST(root<span class="op">=</span><span class="st">&#39;./mnist/&#39;</span>, train<span class="op">=</span><span class="va">False</span>, transform<span class="op">=</span>transforms.ToTensor())</a>
<a class="sourceLine" id="cb34-201" data-line-number="201">test_x <span class="op">=</span> test_data.test_data.<span class="bu">type</span>(torch.FloatTensor)[:<span class="dv">2000</span>]<span class="op">/</span><span class="fl">255.</span>   <span class="co"># shape (2000, 28, 28) value in range(0,1)</span></a>
<a class="sourceLine" id="cb34-202" data-line-number="202">test_y <span class="op">=</span> test_data.test_labels.numpy()[:<span class="dv">2000</span>]    <span class="co"># covert to numpy array</span></a>
<a class="sourceLine" id="cb34-203" data-line-number="203"></a>
<a class="sourceLine" id="cb34-204" data-line-number="204"></a>
<a class="sourceLine" id="cb34-205" data-line-number="205"><span class="kw">class</span> RNN(nn.Module):</a>
<a class="sourceLine" id="cb34-206" data-line-number="206">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</a>
<a class="sourceLine" id="cb34-207" data-line-number="207">        <span class="bu">super</span>(RNN, <span class="va">self</span>).<span class="fu">__init__</span>()</a>
<a class="sourceLine" id="cb34-208" data-line-number="208"></a>
<a class="sourceLine" id="cb34-209" data-line-number="209">        <span class="va">self</span>.rnn <span class="op">=</span> nn.LSTM(         <span class="co"># if use nn.RNN(), it hardly learns</span></a>
<a class="sourceLine" id="cb34-210" data-line-number="210">            input_size<span class="op">=</span>INPUT_SIZE,</a>
<a class="sourceLine" id="cb34-211" data-line-number="211">            hidden_size<span class="op">=</span><span class="dv">64</span>,         <span class="co"># rnn hidden unit</span></a>
<a class="sourceLine" id="cb34-212" data-line-number="212">            num_layers<span class="op">=</span><span class="dv">1</span>,           <span class="co"># number of rnn layer</span></a>
<a class="sourceLine" id="cb34-213" data-line-number="213">            batch_first<span class="op">=</span><span class="va">True</span>,       <span class="co"># input &amp; output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)</span></a>
<a class="sourceLine" id="cb34-214" data-line-number="214">        )</a>
<a class="sourceLine" id="cb34-215" data-line-number="215"></a>
<a class="sourceLine" id="cb34-216" data-line-number="216">        <span class="va">self</span>.out <span class="op">=</span> nn.Linear(<span class="dv">64</span>, <span class="dv">10</span>)</a>
<a class="sourceLine" id="cb34-217" data-line-number="217"></a>
<a class="sourceLine" id="cb34-218" data-line-number="218">    <span class="kw">def</span> forward(<span class="va">self</span>, x):</a>
<a class="sourceLine" id="cb34-219" data-line-number="219">        <span class="co"># x shape (batch, time_step, input_size)</span></a>
<a class="sourceLine" id="cb34-220" data-line-number="220">        <span class="co"># r_out shape (batch, time_step, output_size)</span></a>
<a class="sourceLine" id="cb34-221" data-line-number="221">        <span class="co"># h_n shape (n_layers, batch, hidden_size)</span></a>
<a class="sourceLine" id="cb34-222" data-line-number="222">        <span class="co"># h_c shape (n_layers, batch, hidden_size)</span></a>
<a class="sourceLine" id="cb34-223" data-line-number="223">        r_out, (h_n, h_c) <span class="op">=</span> <span class="va">self</span>.rnn(x, <span class="va">None</span>)   <span class="co"># None represents zero initial hidden state</span></a>
<a class="sourceLine" id="cb34-224" data-line-number="224"></a>
<a class="sourceLine" id="cb34-225" data-line-number="225">        <span class="co"># choose r_out at the last time step</span></a>
<a class="sourceLine" id="cb34-226" data-line-number="226">        out <span class="op">=</span> <span class="va">self</span>.out(r_out[:, <span class="dv">-1</span>, :])</a>
<a class="sourceLine" id="cb34-227" data-line-number="227">        <span class="cf">return</span> out</a>
<a class="sourceLine" id="cb34-228" data-line-number="228"></a>
<a class="sourceLine" id="cb34-229" data-line-number="229"></a>
<a class="sourceLine" id="cb34-230" data-line-number="230">rnn <span class="op">=</span> RNN()</a>
<a class="sourceLine" id="cb34-231" data-line-number="231"><span class="bu">print</span>(rnn)</a>
<a class="sourceLine" id="cb34-232" data-line-number="232"></a>
<a class="sourceLine" id="cb34-233" data-line-number="233">optimizer <span class="op">=</span> torch.optim.Adam(rnn.parameters(), lr<span class="op">=</span>LR)   <span class="co"># optimize all cnn parameters</span></a>
<a class="sourceLine" id="cb34-234" data-line-number="234">loss_func <span class="op">=</span> nn.CrossEntropyLoss()                       <span class="co"># the target label is not one-hotted</span></a>
<a class="sourceLine" id="cb34-235" data-line-number="235"></a>
<a class="sourceLine" id="cb34-236" data-line-number="236"><span class="co"># training and testing</span></a>
<a class="sourceLine" id="cb34-237" data-line-number="237"><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(EPOCH):</a>
<a class="sourceLine" id="cb34-238" data-line-number="238">    <span class="cf">for</span> step, (b_x, b_y) <span class="kw">in</span> <span class="bu">enumerate</span>(train_loader):        <span class="co"># gives batch data</span></a>
<a class="sourceLine" id="cb34-239" data-line-number="239">        b_x <span class="op">=</span> b_x.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">28</span>, <span class="dv">28</span>)              <span class="co"># reshape x to (batch, time_step, input_size)</span></a>
<a class="sourceLine" id="cb34-240" data-line-number="240"></a>
<a class="sourceLine" id="cb34-241" data-line-number="241">        output <span class="op">=</span> rnn(b_x)                               <span class="co"># rnn output</span></a>
<a class="sourceLine" id="cb34-242" data-line-number="242">        loss <span class="op">=</span> loss_func(output, b_y)                   <span class="co"># cross entropy loss</span></a>
<a class="sourceLine" id="cb34-243" data-line-number="243">        optimizer.zero_grad()                           <span class="co"># clear gradients for this training step</span></a>
<a class="sourceLine" id="cb34-244" data-line-number="244">        loss.backward()                                 <span class="co"># backpropagation, compute gradients</span></a>
<a class="sourceLine" id="cb34-245" data-line-number="245">        optimizer.step()                                <span class="co"># apply gradients</span></a>
<a class="sourceLine" id="cb34-246" data-line-number="246"></a>
<a class="sourceLine" id="cb34-247" data-line-number="247">        <span class="cf">if</span> step <span class="op">%</span> <span class="dv">50</span> <span class="op">==</span> <span class="dv">0</span>:</a>
<a class="sourceLine" id="cb34-248" data-line-number="248">            test_output <span class="op">=</span> rnn(test_x)                   <span class="co"># (samples, time_step, input_size)</span></a>
<a class="sourceLine" id="cb34-249" data-line-number="249">            pred_y <span class="op">=</span> torch.<span class="bu">max</span>(test_output, <span class="dv">1</span>)[<span class="dv">1</span>].data.numpy()</a>
<a class="sourceLine" id="cb34-250" data-line-number="250">            accuracy <span class="op">=</span> <span class="bu">float</span>((pred_y <span class="op">==</span> test_y).astype(<span class="bu">int</span>).<span class="bu">sum</span>()) <span class="op">/</span> <span class="bu">float</span>(test_y.size)</a>
<a class="sourceLine" id="cb34-251" data-line-number="251">            <span class="bu">print</span>(<span class="st">&#39;Epoch: &#39;</span>, epoch, <span class="st">&#39;| train loss: </span><span class="sc">%.4f</span><span class="st">&#39;</span> <span class="op">%</span> loss.data.numpy(), <span class="st">&#39;| test accuracy: </span><span class="sc">%.2f</span><span class="st">&#39;</span> <span class="op">%</span> accuracy)</a>
<a class="sourceLine" id="cb34-252" data-line-number="252"></a>
<a class="sourceLine" id="cb34-253" data-line-number="253"><span class="co"># print 10 predictions from test data</span></a>
<a class="sourceLine" id="cb34-254" data-line-number="254">test_output <span class="op">=</span> rnn(test_x[:<span class="dv">10</span>].view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">28</span>, <span class="dv">28</span>))</a>
<a class="sourceLine" id="cb34-255" data-line-number="255">pred_y <span class="op">=</span> torch.<span class="bu">max</span>(test_output, <span class="dv">1</span>)[<span class="dv">1</span>].data.numpy()</a>
<a class="sourceLine" id="cb34-256" data-line-number="256"><span class="bu">print</span>(pred_y, <span class="st">&#39;prediction number&#39;</span>)</a>
<a class="sourceLine" id="cb34-257" data-line-number="257"><span class="bu">print</span>(test_y[:<span class="dv">10</span>], <span class="st">&#39;real number&#39;</span>)</a></code></pre></div>
<h1 id="rnn回归">RNN回归</h1>
<p><img src="/img/in-post/20_07/RNN_LR.gif" alt="RNN_LR" style="zoom:50%;" /></p>
<h2 id="注意">注意</h2>
<ol type="1">
<li><p>前向传播过程</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb35-1" data-line-number="1"><span class="kw">def</span> forward(<span class="va">self</span>, x, h_state):</a>
<a class="sourceLine" id="cb35-2" data-line-number="2">    <span class="co"># 因为 hidden state 是连续的, 所以我们要一直传递这一个 state</span></a>
<a class="sourceLine" id="cb35-3" data-line-number="3">    <span class="co"># x (batch, time_step, input_size)</span></a>
<a class="sourceLine" id="cb35-4" data-line-number="4">    <span class="co"># h_state (n_layers, batch, hidden_size)</span></a>
<a class="sourceLine" id="cb35-5" data-line-number="5">    <span class="co"># r_out (batch, time_step, output_size)</span></a>
<a class="sourceLine" id="cb35-6" data-line-number="6">    <span class="co"># h_state是之前事情的记忆, 也要作为 RNN 的一个输入</span></a>
<a class="sourceLine" id="cb35-7" data-line-number="7">    r_out, h_state <span class="op">=</span> <span class="va">self</span>.rnn(x, h_state)</a>
<a class="sourceLine" id="cb35-8" data-line-number="8"></a>
<a class="sourceLine" id="cb35-9" data-line-number="9">    outs <span class="op">=</span> []    <span class="co"># 保存所有时间点的预测值, 利用动态图</span></a>
<a class="sourceLine" id="cb35-10" data-line-number="10">    <span class="cf">for</span> time_step <span class="kw">in</span> <span class="bu">range</span>(r_out.size(<span class="dv">1</span>)):    <span class="co"># 对每一个时间点计算 output</span></a>
<a class="sourceLine" id="cb35-11" data-line-number="11">        outs.append(<span class="va">self</span>.out(r_out[:, time_step, :]))  <span class="co"># 每个time_step计算</span></a>
<a class="sourceLine" id="cb35-12" data-line-number="12">    <span class="cf">return</span> t.stack(outs, dim<span class="op">=</span><span class="dv">1</span>), h_state    <span class="co"># t.stack(): list -&gt; tensor</span></a></code></pre></div></li>
<li><p>预测过程</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb36-1" data-line-number="1">    prediction, h_state <span class="op">=</span> rnn(x, h_state)  <span class="co"># rnn 对于每个 step 的 prediction, 还有最后一个 step 的 h_state</span></a>
<a class="sourceLine" id="cb36-2" data-line-number="2">    <span class="co"># !!  下一步十分重要 !! 否则无法反向传播</span></a>
<a class="sourceLine" id="cb36-3" data-line-number="3">    h_state <span class="op">=</span> h_state.data  <span class="co"># 要把 h_state 重新包装一下才能放入下一个 iteration, 不然会报错</span></a></code></pre></div></li>
</ol>
<p>完整代码:</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb37-1" data-line-number="1"><span class="im">import</span> os</a>
<a class="sourceLine" id="cb37-2" data-line-number="2"></a>
<a class="sourceLine" id="cb37-3" data-line-number="3">os.environ[<span class="st">&#39;KMP_DUPLICATE_LIB_OK&#39;</span>] <span class="op">=</span> <span class="st">&#39;True&#39;</span></a>
<a class="sourceLine" id="cb37-4" data-line-number="4"><span class="im">import</span> imageio</a>
<a class="sourceLine" id="cb37-5" data-line-number="5"><span class="im">from</span> tqdm <span class="im">import</span> tqdm</a>
<a class="sourceLine" id="cb37-6" data-line-number="6"><span class="im">import</span> torch <span class="im">as</span> t</a>
<a class="sourceLine" id="cb37-7" data-line-number="7"><span class="im">import</span> torch.nn <span class="im">as</span> nn</a>
<a class="sourceLine" id="cb37-8" data-line-number="8"><span class="im">import</span> torch.utils.data <span class="im">as</span> Data</a>
<a class="sourceLine" id="cb37-9" data-line-number="9"><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</a>
<a class="sourceLine" id="cb37-10" data-line-number="10"><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</a>
<a class="sourceLine" id="cb37-11" data-line-number="11"><span class="im">import</span> torchvision</a>
<a class="sourceLine" id="cb37-12" data-line-number="12"><span class="im">import</span> torchvision.transforms <span class="im">as</span> transforms</a>
<a class="sourceLine" id="cb37-13" data-line-number="13"><span class="im">import</span> numpy <span class="im">as</span> np</a>
<a class="sourceLine" id="cb37-14" data-line-number="14"></a>
<a class="sourceLine" id="cb37-15" data-line-number="15">plt.style.use(<span class="st">&#39;seaborn&#39;</span>)</a>
<a class="sourceLine" id="cb37-16" data-line-number="16"></a>
<a class="sourceLine" id="cb37-17" data-line-number="17">LR <span class="op">=</span> <span class="fl">0.02</span></a>
<a class="sourceLine" id="cb37-18" data-line-number="18">BATCH_SIZE <span class="op">=</span> <span class="dv">64</span></a>
<a class="sourceLine" id="cb37-19" data-line-number="19">TIME_STEP <span class="op">=</span> <span class="dv">10</span>      <span class="co"># rnn 时间步数 / 图片高度</span></a>
<a class="sourceLine" id="cb37-20" data-line-number="20">INPUT_SIZE <span class="op">=</span> <span class="dv">1</span>     <span class="co"># rnn 每步输入值 / 图片每行像素</span></a>
<a class="sourceLine" id="cb37-21" data-line-number="21"></a>
<a class="sourceLine" id="cb37-22" data-line-number="22"><span class="co"># show data</span></a>
<a class="sourceLine" id="cb37-23" data-line-number="23">steps <span class="op">=</span> np.linspace(<span class="dv">0</span>, np.pi <span class="op">*</span> <span class="dv">2</span>, <span class="dv">100</span>, dtype<span class="op">=</span>np.float32)  <span class="co"># float32 for converting torch FloatTensor</span></a>
<a class="sourceLine" id="cb37-24" data-line-number="24">x_np <span class="op">=</span> np.sin(steps)</a>
<a class="sourceLine" id="cb37-25" data-line-number="25">y_np <span class="op">=</span> np.cos(steps)</a>
<a class="sourceLine" id="cb37-26" data-line-number="26">plt.plot(steps, y_np, <span class="st">&#39;r-&#39;</span>, label<span class="op">=</span><span class="st">&#39;target (cos)&#39;</span>)</a>
<a class="sourceLine" id="cb37-27" data-line-number="27">plt.plot(steps, x_np, <span class="st">&#39;b-&#39;</span>, label<span class="op">=</span><span class="st">&#39;input (sin)&#39;</span>)</a>
<a class="sourceLine" id="cb37-28" data-line-number="28">plt.legend(loc<span class="op">=</span><span class="st">&#39;best&#39;</span>)</a>
<a class="sourceLine" id="cb37-29" data-line-number="29">plt.show()</a>
<a class="sourceLine" id="cb37-30" data-line-number="30"></a>
<a class="sourceLine" id="cb37-31" data-line-number="31"></a>
<a class="sourceLine" id="cb37-32" data-line-number="32"><span class="kw">class</span> RNN(t.nn.Module):</a>
<a class="sourceLine" id="cb37-33" data-line-number="33">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</a>
<a class="sourceLine" id="cb37-34" data-line-number="34">        <span class="bu">super</span>(RNN, <span class="va">self</span>).<span class="fu">__init__</span>()</a>
<a class="sourceLine" id="cb37-35" data-line-number="35">        <span class="va">self</span>.rnn <span class="op">=</span> nn.RNN(      <span class="co"># 注意这里是RNN就够了</span></a>
<a class="sourceLine" id="cb37-36" data-line-number="36">            input_size<span class="op">=</span>INPUT_SIZE,  <span class="co"># 函数幅度是1</span></a>
<a class="sourceLine" id="cb37-37" data-line-number="37">            hidden_size<span class="op">=</span><span class="dv">32</span>,  <span class="co"># rnn hidden unit</span></a>
<a class="sourceLine" id="cb37-38" data-line-number="38">            num_layers<span class="op">=</span><span class="dv">1</span>,  <span class="co"># 有几层 RNN layers</span></a>
<a class="sourceLine" id="cb37-39" data-line-number="39">            batch_first<span class="op">=</span><span class="va">True</span>,  <span class="co"># input &amp; output 会是以 batch size 为第一维度的特征集 e.g. (batch, time_step, input_size)</span></a>
<a class="sourceLine" id="cb37-40" data-line-number="40">        )</a>
<a class="sourceLine" id="cb37-41" data-line-number="41">        <span class="va">self</span>.out <span class="op">=</span> nn.Linear(<span class="dv">32</span>, <span class="dv">1</span>)  <span class="co"># 输出层</span></a>
<a class="sourceLine" id="cb37-42" data-line-number="42"></a>
<a class="sourceLine" id="cb37-43" data-line-number="43">    <span class="kw">def</span> forward(<span class="va">self</span>, x, h_state):</a>
<a class="sourceLine" id="cb37-44" data-line-number="44">        <span class="co"># 因为 hidden state 是连续的, 所以我们要一直传递这一个 state</span></a>
<a class="sourceLine" id="cb37-45" data-line-number="45">        <span class="co"># x (batch, time_step, input_size)</span></a>
<a class="sourceLine" id="cb37-46" data-line-number="46">        <span class="co"># h_state (n_layers, batch, hidden_size)</span></a>
<a class="sourceLine" id="cb37-47" data-line-number="47">        <span class="co"># r_out (batch, time_step, output_size)</span></a>
<a class="sourceLine" id="cb37-48" data-line-number="48">        <span class="co"># h_state是之前事情的记忆, 也要作为 RNN 的一个输入</span></a>
<a class="sourceLine" id="cb37-49" data-line-number="49">        r_out, h_state <span class="op">=</span> <span class="va">self</span>.rnn(x, h_state)</a>
<a class="sourceLine" id="cb37-50" data-line-number="50"></a>
<a class="sourceLine" id="cb37-51" data-line-number="51">        outs <span class="op">=</span> []    <span class="co"># 保存所有时间点的预测值, 利用动态图</span></a>
<a class="sourceLine" id="cb37-52" data-line-number="52">        <span class="cf">for</span> time_step <span class="kw">in</span> <span class="bu">range</span>(r_out.size(<span class="dv">1</span>)):    <span class="co"># 对每一个时间点计算 output</span></a>
<a class="sourceLine" id="cb37-53" data-line-number="53">            outs.append(<span class="va">self</span>.out(r_out[:, time_step, :]))  <span class="co"># 每个time_step计算</span></a>
<a class="sourceLine" id="cb37-54" data-line-number="54">        <span class="cf">return</span> t.stack(outs, dim<span class="op">=</span><span class="dv">1</span>), h_state    <span class="co"># t.stack(): list -&gt; tensor</span></a>
<a class="sourceLine" id="cb37-55" data-line-number="55"><span class="co">#</span></a>
<a class="sourceLine" id="cb37-56" data-line-number="56">rnn <span class="op">=</span> RNN()</a>
<a class="sourceLine" id="cb37-57" data-line-number="57"><span class="bu">print</span>(rnn)</a>
<a class="sourceLine" id="cb37-58" data-line-number="58"><span class="co">&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb37-59" data-line-number="59"><span class="co">RNN (</span></a>
<a class="sourceLine" id="cb37-60" data-line-number="60"><span class="co">  (rnn): RNN(1, 32, batch_first=True)</span></a>
<a class="sourceLine" id="cb37-61" data-line-number="61"><span class="co">  (out): Linear (32 -&gt; 1)</span></a>
<a class="sourceLine" id="cb37-62" data-line-number="62"><span class="co">)</span></a>
<a class="sourceLine" id="cb37-63" data-line-number="63"><span class="co">&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb37-64" data-line-number="64"></a>
<a class="sourceLine" id="cb37-65" data-line-number="65">optimizer <span class="op">=</span> t.optim.Adam(rnn.parameters(), lr<span class="op">=</span>LR)</a>
<a class="sourceLine" id="cb37-66" data-line-number="66">loss_func <span class="op">=</span> nn.MSELoss()</a>
<a class="sourceLine" id="cb37-67" data-line-number="67"></a>
<a class="sourceLine" id="cb37-68" data-line-number="68"></a>
<a class="sourceLine" id="cb37-69" data-line-number="69"><span class="kw">def</span> Train():</a>
<a class="sourceLine" id="cb37-70" data-line-number="70">    plt.ion()</a>
<a class="sourceLine" id="cb37-71" data-line-number="71">    h_state <span class="op">=</span> <span class="va">None</span>  <span class="co"># for initial hidden state, 因为最初没有前面的数据</span></a>
<a class="sourceLine" id="cb37-72" data-line-number="72">    plt.figure(<span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">5</span>))</a>
<a class="sourceLine" id="cb37-73" data-line-number="73">    image_list <span class="op">=</span> []</a>
<a class="sourceLine" id="cb37-74" data-line-number="74">    <span class="cf">for</span> step <span class="kw">in</span> tqdm(<span class="bu">range</span>(<span class="dv">100</span>)):</a>
<a class="sourceLine" id="cb37-75" data-line-number="75">        start, end <span class="op">=</span> step <span class="op">*</span> np.pi, (step <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> np.pi  <span class="co"># time steps, 截取一小段起始位置</span></a>
<a class="sourceLine" id="cb37-76" data-line-number="76">        <span class="co"># sin 预测 cos</span></a>
<a class="sourceLine" id="cb37-77" data-line-number="77">        steps <span class="op">=</span> np.linspace(start, end, <span class="dv">10</span>, dtype<span class="op">=</span>np.float32, endpoint<span class="op">=</span><span class="va">False</span>)</a>
<a class="sourceLine" id="cb37-78" data-line-number="78">        x_np <span class="op">=</span> np.sin(steps)  <span class="co"># float32 for converting torch FloatTensor</span></a>
<a class="sourceLine" id="cb37-79" data-line-number="79">        y_np <span class="op">=</span> np.cos(steps)</a>
<a class="sourceLine" id="cb37-80" data-line-number="80"></a>
<a class="sourceLine" id="cb37-81" data-line-number="81">        x <span class="op">=</span> t.from_numpy(x_np[np.newaxis, :, np.newaxis])  <span class="co"># shape (batch, time_step, input_size) 加1个维度</span></a>
<a class="sourceLine" id="cb37-82" data-line-number="82">        y <span class="op">=</span> t.from_numpy(y_np[np.newaxis, :, np.newaxis])</a>
<a class="sourceLine" id="cb37-83" data-line-number="83"></a>
<a class="sourceLine" id="cb37-84" data-line-number="84">        prediction, h_state <span class="op">=</span> rnn(x, h_state)  <span class="co"># rnn 对于每个 step 的 prediction, 还有最后一个 step 的 h_state</span></a>
<a class="sourceLine" id="cb37-85" data-line-number="85">        <span class="co"># !!  下一步十分重要 !! 否则无法反向传播</span></a>
<a class="sourceLine" id="cb37-86" data-line-number="86">        h_state <span class="op">=</span> h_state.data  <span class="co"># 要把 h_state 重新包装一下才能放入下一个 iteration, 不然会报错</span></a>
<a class="sourceLine" id="cb37-87" data-line-number="87"></a>
<a class="sourceLine" id="cb37-88" data-line-number="88"></a>
<a class="sourceLine" id="cb37-89" data-line-number="89">        loss <span class="op">=</span> loss_func(prediction, y)</a>
<a class="sourceLine" id="cb37-90" data-line-number="90">        optimizer.zero_grad()</a>
<a class="sourceLine" id="cb37-91" data-line-number="91">        loss.backward()</a>
<a class="sourceLine" id="cb37-92" data-line-number="92">        optimizer.step()</a>
<a class="sourceLine" id="cb37-93" data-line-number="93"></a>
<a class="sourceLine" id="cb37-94" data-line-number="94">        plt.plot(steps, y_np.flatten(), <span class="st">&#39;r-&#39;</span>)</a>
<a class="sourceLine" id="cb37-95" data-line-number="95">        plt.plot(steps, prediction.data.numpy().flatten(), <span class="st">&#39;b-&#39;</span>)</a>
<a class="sourceLine" id="cb37-96" data-line-number="96">        plt.draw()</a>
<a class="sourceLine" id="cb37-97" data-line-number="97">        plt.savefig(<span class="st">&#39;temp.jpg&#39;</span>)</a>
<a class="sourceLine" id="cb37-98" data-line-number="98">        plt.pause(<span class="fl">0.05</span>)</a>
<a class="sourceLine" id="cb37-99" data-line-number="99">        <span class="cf">if</span> step <span class="op">%</span> <span class="dv">4</span> <span class="op">==</span> <span class="dv">0</span>:</a>
<a class="sourceLine" id="cb37-100" data-line-number="100">            image_list.append(imageio.imread(<span class="st">&#39;temp.jpg&#39;</span>))</a>
<a class="sourceLine" id="cb37-101" data-line-number="101"></a>
<a class="sourceLine" id="cb37-102" data-line-number="102">    imageio.mimsave(<span class="st">&#39;RNN_MNIST.gif&#39;</span>, image_list, <span class="st">&#39;GIF&#39;</span>, duration<span class="op">=</span><span class="fl">0.2</span>)</a>
<a class="sourceLine" id="cb37-103" data-line-number="103">    plt.ioff()</a>
<a class="sourceLine" id="cb37-104" data-line-number="104">    plt.show()</a>
<a class="sourceLine" id="cb37-105" data-line-number="105"></a>
<a class="sourceLine" id="cb37-106" data-line-number="106"></a>
<a class="sourceLine" id="cb37-107" data-line-number="107">Train()</a></code></pre></div>
<h1 id="自编码autoencoder-用nn进行非监督学习">自编码(Autoencoder) — 用NN进行非监督学习</h1>
<p>自编码是一种神经网络的形式</p>
<h2 id="压缩与解压">压缩与解压</h2>
<p>对自编码器的训练, 相当于将图片经过了压缩,再解压的这一道工序. 当压缩的时候, 原有的图片质量被缩减, 解压时用信息量小却包含了所有关键信息的文件恢复出原本的图片.</p>
<p>编码器: 进而可以训练编码器可以提取足够的有用信息以够恢复原始数据.<strong>实际使用时只使用前半部分, 目的是经过编码器网络可以极大减少数据量却不损失关键信息(提取关键特征)</strong></p>
<p>==<strong>自编码器能自动分类数据, 也可以其那套在半监督上, 用少量有标签样本和大量无标签样本进行学习.</strong>==</p>
<p><img src="/img/in-post/20_07/auto2.png" alt="auto2.png" style="zoom:50%;" /></p>
<p><img src="/img/in-post/20_07/auto3.png" alt="auto3.png" style="zoom:50%;" /></p>
<h2 id="编码器部分encoder">编码器部分(encoder)</h2>
<p>编码器能得到原数据的精髓, 然后我们<strong>只需要再创建一个小的神经网络学习这个精髓的数据,不仅减少了神经网络的负担, 而且同样能达到很好的效果.</strong></p>
<p><img src="/img/in-post/20_07/auto4.png" alt="auto4.png" style="zoom:50%;" /></p>
<p>自编码是类似于PCA可以对特征属性降维, 以用来区分原数据. 效果甚至超越PCA</p>
<h2 id="解码器decoder">解码器(decoder)</h2>
<p>解码器训练时是用来将精髓信息解压成原始信息, 所以可以认为是类似于解压器或者生成器的作用(类似GAN), 那做这件事的一种特殊自编码叫做 variational autoencoders,</p>
<p>VAE解释: <a href="http://kvfrans.com/variational-autoencoders-explained/">Variational Autoencoders Explained</a></p>
<p>有一个例子就是让它能模仿并生成手写数字.</p>
<p><a href="https://mofanpy.com/static/results/ML-intro/auto6.jpg"><img src="/img/in-post/20_07/auto6.jpg" alt="auto6.jpg" /></a></p>
<h2 id="自编码器无监督分类mnist代码">自编码器无监督分类MNIST代码</h2>
<ol type="1">
<li><p>只需要使用training data, 且不需要labels</p></li>
<li><p>训练过程(上面是输入图片, 下面是网络选择的图片)</p>
<blockquote>
<p>可以看出5和3,4和9比较难分辨</p>
</blockquote></li>
</ol>
<figure>
<img src="/img/in-post/20_07/AE_MNIST.gif" alt="AE_MNIST" /><figcaption>AE_MNIST</figcaption>
</figure>
<ol start="3" type="1">
<li><p>最终分类图 <img src="/img/in-post/20_07/AE_Classification_3D.png" alt="AE_Classification_3D" style="zoom:50%;" /></p></li>
<li><p>注意</p>
<ol type="1">
<li><div class="sourceCode" id="cb38"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb38-1" data-line-number="1"><span class="kw">class</span> AutoEncoder(nn.Module):</a>
<a class="sourceLine" id="cb38-2" data-line-number="2">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</a>
<a class="sourceLine" id="cb38-3" data-line-number="3">        <span class="bu">super</span>(AutoEncoder, <span class="va">self</span>).<span class="fu">__init__</span>()</a>
<a class="sourceLine" id="cb38-4" data-line-number="4">        <span class="co"># 压缩</span></a>
<a class="sourceLine" id="cb38-5" data-line-number="5">        <span class="va">self</span>.encoder <span class="op">=</span> nn.Sequential(</a>
<a class="sourceLine" id="cb38-6" data-line-number="6">            nn.Linear(<span class="dv">28</span><span class="op">*</span><span class="dv">28</span>, <span class="dv">128</span>),</a>
<a class="sourceLine" id="cb38-7" data-line-number="7">            nn.Tanh(),</a>
<a class="sourceLine" id="cb38-8" data-line-number="8">            nn.Linear(<span class="dv">128</span>, <span class="dv">64</span>),</a>
<a class="sourceLine" id="cb38-9" data-line-number="9">            nn.Tanh(),</a>
<a class="sourceLine" id="cb38-10" data-line-number="10">            nn.Linear(<span class="dv">64</span>, <span class="dv">12</span>),</a>
<a class="sourceLine" id="cb38-11" data-line-number="11">            nn.Tanh(),</a>
<a class="sourceLine" id="cb38-12" data-line-number="12">            nn.Linear(<span class="dv">12</span>, <span class="dv">3</span>),   <span class="co"># 压缩成3个feature, 进行3D图像可视化</span></a>
<a class="sourceLine" id="cb38-13" data-line-number="13">        )</a>
<a class="sourceLine" id="cb38-14" data-line-number="14">        <span class="co"># 解压</span></a>
<a class="sourceLine" id="cb38-15" data-line-number="15">        <span class="va">self</span>.decoder <span class="op">=</span> nn.Sequential(</a>
<a class="sourceLine" id="cb38-16" data-line-number="16">            nn.Linear(<span class="dv">3</span>, <span class="dv">12</span>),</a>
<a class="sourceLine" id="cb38-17" data-line-number="17">            nn.Tanh(),</a>
<a class="sourceLine" id="cb38-18" data-line-number="18">            nn.Linear(<span class="dv">12</span>, <span class="dv">64</span>),</a>
<a class="sourceLine" id="cb38-19" data-line-number="19">            nn.Tanh(),</a>
<a class="sourceLine" id="cb38-20" data-line-number="20">            nn.Linear(<span class="dv">64</span>, <span class="dv">128</span>),</a>
<a class="sourceLine" id="cb38-21" data-line-number="21">            nn.Tanh(),</a>
<a class="sourceLine" id="cb38-22" data-line-number="22">            nn.Linear(<span class="dv">128</span>, <span class="dv">28</span><span class="op">*</span><span class="dv">28</span>),</a>
<a class="sourceLine" id="cb38-23" data-line-number="23">            nn.Sigmoid(),       <span class="co"># 激励函数是为了让输出值在 (0,1)</span></a>
<a class="sourceLine" id="cb38-24" data-line-number="24">        )</a>
<a class="sourceLine" id="cb38-25" data-line-number="25"></a>
<a class="sourceLine" id="cb38-26" data-line-number="26">    <span class="kw">def</span> forward(<span class="va">self</span>, x):</a>
<a class="sourceLine" id="cb38-27" data-line-number="27">        encoder <span class="op">=</span> <span class="va">self</span>.encoder(x)</a>
<a class="sourceLine" id="cb38-28" data-line-number="28">        decoder <span class="op">=</span> <span class="va">self</span>.decoder(encoder)</a>
<a class="sourceLine" id="cb38-29" data-line-number="29">        <span class="cf">return</span> encoder, decoder     <span class="co"># 注意要把encoder和decoder都输出</span></a></code></pre></div></li>
<li><div class="sourceCode" id="cb39"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb39-1" data-line-number="1">    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(EPOCH):</a>
<a class="sourceLine" id="cb39-2" data-line-number="2">        <span class="cf">for</span> step, (x, b_label) <span class="kw">in</span> tqdm(<span class="bu">enumerate</span>(train_loader)):</a>
<a class="sourceLine" id="cb39-3" data-line-number="3">            <span class="co"># 输出的目标图像是和输入完全相同的, 想要恢复的目标</span></a>
<a class="sourceLine" id="cb39-4" data-line-number="4">            b_x <span class="op">=</span> x.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">28</span><span class="op">*</span><span class="dv">28</span>)  <span class="co"># batch x, shape (batch, 28*28)</span></a>
<a class="sourceLine" id="cb39-5" data-line-number="5">            b_y <span class="op">=</span> x.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">28</span><span class="op">*</span><span class="dv">28</span>)   <span class="co"># batch y, shape (batch, 28*28)</span></a>
<a class="sourceLine" id="cb39-6" data-line-number="6"></a>
<a class="sourceLine" id="cb39-7" data-line-number="7">            encoder, decoder <span class="op">=</span> autoencoder(b_x)</a>
<a class="sourceLine" id="cb39-8" data-line-number="8"></a>
<a class="sourceLine" id="cb39-9" data-line-number="9">            loss <span class="op">=</span> loss_func(decoder, b_y)  <span class="co"># 入图像向量和出图像向量差别为loss</span></a>
<a class="sourceLine" id="cb39-10" data-line-number="10">            optimizer.zero_grad()  <span class="co"># 计算完损失就可以清除梯度了</span></a>
<a class="sourceLine" id="cb39-11" data-line-number="11">            loss.backward()</a>
<a class="sourceLine" id="cb39-12" data-line-number="12">            optimizer.step()</a></code></pre></div></li>
</ol></li>
</ol>
<p>完整代码:</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb40-1" data-line-number="1"><span class="im">import</span> os</a>
<a class="sourceLine" id="cb40-2" data-line-number="2"></a>
<a class="sourceLine" id="cb40-3" data-line-number="3">os.environ[<span class="st">&#39;KMP_DUPLICATE_LIB_OK&#39;</span>] <span class="op">=</span> <span class="st">&#39;True&#39;</span></a>
<a class="sourceLine" id="cb40-4" data-line-number="4"><span class="im">import</span> imageio</a>
<a class="sourceLine" id="cb40-5" data-line-number="5"><span class="im">from</span> tqdm <span class="im">import</span> tqdm</a>
<a class="sourceLine" id="cb40-6" data-line-number="6"><span class="im">import</span> torch <span class="im">as</span> t</a>
<a class="sourceLine" id="cb40-7" data-line-number="7"><span class="im">import</span> torch.nn <span class="im">as</span> nn</a>
<a class="sourceLine" id="cb40-8" data-line-number="8"><span class="im">import</span> torch.utils.data <span class="im">as</span> Data</a>
<a class="sourceLine" id="cb40-9" data-line-number="9"><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</a>
<a class="sourceLine" id="cb40-10" data-line-number="10"><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</a>
<a class="sourceLine" id="cb40-11" data-line-number="11"><span class="im">import</span> torchvision</a>
<a class="sourceLine" id="cb40-12" data-line-number="12"><span class="im">from</span> mpl_toolkits.mplot3d <span class="im">import</span> Axes3D</a>
<a class="sourceLine" id="cb40-13" data-line-number="13"><span class="im">import</span> numpy <span class="im">as</span> np</a>
<a class="sourceLine" id="cb40-14" data-line-number="14"><span class="im">import</span> torchvision.transforms <span class="im">as</span> transforms</a>
<a class="sourceLine" id="cb40-15" data-line-number="15"></a>
<a class="sourceLine" id="cb40-16" data-line-number="16">plt.style.use(<span class="st">&#39;seaborn&#39;</span>)  <span class="co"># 设置使用的样式</span></a>
<a class="sourceLine" id="cb40-17" data-line-number="17"></a>
<a class="sourceLine" id="cb40-18" data-line-number="18"><span class="co"># for visualization</span></a>
<a class="sourceLine" id="cb40-19" data-line-number="19"><span class="im">from</span> matplotlib <span class="im">import</span> cm</a>
<a class="sourceLine" id="cb40-20" data-line-number="20"><span class="im">from</span> sklearn.manifold <span class="im">import</span> TSNE<span class="op">;</span> HAS_SK <span class="op">=</span> <span class="va">True</span></a>
<a class="sourceLine" id="cb40-21" data-line-number="21"></a>
<a class="sourceLine" id="cb40-22" data-line-number="22">LR <span class="op">=</span> <span class="fl">0.005</span></a>
<a class="sourceLine" id="cb40-23" data-line-number="23">BATCH_SIZE <span class="op">=</span> <span class="dv">64</span>  <span class="co"># 批训练的数据个数</span></a>
<a class="sourceLine" id="cb40-24" data-line-number="24">EPOCH <span class="op">=</span> <span class="dv">10</span></a>
<a class="sourceLine" id="cb40-25" data-line-number="25">DOWNLOAD_MNIST <span class="op">=</span> <span class="va">False</span></a>
<a class="sourceLine" id="cb40-26" data-line-number="26">N_TEST_IMG <span class="op">=</span> <span class="dv">5</span>  <span class="co"># 到时候显示 5张图片看效果, 如上图一</span></a>
<a class="sourceLine" id="cb40-27" data-line-number="27"></a>
<a class="sourceLine" id="cb40-28" data-line-number="28"><span class="co">&quot;&quot;&quot;Mnist digits dataset&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb40-29" data-line-number="29"><span class="cf">if</span> <span class="kw">not</span>(os.path.exists(<span class="st">&#39;/Volumes/ArcFile/mnist/&#39;</span>)) <span class="kw">or</span> <span class="kw">not</span> os.listdir(<span class="st">&#39;/Volumes/ArcFile/mnist/&#39;</span>):</a>
<a class="sourceLine" id="cb40-30" data-line-number="30">    DOWNLOAD_MNIST <span class="op">=</span> <span class="va">True</span></a>
<a class="sourceLine" id="cb40-31" data-line-number="31"></a>
<a class="sourceLine" id="cb40-32" data-line-number="32">train_data <span class="op">=</span> torchvision.datasets.MNIST(</a>
<a class="sourceLine" id="cb40-33" data-line-number="33">    root<span class="op">=</span><span class="st">&#39;/Volumes/ArcFile/mnist/&#39;</span>,</a>
<a class="sourceLine" id="cb40-34" data-line-number="34">    train<span class="op">=</span><span class="va">True</span>,</a>
<a class="sourceLine" id="cb40-35" data-line-number="35">    <span class="co"># PIL.Image or numpy.ndarray to torch.FloatTensor(C * H * W), and normalize in the range [0.0, 1.0]</span></a>
<a class="sourceLine" id="cb40-36" data-line-number="36">    transform<span class="op">=</span>transforms.ToTensor(),</a>
<a class="sourceLine" id="cb40-37" data-line-number="37"></a>
<a class="sourceLine" id="cb40-38" data-line-number="38">    download<span class="op">=</span>DOWNLOAD_MNIST</a>
<a class="sourceLine" id="cb40-39" data-line-number="39">)</a>
<a class="sourceLine" id="cb40-40" data-line-number="40"><span class="co"># Data Loader for easy mini-batch return in training, the image batch shape will be (50, 1, 28, 28)</span></a>
<a class="sourceLine" id="cb40-41" data-line-number="41">train_loader <span class="op">=</span> Data.DataLoader(dataset<span class="op">=</span>train_data, batch_size<span class="op">=</span>BATCH_SIZE, shuffle<span class="op">=</span><span class="va">True</span>)</a>
<a class="sourceLine" id="cb40-42" data-line-number="42"></a>
<a class="sourceLine" id="cb40-43" data-line-number="43"></a>
<a class="sourceLine" id="cb40-44" data-line-number="44"><span class="kw">class</span> AutoEncoder(nn.Module):</a>
<a class="sourceLine" id="cb40-45" data-line-number="45">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</a>
<a class="sourceLine" id="cb40-46" data-line-number="46">        <span class="bu">super</span>(AutoEncoder, <span class="va">self</span>).<span class="fu">__init__</span>()</a>
<a class="sourceLine" id="cb40-47" data-line-number="47">        <span class="co"># 压缩</span></a>
<a class="sourceLine" id="cb40-48" data-line-number="48">        <span class="va">self</span>.encoder <span class="op">=</span> nn.Sequential(</a>
<a class="sourceLine" id="cb40-49" data-line-number="49">            nn.Linear(<span class="dv">28</span><span class="op">*</span><span class="dv">28</span>, <span class="dv">128</span>),</a>
<a class="sourceLine" id="cb40-50" data-line-number="50">            nn.Tanh(),</a>
<a class="sourceLine" id="cb40-51" data-line-number="51">            nn.Linear(<span class="dv">128</span>, <span class="dv">64</span>),</a>
<a class="sourceLine" id="cb40-52" data-line-number="52">            nn.Tanh(),</a>
<a class="sourceLine" id="cb40-53" data-line-number="53">            nn.Linear(<span class="dv">64</span>, <span class="dv">12</span>),</a>
<a class="sourceLine" id="cb40-54" data-line-number="54">            nn.Tanh(),</a>
<a class="sourceLine" id="cb40-55" data-line-number="55">            nn.Linear(<span class="dv">12</span>, <span class="dv">3</span>),   <span class="co"># 压缩成3个feature, 进行3D图像可视化</span></a>
<a class="sourceLine" id="cb40-56" data-line-number="56">        )</a>
<a class="sourceLine" id="cb40-57" data-line-number="57">        <span class="co"># 解压</span></a>
<a class="sourceLine" id="cb40-58" data-line-number="58">        <span class="va">self</span>.decoder <span class="op">=</span> nn.Sequential(</a>
<a class="sourceLine" id="cb40-59" data-line-number="59">            nn.Linear(<span class="dv">3</span>, <span class="dv">12</span>),</a>
<a class="sourceLine" id="cb40-60" data-line-number="60">            nn.Tanh(),</a>
<a class="sourceLine" id="cb40-61" data-line-number="61">            nn.Linear(<span class="dv">12</span>, <span class="dv">64</span>),</a>
<a class="sourceLine" id="cb40-62" data-line-number="62">            nn.Tanh(),</a>
<a class="sourceLine" id="cb40-63" data-line-number="63">            nn.Linear(<span class="dv">64</span>, <span class="dv">128</span>),</a>
<a class="sourceLine" id="cb40-64" data-line-number="64">            nn.Tanh(),</a>
<a class="sourceLine" id="cb40-65" data-line-number="65">            nn.Linear(<span class="dv">128</span>, <span class="dv">28</span><span class="op">*</span><span class="dv">28</span>),</a>
<a class="sourceLine" id="cb40-66" data-line-number="66">            nn.Sigmoid(),       <span class="co"># 激励函数是为了让输出值在 (0,1)</span></a>
<a class="sourceLine" id="cb40-67" data-line-number="67">        )</a>
<a class="sourceLine" id="cb40-68" data-line-number="68"></a>
<a class="sourceLine" id="cb40-69" data-line-number="69">    <span class="kw">def</span> forward(<span class="va">self</span>, x):</a>
<a class="sourceLine" id="cb40-70" data-line-number="70">        encoder <span class="op">=</span> <span class="va">self</span>.encoder(x)</a>
<a class="sourceLine" id="cb40-71" data-line-number="71">        decoder <span class="op">=</span> <span class="va">self</span>.decoder(encoder)</a>
<a class="sourceLine" id="cb40-72" data-line-number="72">        <span class="cf">return</span> encoder, decoder     <span class="co"># 注意要把encoder和decoder都输出</span></a>
<a class="sourceLine" id="cb40-73" data-line-number="73"></a>
<a class="sourceLine" id="cb40-74" data-line-number="74"></a>
<a class="sourceLine" id="cb40-75" data-line-number="75">autoencoder <span class="op">=</span> AutoEncoder()</a>
<a class="sourceLine" id="cb40-76" data-line-number="76"><span class="bu">print</span>(autoencoder)</a>
<a class="sourceLine" id="cb40-77" data-line-number="77"></a>
<a class="sourceLine" id="cb40-78" data-line-number="78">optimizer <span class="op">=</span> t.optim.Adam(autoencoder.parameters(), lr<span class="op">=</span>LR)</a>
<a class="sourceLine" id="cb40-79" data-line-number="79">loss_func <span class="op">=</span> nn.MSELoss()   <span class="co"># why??</span></a>
<a class="sourceLine" id="cb40-80" data-line-number="80"></a>
<a class="sourceLine" id="cb40-81" data-line-number="81"></a>
<a class="sourceLine" id="cb40-82" data-line-number="82"><span class="kw">def</span> plot_with_labels(lowDWeights, labels, image_list, <span class="op">*</span>param):</a>
<a class="sourceLine" id="cb40-83" data-line-number="83">    <span class="co"># 要观看的数据</span></a>
<a class="sourceLine" id="cb40-84" data-line-number="84">    view_data <span class="op">=</span> train_data.train_data[:<span class="dv">200</span>].view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">28</span> <span class="op">*</span> <span class="dv">28</span>).<span class="bu">type</span>(torch.FloatTensor) <span class="op">/</span> <span class="fl">255.</span></a>
<a class="sourceLine" id="cb40-85" data-line-number="85">    encoded_data, _ <span class="op">=</span> autoencoder(view_data)  <span class="co"># 提取压缩的特征值</span></a>
<a class="sourceLine" id="cb40-86" data-line-number="86">    fig <span class="op">=</span> plt.figure(<span class="dv">2</span>)</a>
<a class="sourceLine" id="cb40-87" data-line-number="87">    ax <span class="op">=</span> Axes3D(fig)  <span class="co"># 3D 图</span></a>
<a class="sourceLine" id="cb40-88" data-line-number="88">    <span class="co"># x, y, z 的数据值</span></a>
<a class="sourceLine" id="cb40-89" data-line-number="89">    X <span class="op">=</span> encoded_data.data[:, <span class="dv">0</span>].numpy()</a>
<a class="sourceLine" id="cb40-90" data-line-number="90">    Y <span class="op">=</span> encoded_data.data[:, <span class="dv">1</span>].numpy()</a>
<a class="sourceLine" id="cb40-91" data-line-number="91">    Z <span class="op">=</span> encoded_data.data[:, <span class="dv">2</span>].numpy()</a>
<a class="sourceLine" id="cb40-92" data-line-number="92">    values <span class="op">=</span> train_data.train_labels[:<span class="dv">200</span>].numpy()  <span class="co"># 标签值</span></a>
<a class="sourceLine" id="cb40-93" data-line-number="93">    <span class="cf">for</span> x, y, z, s <span class="kw">in</span> <span class="bu">zip</span>(X, Y, Z, values):</a>
<a class="sourceLine" id="cb40-94" data-line-number="94">        c <span class="op">=</span> cm.rainbow(<span class="bu">int</span>(<span class="dv">255</span> <span class="op">*</span> s <span class="op">/</span> <span class="dv">9</span>))  <span class="co"># 上色</span></a>
<a class="sourceLine" id="cb40-95" data-line-number="95">        ax.text(x, y, z, s, backgroundcolor<span class="op">=</span>c)  <span class="co"># 标位子</span></a>
<a class="sourceLine" id="cb40-96" data-line-number="96">    ax.set_xlim(X.<span class="bu">min</span>(), X.<span class="bu">max</span>())</a>
<a class="sourceLine" id="cb40-97" data-line-number="97">    ax.set_ylim(Y.<span class="bu">min</span>(), Y.<span class="bu">max</span>())</a>
<a class="sourceLine" id="cb40-98" data-line-number="98">    ax.set_zlim(Z.<span class="bu">min</span>(), Z.<span class="bu">max</span>())</a>
<a class="sourceLine" id="cb40-99" data-line-number="99">    epoch, loss, accuracy <span class="op">=</span> param</a>
<a class="sourceLine" id="cb40-100" data-line-number="100">    text <span class="op">=</span> (<span class="st">&#39;Epoch: </span><span class="sc">%d</span><span class="st">,&#39;</span> <span class="op">%</span> epoch <span class="op">+</span> <span class="st">&#39; train loss: </span><span class="sc">%.4f</span><span class="st">,&#39;</span> <span class="op">%</span> loss.data.numpy() <span class="op">+</span> <span class="st">&#39; test accuracy: </span><span class="sc">%.3f</span><span class="st">&#39;</span> <span class="op">%</span> accuracy)</a>
<a class="sourceLine" id="cb40-101" data-line-number="101">    plt.text(<span class="op">-</span><span class="dv">30</span>, <span class="dv">-35</span>, text, fontdict<span class="op">=</span>{<span class="st">&#39;size&#39;</span>: <span class="dv">13</span>, <span class="st">&#39;color&#39;</span>: <span class="st">&#39;red&#39;</span>})</a>
<a class="sourceLine" id="cb40-102" data-line-number="102">    plt.show()</a>
<a class="sourceLine" id="cb40-103" data-line-number="103">    plt.savefig(<span class="st">&quot;temp.jpg&quot;</span>)</a>
<a class="sourceLine" id="cb40-104" data-line-number="104">    plt.pause(<span class="fl">0.02</span>)</a>
<a class="sourceLine" id="cb40-105" data-line-number="105">    image_list.append(imageio.imread(<span class="st">&quot;temp.jpg&quot;</span>))  <span class="co"># 可以不用循环i，直接用列表形式</span></a>
<a class="sourceLine" id="cb40-106" data-line-number="106"></a>
<a class="sourceLine" id="cb40-107" data-line-number="107"></a>
<a class="sourceLine" id="cb40-108" data-line-number="108"><span class="kw">def</span> Train():</a>
<a class="sourceLine" id="cb40-109" data-line-number="109">    <span class="co"># initialize figure</span></a>
<a class="sourceLine" id="cb40-110" data-line-number="110">    f, a <span class="op">=</span> plt.subplots(<span class="dv">2</span>, N_TEST_IMG, figsize<span class="op">=</span>(<span class="dv">5</span>, <span class="dv">2</span>))</a>
<a class="sourceLine" id="cb40-111" data-line-number="111">    plt.ion()  <span class="co"># continuously plot</span></a>
<a class="sourceLine" id="cb40-112" data-line-number="112"></a>
<a class="sourceLine" id="cb40-113" data-line-number="113">    <span class="co"># original data (first row) for viewing</span></a>
<a class="sourceLine" id="cb40-114" data-line-number="114">    view_data <span class="op">=</span> train_data.train_data[:N_TEST_IMG].view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">28</span> <span class="op">*</span> <span class="dv">28</span>).<span class="bu">type</span>(t.FloatTensor) <span class="op">/</span> <span class="fl">255.</span></a>
<a class="sourceLine" id="cb40-115" data-line-number="115">    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(N_TEST_IMG):</a>
<a class="sourceLine" id="cb40-116" data-line-number="116">        a[<span class="dv">0</span>][i].imshow(np.reshape(view_data.data.numpy()[i], (<span class="dv">28</span>, <span class="dv">28</span>)), cmap<span class="op">=</span><span class="st">&#39;gray&#39;</span>)</a>
<a class="sourceLine" id="cb40-117" data-line-number="117">        a[<span class="dv">0</span>][i].set_xticks(())</a>
<a class="sourceLine" id="cb40-118" data-line-number="118">        a[<span class="dv">0</span>][i].set_yticks(())</a>
<a class="sourceLine" id="cb40-119" data-line-number="119"></a>
<a class="sourceLine" id="cb40-120" data-line-number="120">    image_list <span class="op">=</span> []</a>
<a class="sourceLine" id="cb40-121" data-line-number="121">    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(EPOCH):</a>
<a class="sourceLine" id="cb40-122" data-line-number="122">        <span class="cf">for</span> step, (x, b_label) <span class="kw">in</span> tqdm(<span class="bu">enumerate</span>(train_loader)):</a>
<a class="sourceLine" id="cb40-123" data-line-number="123">            <span class="co"># 输出的目标图像是和输入完全相同的, 想要恢复的目标</span></a>
<a class="sourceLine" id="cb40-124" data-line-number="124">            b_x <span class="op">=</span> x.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">28</span><span class="op">*</span><span class="dv">28</span>)  <span class="co"># batch x, shape (batch, 28*28)</span></a>
<a class="sourceLine" id="cb40-125" data-line-number="125">            b_y <span class="op">=</span> x.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">28</span><span class="op">*</span><span class="dv">28</span>)   <span class="co"># batch y, shape (batch, 28*28)</span></a>
<a class="sourceLine" id="cb40-126" data-line-number="126"></a>
<a class="sourceLine" id="cb40-127" data-line-number="127">            encoder, decoder <span class="op">=</span> autoencoder(b_x)</a>
<a class="sourceLine" id="cb40-128" data-line-number="128"></a>
<a class="sourceLine" id="cb40-129" data-line-number="129">            loss <span class="op">=</span> loss_func(decoder, b_y)  <span class="co"># 入图像向量和出图像向量差别为loss</span></a>
<a class="sourceLine" id="cb40-130" data-line-number="130">            optimizer.zero_grad()  <span class="co"># 计算完损失就可以清除梯度了</span></a>
<a class="sourceLine" id="cb40-131" data-line-number="131">            loss.backward()</a>
<a class="sourceLine" id="cb40-132" data-line-number="132">            optimizer.step()</a>
<a class="sourceLine" id="cb40-133" data-line-number="133"></a>
<a class="sourceLine" id="cb40-134" data-line-number="134">            <span class="co"># visualization use t-SNE</span></a>
<a class="sourceLine" id="cb40-135" data-line-number="135">            <span class="cf">if</span> step <span class="op">%</span> <span class="dv">50</span> <span class="op">==</span> <span class="dv">0</span>:</a>
<a class="sourceLine" id="cb40-136" data-line-number="136">                <span class="bu">print</span>(<span class="st">&#39;Epoch: &#39;</span>, epoch, <span class="st">&#39;| train loss: </span><span class="sc">%.4f</span><span class="st">&#39;</span> <span class="op">%</span> loss.data.numpy())</a>
<a class="sourceLine" id="cb40-137" data-line-number="137"></a>
<a class="sourceLine" id="cb40-138" data-line-number="138">                <span class="co"># plotting decoded image (second row)</span></a>
<a class="sourceLine" id="cb40-139" data-line-number="139">                _, decoded_data <span class="op">=</span> autoencoder(view_data)</a>
<a class="sourceLine" id="cb40-140" data-line-number="140">                <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(N_TEST_IMG):</a>
<a class="sourceLine" id="cb40-141" data-line-number="141">                    a[<span class="dv">1</span>][i].clear()</a>
<a class="sourceLine" id="cb40-142" data-line-number="142">                    a[<span class="dv">1</span>][i].imshow(np.reshape(decoded_data.data.numpy()[i], (<span class="dv">28</span>, <span class="dv">28</span>)), cmap<span class="op">=</span><span class="st">&#39;gray&#39;</span>)</a>
<a class="sourceLine" id="cb40-143" data-line-number="143">                    a[<span class="dv">1</span>][i].set_xticks(())</a>
<a class="sourceLine" id="cb40-144" data-line-number="144">                    a[<span class="dv">1</span>][i].set_yticks(())</a>
<a class="sourceLine" id="cb40-145" data-line-number="145">                plt.draw()</a>
<a class="sourceLine" id="cb40-146" data-line-number="146">                plt.savefig(<span class="st">&#39;temp.jpg&#39;</span>)</a>
<a class="sourceLine" id="cb40-147" data-line-number="147">                plt.pause(<span class="fl">0.05</span>)</a>
<a class="sourceLine" id="cb40-148" data-line-number="148">                <span class="cf">if</span> step <span class="op">%</span> <span class="dv">200</span> <span class="op">==</span> <span class="dv">0</span>:</a>
<a class="sourceLine" id="cb40-149" data-line-number="149">                    image_list.append(imageio.imread(<span class="st">&#39;temp.jpg&#39;</span>))</a>
<a class="sourceLine" id="cb40-150" data-line-number="150"></a>
<a class="sourceLine" id="cb40-151" data-line-number="151">    plt.ioff()</a>
<a class="sourceLine" id="cb40-152" data-line-number="152">    imageio.mimsave(<span class="st">&#39;AE_MNIST.gif&#39;</span>, image_list, <span class="st">&#39;GIF&#39;</span>, duration<span class="op">=</span><span class="fl">0.2</span>)</a>
<a class="sourceLine" id="cb40-153" data-line-number="153"></a>
<a class="sourceLine" id="cb40-154" data-line-number="154">    t.save(autoencoder, <span class="st">&#39;./Try_ModelSaved/autoencoder_mnist.pkl&#39;</span>)  <span class="co"># 保存整个网络</span></a>
<a class="sourceLine" id="cb40-155" data-line-number="155"></a>
<a class="sourceLine" id="cb40-156" data-line-number="156"></a>
<a class="sourceLine" id="cb40-157" data-line-number="157">Train()</a>
<a class="sourceLine" id="cb40-158" data-line-number="158"></a>
<a class="sourceLine" id="cb40-159" data-line-number="159"><span class="co"># visualize in 3D plot</span></a>
<a class="sourceLine" id="cb40-160" data-line-number="160">view_data <span class="op">=</span> train_data.train_data[:<span class="dv">200</span>].view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">28</span><span class="op">*</span><span class="dv">28</span>).<span class="bu">type</span>(t.FloatTensor)<span class="op">/</span><span class="fl">255.</span></a>
<a class="sourceLine" id="cb40-161" data-line-number="161">encoded_data, _ <span class="op">=</span> autoencoder(view_data)</a>
<a class="sourceLine" id="cb40-162" data-line-number="162">fig <span class="op">=</span> plt.figure(<span class="dv">2</span>)<span class="op">;</span> ax <span class="op">=</span> Axes3D(fig)</a>
<a class="sourceLine" id="cb40-163" data-line-number="163">X, Y, Z <span class="op">=</span> encoded_data.data[:, <span class="dv">0</span>].numpy(), encoded_data.data[:, <span class="dv">1</span>].numpy(), encoded_data.data[:, <span class="dv">2</span>].numpy()</a>
<a class="sourceLine" id="cb40-164" data-line-number="164">values <span class="op">=</span> train_data.train_labels[:<span class="dv">200</span>].numpy()</a>
<a class="sourceLine" id="cb40-165" data-line-number="165"><span class="cf">for</span> x, y, z, s <span class="kw">in</span> <span class="bu">zip</span>(X, Y, Z, values):</a>
<a class="sourceLine" id="cb40-166" data-line-number="166">    c <span class="op">=</span> cm.rainbow(<span class="bu">int</span>(<span class="dv">255</span><span class="op">*</span>s<span class="op">/</span><span class="dv">9</span>))<span class="op">;</span> ax.text(x, y, z, s, backgroundcolor<span class="op">=</span>c)</a>
<a class="sourceLine" id="cb40-167" data-line-number="167">ax.set_xlim(X.<span class="bu">min</span>(), X.<span class="bu">max</span>())<span class="op">;</span> ax.set_ylim(Y.<span class="bu">min</span>(), Y.<span class="bu">max</span>())<span class="op">;</span> ax.set_zlim(Z.<span class="bu">min</span>(), Z.<span class="bu">max</span>())</a>
<a class="sourceLine" id="cb40-168" data-line-number="168">plt.show()</a></code></pre></div>
<h1 id="dqndeep-q-network-强化学习">DQN(Deep Q Network ) 强化学习</h1>
<h2 id="dqn对比q-learning">DQN对比Q-Learning</h2>
<p>RL中用表格村春每个状态state 和 在这个状态下每个行为action所拥有的Q值. 但当今问题太复杂, 状态非常多, 其存储和搜索都很难. 所以引入神经网络.</p>
<ol type="1">
<li>我们可以将状态和动作当成神经网络的输入, 然后经过神经网络分析后得到动作的 Q 值, 这样我们就没必要在表格中记录 Q 值, 而是直接使用神经网络生成 Q 值.</li>
<li>还有一种形式的是这样, 我们也能只输入状态值, 输出所有的动作值, 然后按照 Q learning 的原则, 直接选择拥有最大值的动作当做下一步要做的动作.</li>
</ol>
<figure>
<img src="/img/in-post/20_07/DQN2.png" alt="DQN2.png" /><figcaption>DQN2.png</figcaption>
</figure>
<h2 id="更新神经网络">更新神经网络</h2>
<figure>
<img src="/img/in-post/20_07/DQN3.png" alt="DQN3.png" /><figcaption>DQN3.png</figcaption>
</figure>
<figure>
<img src="/img/in-post/20_07/DQN4.png" alt="DQN4.png" /><figcaption>DQN4.png</figcaption>
</figure>
<p>我们通过 NN 预测出Q(s2, a1) 和 Q(s2,a2) 的值, 这就是 <strong>Q 估计</strong>.</p>
<p>然后我们<strong>选取 Q 估计中最大值的动作来换取环境中的奖励</strong> reward.</p>
<p>而 Q 现实中也包含从神经网络分析出来的两个 Q 估计值, 不过这个 Q 估计是<strong>针对于下一步在 s’ 的估计.</strong></p>
<p>最后再通过算法更新神经网络中的参数</p>
<ol type="1">
<li>首先, 我们需要 a1, a2 正确的Q值, 这个 Q 值我们就用之前在 Q learning 中的 Q 现实来代替. 同样我们还需要一个 Q 估计 来实现神经网络的更新.</li>
<li>所以神经网络的的参数就是老的 NN 参数 加学习率 alpha 乘以 Q 现实 和 Q 估计 的差距. 我们整理一下.</li>
</ol>
<p>还有两大因素支撑着 DQN 使得它变得无比强大. 这两大因素就是 <strong>Experience replay 和 Fixed Q-targets.</strong></p>
<p>Q learning 是一种 off-policy 离线学习法, 它能学习当前经历着的, 也能学习过去经历过的, 甚至是学习别人的经历. 所以每次 DQN 更新的时候, 我们都可以随机抽取一些之前的经历进行学习. 随机抽取这种做法<strong>打乱了经历之间的相关性</strong>, 也使得神经网络更新更有效率. Fixed Q-targets 也是一种打乱相关性的机理<strong>, 如果使用 fixed Q-targets, 我们就会在 DQN 中使用到两个结构相同但参数不同的神经网络, 预测 Q 估计 的神经网络具备最新的参数, 而预测 Q 现实 的神经网络使用的参数则是很久以前的. 有了这两种提升手段, DQN 才能在一些游戏中超越人类.</strong></p>
<figure>
<img src="/img/in-post/20_07/DQN5.png" alt="DQN5.png" /><figcaption>DQN5.png</figcaption>
</figure>
<div class="sourceCode" id="cb41"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb41-1" data-line-number="1"><span class="im">import</span> torch</a>
<a class="sourceLine" id="cb41-2" data-line-number="2"><span class="im">import</span> torch.nn <span class="im">as</span> nn</a>
<a class="sourceLine" id="cb41-3" data-line-number="3"><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</a>
<a class="sourceLine" id="cb41-4" data-line-number="4"><span class="im">import</span> numpy <span class="im">as</span> np</a>
<a class="sourceLine" id="cb41-5" data-line-number="5"><span class="im">import</span> gym</a>
<a class="sourceLine" id="cb41-6" data-line-number="6"></a>
<a class="sourceLine" id="cb41-7" data-line-number="7"><span class="co"># 超参数</span></a>
<a class="sourceLine" id="cb41-8" data-line-number="8">BATCH_SIZE <span class="op">=</span> <span class="dv">32</span></a>
<a class="sourceLine" id="cb41-9" data-line-number="9">LR <span class="op">=</span> <span class="fl">0.01</span></a>
<a class="sourceLine" id="cb41-10" data-line-number="10">EPSILON <span class="op">=</span> <span class="fl">0.9</span>               <span class="co"># 最优选择动作百分比</span></a>
<a class="sourceLine" id="cb41-11" data-line-number="11">GAMMA <span class="op">=</span> <span class="fl">0.9</span>                 <span class="co"># 奖励递减参数</span></a>
<a class="sourceLine" id="cb41-12" data-line-number="12">TARGET_REPLACE_ITER <span class="op">=</span> <span class="dv">100</span>   <span class="co"># Q 现实网络的更新频率</span></a>
<a class="sourceLine" id="cb41-13" data-line-number="13">MEMORY_CAPACITY <span class="op">=</span> <span class="dv">2000</span>      <span class="co"># 记忆库大小</span></a>
<a class="sourceLine" id="cb41-14" data-line-number="14">env <span class="op">=</span> gym.make(<span class="st">&#39;CartPole-v0&#39;</span>)   <span class="co"># 立杆子游戏</span></a>
<a class="sourceLine" id="cb41-15" data-line-number="15">env <span class="op">=</span> env.unwrapped             <span class="co"># ??????</span></a>
<a class="sourceLine" id="cb41-16" data-line-number="16">N_ACTIONS <span class="op">=</span> env.action_space.n  <span class="co"># 杆子能做的动作</span></a>
<a class="sourceLine" id="cb41-17" data-line-number="17">N_STATES <span class="op">=</span> env.observation_space.shape[<span class="dv">0</span>]   <span class="co"># 杆子能获取的环境信息数</span></a>
<a class="sourceLine" id="cb41-18" data-line-number="18"></a>
<a class="sourceLine" id="cb41-19" data-line-number="19"></a>
<a class="sourceLine" id="cb41-20" data-line-number="20"></a>
<a class="sourceLine" id="cb41-21" data-line-number="21"><span class="kw">class</span> Net(nn.Module):</a>
<a class="sourceLine" id="cb41-22" data-line-number="22">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, ):</a>
<a class="sourceLine" id="cb41-23" data-line-number="23">        <span class="bu">super</span>(Net, <span class="va">self</span>).<span class="fu">__init__</span>()</a>
<a class="sourceLine" id="cb41-24" data-line-number="24">        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(N_STATES, <span class="dv">10</span>)</a>
<a class="sourceLine" id="cb41-25" data-line-number="25">        <span class="va">self</span>.fc1.weight.data.normal_(<span class="dv">0</span>, <span class="fl">0.1</span>)   <span class="co"># initialization, 正态分布随机生成起始参数值</span></a>
<a class="sourceLine" id="cb41-26" data-line-number="26">        <span class="va">self</span>.out <span class="op">=</span> nn.Linear(<span class="dv">10</span>, N_ACTIONS)     <span class="co"># 输出价值</span></a>
<a class="sourceLine" id="cb41-27" data-line-number="27">        <span class="va">self</span>.out.weight.data.normal_(<span class="dv">0</span>, <span class="fl">0.1</span>)   <span class="co"># initialization</span></a>
<a class="sourceLine" id="cb41-28" data-line-number="28"></a>
<a class="sourceLine" id="cb41-29" data-line-number="29">    <span class="kw">def</span> forward(<span class="va">self</span>, x):</a>
<a class="sourceLine" id="cb41-30" data-line-number="30">        x <span class="op">=</span> <span class="va">self</span>.fc1(x)</a>
<a class="sourceLine" id="cb41-31" data-line-number="31">        x <span class="op">=</span> F.relu(x)</a>
<a class="sourceLine" id="cb41-32" data-line-number="32">        actions_value <span class="op">=</span> <span class="va">self</span>.out(x)</a>
<a class="sourceLine" id="cb41-33" data-line-number="33">        <span class="cf">return</span> actions_value</a>
<a class="sourceLine" id="cb41-34" data-line-number="34"></a>
<a class="sourceLine" id="cb41-35" data-line-number="35"></a>
<a class="sourceLine" id="cb41-36" data-line-number="36"><span class="kw">class</span> DQN(<span class="bu">object</span>):</a>
<a class="sourceLine" id="cb41-37" data-line-number="37">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</a>
<a class="sourceLine" id="cb41-38" data-line-number="38">        <span class="co"># 建立 target net 和 eval net 还有 memory</span></a>
<a class="sourceLine" id="cb41-39" data-line-number="39">        <span class="va">self</span>.eval_net, <span class="va">self</span>.target_net <span class="op">=</span> Net(), Net()   <span class="co"># 相同的网络,参数延迟更新</span></a>
<a class="sourceLine" id="cb41-40" data-line-number="40"></a>
<a class="sourceLine" id="cb41-41" data-line-number="41">        <span class="va">self</span>.learn_step_counter <span class="op">=</span> <span class="dv">0</span>     <span class="co"># 用于 target 更新计时</span></a>
<a class="sourceLine" id="cb41-42" data-line-number="42">        <span class="va">self</span>.memory_counter <span class="op">=</span> <span class="dv">0</span>         <span class="co"># 记忆库记数</span></a>
<a class="sourceLine" id="cb41-43" data-line-number="43">        <span class="va">self</span>.memory <span class="op">=</span> np.zeros((MEMORY_CAPACITY, N_STATES <span class="op">*</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">2</span>))     <span class="co"># 初始化记忆库</span></a>
<a class="sourceLine" id="cb41-44" data-line-number="44">        <span class="va">self</span>.optimizer <span class="op">=</span> torch.optim.Adam(<span class="va">self</span>.eval_net.parameters(), lr<span class="op">=</span>LR)    <span class="co"># torch 的优化器</span></a>
<a class="sourceLine" id="cb41-45" data-line-number="45">        <span class="va">self</span>.loss_func <span class="op">=</span> nn.MSELoss()   <span class="co"># 误差公式</span></a>
<a class="sourceLine" id="cb41-46" data-line-number="46"></a>
<a class="sourceLine" id="cb41-47" data-line-number="47">    <span class="kw">def</span> choose_action(<span class="va">self</span>, x):</a>
<a class="sourceLine" id="cb41-48" data-line-number="48">        <span class="co"># 根据环境观测值选择动作的机制</span></a>
<a class="sourceLine" id="cb41-49" data-line-number="49">        x <span class="op">=</span> torch.unsqueeze(torch.FloatTensor(x), <span class="dv">0</span>)</a>
<a class="sourceLine" id="cb41-50" data-line-number="50">        <span class="co"># 这里只输入一个 sample</span></a>
<a class="sourceLine" id="cb41-51" data-line-number="51">        <span class="cf">if</span> np.random.uniform() <span class="op">&lt;</span> EPSILON:   <span class="co"># 选最优动作</span></a>
<a class="sourceLine" id="cb41-52" data-line-number="52">            actions_value <span class="op">=</span> <span class="va">self</span>.eval_net.forward(x)</a>
<a class="sourceLine" id="cb41-53" data-line-number="53">            action <span class="op">=</span> torch.<span class="bu">max</span>(actions_value, <span class="dv">1</span>)[<span class="dv">1</span>].data.numpy()[<span class="dv">0</span>]     <span class="co"># return the argmax</span></a>
<a class="sourceLine" id="cb41-54" data-line-number="54">        <span class="cf">else</span>:   <span class="co"># 选随机动作</span></a>
<a class="sourceLine" id="cb41-55" data-line-number="55">            action <span class="op">=</span> np.random.randint(<span class="dv">0</span>, N_ACTIONS)</a>
<a class="sourceLine" id="cb41-56" data-line-number="56">        <span class="cf">return</span> action</a>
<a class="sourceLine" id="cb41-57" data-line-number="57"></a>
<a class="sourceLine" id="cb41-58" data-line-number="58">    <span class="kw">def</span> store_transition(<span class="va">self</span>, s, a, r, s_):</a>
<a class="sourceLine" id="cb41-59" data-line-number="59">        <span class="co"># 存储记忆</span></a>
<a class="sourceLine" id="cb41-60" data-line-number="60">        transition <span class="op">=</span> np.hstack((s, [a, r], s_))</a>
<a class="sourceLine" id="cb41-61" data-line-number="61">        <span class="co"># 如果记忆库满了, 就覆盖老数据</span></a>
<a class="sourceLine" id="cb41-62" data-line-number="62">        index <span class="op">=</span> <span class="va">self</span>.memory_counter <span class="op">%</span> MEMORY_CAPACITY</a>
<a class="sourceLine" id="cb41-63" data-line-number="63">        <span class="va">self</span>.memory[index, :] <span class="op">=</span> transition</a>
<a class="sourceLine" id="cb41-64" data-line-number="64">        <span class="va">self</span>.memory_counter <span class="op">+=</span> <span class="dv">1</span></a>
<a class="sourceLine" id="cb41-65" data-line-number="65"></a>
<a class="sourceLine" id="cb41-66" data-line-number="66">    <span class="kw">def</span> learn(<span class="va">self</span>):</a>
<a class="sourceLine" id="cb41-67" data-line-number="67">        <span class="co"># target 网络更新</span></a>
<a class="sourceLine" id="cb41-68" data-line-number="68">        <span class="co"># 学习记忆库中的记忆</span></a>
<a class="sourceLine" id="cb41-69" data-line-number="69">        <span class="cf">if</span> <span class="va">self</span>.learn_step_counter <span class="op">%</span> TARGET_REPLACE_ITER <span class="op">==</span> <span class="dv">0</span>:</a>
<a class="sourceLine" id="cb41-70" data-line-number="70">            <span class="va">self</span>.target_net.load_state_dict(<span class="va">self</span>.eval_net.state_dict())</a>
<a class="sourceLine" id="cb41-71" data-line-number="71">        <span class="va">self</span>.learn_step_counter <span class="op">+=</span> <span class="dv">1</span></a>
<a class="sourceLine" id="cb41-72" data-line-number="72"></a>
<a class="sourceLine" id="cb41-73" data-line-number="73">        <span class="co"># 抽取记忆库中的批数据</span></a>
<a class="sourceLine" id="cb41-74" data-line-number="74">        sample_index <span class="op">=</span> np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)</a>
<a class="sourceLine" id="cb41-75" data-line-number="75">        b_memory <span class="op">=</span> <span class="va">self</span>.memory[sample_index, :]</a>
<a class="sourceLine" id="cb41-76" data-line-number="76">        b_s <span class="op">=</span> torch.FloatTensor(b_memory[:, :N_STATES])</a>
<a class="sourceLine" id="cb41-77" data-line-number="77">        b_a <span class="op">=</span> torch.LongTensor(b_memory[:, N_STATES:N_STATES <span class="op">+</span> <span class="dv">1</span>].astype(<span class="bu">int</span>))</a>
<a class="sourceLine" id="cb41-78" data-line-number="78">        b_r <span class="op">=</span> torch.FloatTensor(b_memory[:, N_STATES <span class="op">+</span> <span class="dv">1</span>:N_STATES <span class="op">+</span> <span class="dv">2</span>])</a>
<a class="sourceLine" id="cb41-79" data-line-number="79">        b_s_ <span class="op">=</span> torch.FloatTensor(b_memory[:, <span class="op">-</span>N_STATES:])</a>
<a class="sourceLine" id="cb41-80" data-line-number="80"></a>
<a class="sourceLine" id="cb41-81" data-line-number="81">        <span class="co"># 针对做过的动作b_a, 来选 q_eval 的值, (q_eval 原本有所有动作的值)</span></a>
<a class="sourceLine" id="cb41-82" data-line-number="82">        q_eval <span class="op">=</span> <span class="va">self</span>.eval_net(b_s).gather(<span class="dv">1</span>, b_a)  <span class="co"># shape (batch, 1)</span></a>
<a class="sourceLine" id="cb41-83" data-line-number="83">        q_next <span class="op">=</span> <span class="va">self</span>.target_net(b_s_).detach()  <span class="co"># q_next 不进行反向传递误差, 所以 detach</span></a>
<a class="sourceLine" id="cb41-84" data-line-number="84">        q_target <span class="op">=</span> b_r <span class="op">+</span> GAMMA <span class="op">*</span> q_next.<span class="bu">max</span>(<span class="dv">1</span>)[<span class="dv">0</span>]  <span class="co"># shape (batch, 1)</span></a>
<a class="sourceLine" id="cb41-85" data-line-number="85">        loss <span class="op">=</span> <span class="va">self</span>.loss_func(q_eval, q_target)</a>
<a class="sourceLine" id="cb41-86" data-line-number="86"></a>
<a class="sourceLine" id="cb41-87" data-line-number="87">        <span class="co"># 计算, 更新 eval net</span></a>
<a class="sourceLine" id="cb41-88" data-line-number="88">        <span class="va">self</span>.optimizer.zero_grad()</a>
<a class="sourceLine" id="cb41-89" data-line-number="89">        loss.backward()</a>
<a class="sourceLine" id="cb41-90" data-line-number="90">        <span class="va">self</span>.optimizer.step()</a>
<a class="sourceLine" id="cb41-91" data-line-number="91"></a>
<a class="sourceLine" id="cb41-92" data-line-number="92">dqn <span class="op">=</span> DQN() <span class="co"># 定义 DQN 系统</span></a>
<a class="sourceLine" id="cb41-93" data-line-number="93"></a>
<a class="sourceLine" id="cb41-94" data-line-number="94"><span class="cf">for</span> i_episode <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">400</span>):</a>
<a class="sourceLine" id="cb41-95" data-line-number="95">    s <span class="op">=</span> env.reset()</a>
<a class="sourceLine" id="cb41-96" data-line-number="96">    <span class="cf">while</span> <span class="va">True</span>:</a>
<a class="sourceLine" id="cb41-97" data-line-number="97">        env.render()    <span class="co"># 显示实验动画</span></a>
<a class="sourceLine" id="cb41-98" data-line-number="98">        a <span class="op">=</span> dqn.choose_action(s)    <span class="co"># 行为</span></a>
<a class="sourceLine" id="cb41-99" data-line-number="99"></a>
<a class="sourceLine" id="cb41-100" data-line-number="100">        <span class="co"># 选动作, 得到环境反馈</span></a>
<a class="sourceLine" id="cb41-101" data-line-number="101">        s_, r, done, info <span class="op">=</span> env.step(a)     <span class="co"># 根据行为得到的反馈</span></a>
<a class="sourceLine" id="cb41-102" data-line-number="102"></a>
<a class="sourceLine" id="cb41-103" data-line-number="103">        <span class="co"># 修改 reward, 使 DQN 快速学习</span></a>
<a class="sourceLine" id="cb41-104" data-line-number="104">        x, x_dot, theta, theta_dot <span class="op">=</span> s_</a>
<a class="sourceLine" id="cb41-105" data-line-number="105">        r1 <span class="op">=</span> (env.x_threshold <span class="op">-</span> <span class="bu">abs</span>(x)) <span class="op">/</span> env.x_threshold <span class="op">-</span> <span class="fl">0.8</span></a>
<a class="sourceLine" id="cb41-106" data-line-number="106">        r2 <span class="op">=</span> (env.theta_threshold_radians <span class="op">-</span> <span class="bu">abs</span>(theta)) <span class="op">/</span> env.theta_threshold_radians <span class="op">-</span> <span class="fl">0.5</span></a>
<a class="sourceLine" id="cb41-107" data-line-number="107">        r <span class="op">=</span> r1 <span class="op">+</span> r2</a>
<a class="sourceLine" id="cb41-108" data-line-number="108"></a>
<a class="sourceLine" id="cb41-109" data-line-number="109">        <span class="co"># 存记忆</span></a>
<a class="sourceLine" id="cb41-110" data-line-number="110">        dqn.store_transition(s, a, r, s_)        <span class="co"># 状态 动作 奖励 下一个状态 存储</span></a>
<a class="sourceLine" id="cb41-111" data-line-number="111"></a>
<a class="sourceLine" id="cb41-112" data-line-number="112">        <span class="cf">if</span> dqn.memory_counter <span class="op">&gt;</span> MEMORY_CAPACITY:</a>
<a class="sourceLine" id="cb41-113" data-line-number="113">            dqn.learn() <span class="co"># 记忆库满了就进行学习</span></a>
<a class="sourceLine" id="cb41-114" data-line-number="114"></a>
<a class="sourceLine" id="cb41-115" data-line-number="115">        <span class="cf">if</span> done:    <span class="co"># 如果回合结束, 进入下回合</span></a>
<a class="sourceLine" id="cb41-116" data-line-number="116">            <span class="cf">break</span></a>
<a class="sourceLine" id="cb41-117" data-line-number="117"></a>
<a class="sourceLine" id="cb41-118" data-line-number="118">        s <span class="op">=</span> s_</a></code></pre></div>
<h1 id="gan">GAN</h1>
<div class="sourceCode" id="cb42"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb42-1" data-line-number="1"><span class="im">import</span> torch</a>
<a class="sourceLine" id="cb42-2" data-line-number="2"><span class="im">import</span> torch.nn <span class="im">as</span> nn</a>
<a class="sourceLine" id="cb42-3" data-line-number="3"><span class="im">import</span> numpy <span class="im">as</span> np</a>
<a class="sourceLine" id="cb42-4" data-line-number="4"><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</a>
<a class="sourceLine" id="cb42-5" data-line-number="5"></a>
<a class="sourceLine" id="cb42-6" data-line-number="6"><span class="co"># torch.manual_seed(1)    # reproducible</span></a>
<a class="sourceLine" id="cb42-7" data-line-number="7"><span class="co"># np.random.seed(1)</span></a>
<a class="sourceLine" id="cb42-8" data-line-number="8"></a>
<a class="sourceLine" id="cb42-9" data-line-number="9"><span class="co"># Hyper Parameters</span></a>
<a class="sourceLine" id="cb42-10" data-line-number="10">BATCH_SIZE <span class="op">=</span> <span class="dv">64</span></a>
<a class="sourceLine" id="cb42-11" data-line-number="11">LR_G <span class="op">=</span> <span class="fl">0.0001</span>  <span class="co"># learning rate for generator</span></a>
<a class="sourceLine" id="cb42-12" data-line-number="12">LR_D <span class="op">=</span> <span class="fl">0.0001</span>  <span class="co"># learning rate for discriminator</span></a>
<a class="sourceLine" id="cb42-13" data-line-number="13">N_IDEAS <span class="op">=</span> <span class="dv">5</span>  <span class="co"># 灵感(random noise)的个数</span></a>
<a class="sourceLine" id="cb42-14" data-line-number="14">ART_COMPONENTS <span class="op">=</span> <span class="dv">15</span>  <span class="co"># it could be total point G can draw in the canvas</span></a>
<a class="sourceLine" id="cb42-15" data-line-number="15">PAINT_POINTS <span class="op">=</span> np.vstack([np.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, ART_COMPONENTS) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(BATCH_SIZE)])</a>
<a class="sourceLine" id="cb42-16" data-line-number="16"></a>
<a class="sourceLine" id="cb42-17" data-line-number="17"></a>
<a class="sourceLine" id="cb42-18" data-line-number="18"><span class="co"># show our beautiful painting range</span></a>
<a class="sourceLine" id="cb42-19" data-line-number="19"><span class="co"># plt.plot(PAINT_POINTS[0], 2 * np.power(PAINT_POINTS[0], 2) + 1, c=&#39;#74BCFF&#39;, lw=3, label=&#39;upper bound&#39;)</span></a>
<a class="sourceLine" id="cb42-20" data-line-number="20"><span class="co"># plt.plot(PAINT_POINTS[0], 1 * np.power(PAINT_POINTS[0], 2) + 0, c=&#39;#FF9359&#39;, lw=3, label=&#39;lower bound&#39;)</span></a>
<a class="sourceLine" id="cb42-21" data-line-number="21"><span class="co"># plt.legend(loc=&#39;upper right&#39;)</span></a>
<a class="sourceLine" id="cb42-22" data-line-number="22"><span class="co"># plt.show()</span></a>
<a class="sourceLine" id="cb42-23" data-line-number="23"></a>
<a class="sourceLine" id="cb42-24" data-line-number="24"></a>
<a class="sourceLine" id="cb42-25" data-line-number="25"><span class="kw">def</span> artist_works():  <span class="co"># painting from the famous artist (real target)</span></a>
<a class="sourceLine" id="cb42-26" data-line-number="26">    a <span class="op">=</span> np.random.uniform(<span class="dv">1</span>, <span class="dv">2</span>, size<span class="op">=</span>BATCH_SIZE)[:, np.newaxis]</a>
<a class="sourceLine" id="cb42-27" data-line-number="27">    paintings <span class="op">=</span> a <span class="op">*</span> np.power(PAINT_POINTS, <span class="dv">2</span>) <span class="op">+</span> (a <span class="op">-</span> <span class="dv">1</span>)     <span class="co"># 15个x, a*x^2+a-1</span></a>
<a class="sourceLine" id="cb42-28" data-line-number="28">    paintings <span class="op">=</span> torch.from_numpy(paintings).<span class="bu">float</span>()     <span class="co"># 转化成torch 的形式</span></a>
<a class="sourceLine" id="cb42-29" data-line-number="29">    <span class="cf">return</span> paintings</a>
<a class="sourceLine" id="cb42-30" data-line-number="30"></a>
<a class="sourceLine" id="cb42-31" data-line-number="31"></a>
<a class="sourceLine" id="cb42-32" data-line-number="32">G <span class="op">=</span> nn.Sequential(  <span class="co"># Generator</span></a>
<a class="sourceLine" id="cb42-33" data-line-number="33">    <span class="co"># N_IDEAS=5 -&gt; ART_COMPONENTS=15</span></a>
<a class="sourceLine" id="cb42-34" data-line-number="34">    nn.Linear(N_IDEAS, <span class="dv">128</span>),  <span class="co"># random ideas (could from normal distribution)</span></a>
<a class="sourceLine" id="cb42-35" data-line-number="35">    nn.ReLU(),</a>
<a class="sourceLine" id="cb42-36" data-line-number="36">    nn.Linear(<span class="dv">128</span>, ART_COMPONENTS),  <span class="co"># making a painting from these random ideas</span></a>
<a class="sourceLine" id="cb42-37" data-line-number="37">)</a>
<a class="sourceLine" id="cb42-38" data-line-number="38"></a>
<a class="sourceLine" id="cb42-39" data-line-number="39">D <span class="op">=</span> nn.Sequential(  <span class="co"># Discriminator</span></a>
<a class="sourceLine" id="cb42-40" data-line-number="40">    nn.Linear(ART_COMPONENTS, <span class="dv">128</span>),  <span class="co"># receive art work either from the famous artist or a newbie like G</span></a>
<a class="sourceLine" id="cb42-41" data-line-number="41">    nn.ReLU(),</a>
<a class="sourceLine" id="cb42-42" data-line-number="42">    nn.Linear(<span class="dv">128</span>, <span class="dv">1</span>),</a>
<a class="sourceLine" id="cb42-43" data-line-number="43">    nn.Sigmoid(),  <span class="co"># 转化为判断为真的概率 [0,1]</span></a>
<a class="sourceLine" id="cb42-44" data-line-number="44">)</a>
<a class="sourceLine" id="cb42-45" data-line-number="45"></a>
<a class="sourceLine" id="cb42-46" data-line-number="46">opt_D <span class="op">=</span> torch.optim.Adam(D.parameters(), lr<span class="op">=</span>LR_D)</a>
<a class="sourceLine" id="cb42-47" data-line-number="47">opt_G <span class="op">=</span> torch.optim.Adam(G.parameters(), lr<span class="op">=</span>LR_G)</a>
<a class="sourceLine" id="cb42-48" data-line-number="48"></a>
<a class="sourceLine" id="cb42-49" data-line-number="49">plt.ion()  <span class="co"># something about continuous plotting</span></a>
<a class="sourceLine" id="cb42-50" data-line-number="50"><span class="cf">for</span> step <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10000</span>):</a>
<a class="sourceLine" id="cb42-51" data-line-number="51">    artist_paintings <span class="op">=</span> artist_works()  <span class="co"># real painting from artist</span></a>
<a class="sourceLine" id="cb42-52" data-line-number="52">    G_ideas <span class="op">=</span> torch.randn(BATCH_SIZE, N_IDEAS, requires_grad<span class="op">=</span><span class="va">True</span>)  <span class="co"># random ideas\n</span></a>
<a class="sourceLine" id="cb42-53" data-line-number="53">    G_paintings <span class="op">=</span> G(G_ideas)  <span class="co"># fake painting from G (random ideas)</span></a>
<a class="sourceLine" id="cb42-54" data-line-number="54"></a>
<a class="sourceLine" id="cb42-55" data-line-number="55">    prob_artist1 <span class="op">=</span> D(G_paintings)  <span class="co"># D try to reduce this prob</span></a>
<a class="sourceLine" id="cb42-56" data-line-number="56">    G_loss <span class="op">=</span> torch.mean(torch.log(<span class="fl">1.</span> <span class="op">-</span> prob_artist1))</a>
<a class="sourceLine" id="cb42-57" data-line-number="57"></a>
<a class="sourceLine" id="cb42-58" data-line-number="58">    opt_G.zero_grad()</a>
<a class="sourceLine" id="cb42-59" data-line-number="59">    G_loss.backward()</a>
<a class="sourceLine" id="cb42-60" data-line-number="60">    opt_G.step()</a>
<a class="sourceLine" id="cb42-61" data-line-number="61"></a>
<a class="sourceLine" id="cb42-62" data-line-number="62">    prob_artist0 <span class="op">=</span> D(artist_paintings)  <span class="co"># 真判真概率 D try to increase this prob</span></a>
<a class="sourceLine" id="cb42-63" data-line-number="63">    prob_artist1 <span class="op">=</span> D(G_paintings.detach())  <span class="co"># 假判真概率 D try to reduce this prob</span></a>
<a class="sourceLine" id="cb42-64" data-line-number="64">    D_loss <span class="op">=</span> <span class="op">-</span> torch.mean(torch.log(prob_artist0) <span class="op">+</span> torch.log(<span class="fl">1.</span> <span class="op">-</span> prob_artist1))</a>
<a class="sourceLine" id="cb42-65" data-line-number="65"></a>
<a class="sourceLine" id="cb42-66" data-line-number="66">    opt_D.zero_grad()</a>
<a class="sourceLine" id="cb42-67" data-line-number="67">    D_loss.backward(retain_graph<span class="op">=</span><span class="va">True</span>)  <span class="co"># 保留计算图 reusing computational graph</span></a>
<a class="sourceLine" id="cb42-68" data-line-number="68">    opt_D.step()</a>
<a class="sourceLine" id="cb42-69" data-line-number="69"></a>
<a class="sourceLine" id="cb42-70" data-line-number="70">    <span class="cf">if</span> step <span class="op">%</span> <span class="dv">50</span> <span class="op">==</span> <span class="dv">0</span>:  <span class="co"># plotting</span></a>
<a class="sourceLine" id="cb42-71" data-line-number="71">        plt.cla()</a>
<a class="sourceLine" id="cb42-72" data-line-number="72">        plt.plot(PAINT_POINTS[<span class="dv">0</span>], G_paintings.data.numpy()[<span class="dv">0</span>], c<span class="op">=</span><span class="st">&#39;#4AD631&#39;</span>, lw<span class="op">=</span><span class="dv">3</span>, label<span class="op">=</span><span class="st">&#39;Generated painting&#39;</span>, )</a>
<a class="sourceLine" id="cb42-73" data-line-number="73">        plt.plot(PAINT_POINTS[<span class="dv">0</span>], <span class="dv">2</span> <span class="op">*</span> np.power(PAINT_POINTS[<span class="dv">0</span>], <span class="dv">2</span>) <span class="op">+</span> <span class="dv">1</span>, c<span class="op">=</span><span class="st">&#39;#74BCFF&#39;</span>, lw<span class="op">=</span><span class="dv">3</span>, label<span class="op">=</span><span class="st">&#39;upper bound&#39;</span>)</a>
<a class="sourceLine" id="cb42-74" data-line-number="74">        plt.plot(PAINT_POINTS[<span class="dv">0</span>], <span class="dv">1</span> <span class="op">*</span> np.power(PAINT_POINTS[<span class="dv">0</span>], <span class="dv">2</span>) <span class="op">+</span> <span class="dv">0</span>, c<span class="op">=</span><span class="st">&#39;#FF9359&#39;</span>, lw<span class="op">=</span><span class="dv">3</span>, label<span class="op">=</span><span class="st">&#39;lower bound&#39;</span>)</a>
<a class="sourceLine" id="cb42-75" data-line-number="75">        plt.text(<span class="op">-</span>.<span class="dv">5</span>, <span class="fl">2.3</span>, <span class="st">&#39;D accuracy=</span><span class="sc">%.2f</span><span class="st"> (0.5 for D to converge)&#39;</span> <span class="op">%</span> prob_artist0.data.numpy().mean(),</a>
<a class="sourceLine" id="cb42-76" data-line-number="76">                 fontdict<span class="op">=</span>{<span class="st">&#39;size&#39;</span>: <span class="dv">13</span>})</a>
<a class="sourceLine" id="cb42-77" data-line-number="77">        plt.text(<span class="op">-</span>.<span class="dv">5</span>, <span class="dv">2</span>, <span class="st">&#39;D score= </span><span class="sc">%.2f</span><span class="st"> (-1.38 for G to converge)&#39;</span> <span class="op">%</span> <span class="op">-</span>D_loss.data.numpy(), fontdict<span class="op">=</span>{<span class="st">&#39;size&#39;</span>: <span class="dv">13</span>})</a>
<a class="sourceLine" id="cb42-78" data-line-number="78">        plt.ylim((<span class="dv">0</span>, <span class="dv">3</span>))<span class="op">;</span></a>
<a class="sourceLine" id="cb42-79" data-line-number="79">        plt.legend(loc<span class="op">=</span><span class="st">&#39;upper right&#39;</span>, fontsize<span class="op">=</span><span class="dv">10</span>)<span class="op">;</span></a>
<a class="sourceLine" id="cb42-80" data-line-number="80">        plt.draw()<span class="op">;</span></a>
<a class="sourceLine" id="cb42-81" data-line-number="81">        plt.pause(<span class="fl">0.01</span>)</a>
<a class="sourceLine" id="cb42-82" data-line-number="82"></a>
<a class="sourceLine" id="cb42-83" data-line-number="83">plt.ioff()</a>
<a class="sourceLine" id="cb42-84" data-line-number="84">plt.show()</a></code></pre></div>
<h1 id="reference">Reference</h1>
<ol type="1">
<li>莫烦Python</li>
</ol>
