# 整体架构

只有建立起深度学习推荐系统的知识体系，从**系统的层面**考虑问题，我们才能够实现整体效果上的优化。与你一同从“0”开始，搭建一个**“工业级”**的“深度学习”推荐系统。

在所有业界巨头的推荐引擎都由深度学习驱动的今天，作为一名推荐系统从业者，我们不应该止步于，或者说满足于继续使用协同过滤、矩阵分解这类传统方法，而应该**加深对深度学习模型的理解，加强对大数据平台的熟悉程度，培养结合业务和模型的技术直觉，提高我们整体的技术格局**，这些都是我们取得成功的关键。

- **基础架构篇**：从 0 出发，建立深度学习推荐系统的知识体系
  - 会使用到 Spark、Flink、TensorFlow 这些业界目前最流行的机器学习和大数据框架，麻雀虽小，但五脏俱全。
- **特征工程篇**：又快又好，用心准备推荐系统的“食材”
  - 在特征工程篇中，我会和你一起讨论推荐系统会用到的**特征**，以及主要的**特征处理方式**，并且把它们都实践在 **Spark** 上。除此之外，我还会讲解深度学习中非常流行的 **Embedding、Graph Embedding** 技术。
- **线上服务篇**：实践出真知，掌握搭建工业级推荐系统的核心技能
  - 初步掌握 Jetty Server、Spark、Redis，这些工程领域的核心技能。
- **推荐模型篇**：深度学习推荐系统上的明珠
  - 学习深度学习推荐模型的原理和实现方法，主要包括 Embedding+MLP 、Wide&Deep、PNN 等深度学习模型的架构和 TensorFlow 实现，以及注意力机制、序列模型、增强学习等相关领域的前沿进展。
- **效果评估篇**：建立成体系的推荐系统评估机制
  - 建立起包括线下评估、线上 AB 测试、评估反馈闭环等整套的评估体系，真正能够用业界的方法而不是实验室的指标来评价一个推荐系统。
- **前沿拓展篇**：融会贯通，追踪业界前沿
  - 讲解 YouTube、阿里巴巴、微软、Pinterest 等一线公司的深度学习应用

<img src="/img/in-post/20_07/066c5f56f4e0a5e8d4648e0cfb85e72e.jpg" alt="img" style="zoom: 33%;" />

## 基础架构

### 解决什么问题？

推荐系统要解决的问题用一句话总结就是，**在“信息过载”的情况下，用户如何高效获取感兴趣的信息。**

推荐系统要处理的问题就可以被形式化地定义为：**对于某个用户U（User），在特定场景C（Context）下，针对海量的“物品”信息构建一个函数 ，预测用户对特定候选物品I（Item）的喜好程度，再根据喜好程度对所有候选物品进行排序，生成推荐列表的问题。**

抽象出推荐系统的逻辑框架：

<img src="/img/in-post/20_07/c75969c5fcc6e5e374a87d4b4b1d5d07.jpg" alt="img" style="zoom:25%;" />

### 深度学习到底给推荐系统带来了什么革命性的影响？

其实正是因为深度学习复杂的模型结构，让深度学习模型具备了理论上**拟合任何函数**的能力。

让深度学习模型的神经网络**模拟很多用户兴趣的变迁过程**，甚至用户做出决定的思考过程。

>  比如阿里巴巴的深度学习模型——**深度兴趣进化网络**（如图 3），它利用了三层序列模型的结构，模拟了用户在购买商品时兴趣进化的过程，如此强大的数据拟合能力和对用户行为的理解能力，是传统机器学习模型不具备的。

![img](/img/in-post/20_07/b2606096aa4ff97461dd91b87d748db9.jpg)





<img src="/img/in-post/20_07/0e269ebf95dcb772ed31f9c28cef2aa1.jpeg" alt="img" style="zoom:25%;" />





# spark解决特征处理

## 特征处理方式

### 经典的特征处理方法有什么？

如何利用 One-hot 编码处理类别型特征

广义上来讲，所有的特征都可以分为两大类。

1. 第一类是**类别、ID 型特征**（以下简称类别型特征）。拿电影推荐来说，电影的风格、ID、标签、导演演员等信息，用户看过的电影 ID、用户的性别、地理位置信息、当前的季节、时间（上午，下午，晚上）、天气等等，这些无法用数字表示的信息全都可以被看作是类别、ID 类特征。
2. 第二类是**数值型特征**，能用数字直接表示的特征就是数值型特征，典型的包括用户的年龄、收入、电影的播放时长、点击量、点击率等。

编码方式：

1. 数值型特征 - 直接把这个数值放到特征向量上相应的维度上就可以了

2. 类别特征、ID特征 - 利用onehot编码

   > 比如，**在我们的 SparrowRecsys 中，用户 U 观看过电影 M，**

3. Multi-hot 编码（多热编码）

   > 比如，对于历史行为序列类、标签特征等数据来说，用户往往会**与多个物品产生交互行为**， 或者**一个物品被打上多个标签**，这时最常用的特征向量生成方式就是把其转换成 Multi-hot 编码。在 SparrowRecsys 中，**因为每个电影都是有多个 Genre（风格）类别的**，所以我们就可以**用 Multi-hot 编码完成标签到向量的转换**。你可以自己尝试着用 Spark 实现该过程，也可以🚩参考 SparrowRecsys 项目中 `multiHotEncoderExample` 的实现，我就不多说啦。

### SparrowRecsys 是如何利用 Spark 完成这一过程的

这里，我们使用 Spark 的机器学习库 `MLlib` 来完成 One-hot 特征的处理。其中，**最主要的步骤是**，

1. 我们先创建一个负责 One-hot 编码的转换器，`OneHotEncoderEstimator`，
2. 然后通过它的 `fit` 函数完成指定特征的预处理，
3. 并利用 `transform` 函数将原始特征转换成 One-hot 特征。

实现思路大体上就是这样，具体的步骤可以参考下面给出的源码：

```java
def oneHotEncoderExample(samples:DataFrame): Unit ={
  //samples样本集中的每一条数据代表一部电影的信息，其中movieId为电影id
  val samplesWithIdNumber = samples.withColumn("movieIdNumber", col("movieId").cast(sql.types.IntegerType))


  //利用Spark的机器学习库Spark MLlib创建One-hot编码器
  val oneHotEncoder = new OneHotEncoderEstimator()
    .setInputCols(Array("movieIdNumber"))
    .setOutputCols(Array("movieIdVector"))
    .setDropLast(false)


  //训练One-hot编码器，并完成从id特征到One-hot向量的转换
  val oneHotEncoderSamples =      oneHotEncoder.fit(samplesWithIdNumber).transform(samplesWithIdNumber)
  //打印最终样本的数据结构
  oneHotEncoderSamples.printSchema()
  //打印10条样本查看结果
  oneHotEncoderSamples.show(10)

_（参考 com.wzhe.sparrowrecsys.offline.spark.featureeng.FeatureEngineering__中的oneHotEncoderExample函数）_
```

## 数值型特征的处理 - 归一化和分桶

### 数值型特征本身不就是数字，为什么还要处理呢？

一是**特征的尺度**，二是**特征的分布**。

1. 特征的尺度问题不难理解，由于特征的尺度差距太大，如果我们把特征的原始数值直接输入推荐模型，就会导致这**特征对于模型的影响程度有显著的区别**。

> 比如在电影推荐中有两个特征，一个是电影的评价次数 fr，一个是电影的平均评分 fs。评价次数其实是一个数值无上限的特征，在 SparrowRecsys 所用 MovieLens 数据集上，fr 的范围一般在[0,10000]之间。对于电影的平均评分来说，因为我们采用了 5 分为满分的评分，所以特征 fs 的取值范围在[0,5]之间。
>
> 由于 fr 和 fs 两个特征的尺度差距太大，如果我们把特征的原始数值直接输入推荐模型，就会导致这**两个特征对于模型的影响程度有显著的区别**。如果模型中未做特殊处理的话，fr 这个特征由于波动范围高出 fs 几个量级，可能会完全掩盖 fs 作用，这当然是我们不愿意看到的。为此我们希望把两个特征的尺度拉平到一个区域内，通常是[0,1]范围，这就是所谓**归一化。**

2. 归一化虽然能够解决特征取值范围不统一的问题，**但无法改变特征值的分布**。

> 比如图 5 就显示了 Sparrow Recsys 中编号在前 1000 的电影平均评分分布。你可以很明显地看到，由于人们打分有“中庸偏上”的倾向，因此评分大量集中在 3.5 的附近，而且越<u>靠近 3.5 的密度越大。这对于模型学习来说也不是一个好的现象，因为特征的区分度并不高。</u>
>
> <img src="/img/in-post/20_07/5675f0777bd9275b5cdd8aa166cebd4e.jpeg" alt="img" style="zoom:50%;" />
>
> 图5 电影的平均评分分布

### 这该怎么解决呢？

我们经常会用分桶的方式来解决**特征值分布极不均匀的问题**。所谓“**分桶**（Bucketing）”，<u>就是将样本按照某特征的值从高到低排序，然后按照桶的数量找到分位数，将样本分到各自的桶中，再用桶 ID 作为特征值。</u>

在 Spark MLlib 中，分别提供了两个转换器 `MinMaxScaler` 和 `QuantileDiscretizer`，来进行归一化和分桶的特征处理。它们的使用方法和之前介绍的 OneHotEncoderEstimator 一样，都是：

1. 先用 fit 函数进行数据预处理，
2. 再用 transform 函数完成特征转换。

下面的代码就是 SparrowRecSys 利用这两个转换器完成特征归一化和分桶的过程。

```java
def ratingFeatures(samples:DataFrame): Unit ={
  samples.printSchema()
  samples.show(10)


  //利用打分表ratings计算电影的平均分、被打分次数等数值型特征
  val movieFeatures = samples.groupBy(col("movieId"))
    .agg(count(lit(1)).as("ratingCount"),
      avg(col("rating")).as("avgRating"),
      variance(col("rating")).as("ratingVar"))
      .withColumn("avgRatingVec", double2vec(col("avgRating")))


  movieFeatures.show(10)


  //分桶处理，创建QuantileDiscretizer进行分桶，将打分次数这一特征分到100个桶中
  val ratingCountDiscretizer = new QuantileDiscretizer()
    .setInputCol("ratingCount")
    .setOutputCol("ratingCountBucket")
    .setNumBuckets(100)


  //归一化处理，创建MinMaxScaler进行归一化，将平均得分进行归一化
  val ratingScaler = new MinMaxScaler()
    .setInputCol("avgRatingVec")
    .setOutputCol("scaleAvgRating")


  //创建一个pipeline，依次执行两个特征处理过程
  val pipelineStage: Array[PipelineStage] = Array(ratingCountDiscretizer, ratingScaler)
  val featurePipeline = new Pipeline().setStages(pipelineStage)


  val movieProcessedFeatures = featurePipeline.fit(movieFeatures).transform(movieFeatures)
  //打印最终结果
  movieProcessedFeatures.show(

_（参考 com.wzhe.sparrowrecsys.offline.spark.featureeng.FeatureEngineering中的ratingFeatures函数）_
```

## 其他特征值处理方式-YouTube 深度推荐模型中

在经典的 YouTube 深度推荐模型中，我们就可以看到一些很有意思的处理方法。比如，在处理观看时间间隔（time since last watch）和视频曝光量（#previous impressions）这两个特征的时，YouTube 模型对它们进行归一化后，又将它们各自处理成了三个特征（图 6 中红框内的部分），分别是**原特征值 x，特征值的平方x^2，以及特征值的开方**，这又是为什么呢？

<img src="/img/in-post/20_07/69f2abc980b8d8448867b58468729eae.jpeg" alt="img" style="zoom:50%;" />

其实，无论是平方还是开方操作，改变的还是这个特征值的分布，这些操作与分桶操作一样，都是希望通过改变特征的分布，**让模型能够更好地学习到特征内包含的有价值信息**。但由于我们没法通过人工的经验判断哪种特征处理方式更好，所以索性把它们都输入模型，让模型来做选择。

这里其实自然而然地引出了我们进行特征处理的一个原则，就是**特征处理并没有标准答案**，不存在一种特征处理方式是一定好于另一种的。

## 总结

<img src="/img/in-post/20_07/b3b8c959df72ce676ae04bd8dd987e7b.jpeg" alt="img" style="zoom: 33%;" />

## 问题-

### 查阅一下 Spark MLlib 的编程手册，找出 Normalizer、StandardScaler、RobustScaler、MinMaxScaler 这个几个特征处理方法有什么不同。

Normalizer、StandardScaler、RobustScaler、MinMaxScaler 都是用让数据无量纲化
Normalizer: 正则化；（和Python的sklearn一样是按行处理，而不是按列[每一列是一个特征]处理，原因是：Normalization主要思想是对每个样本计算其p-范数，然后对该样本中每个元素除以该范数，这样处理的结果是使得每个处理后样本的p-范数(l1-norm,l2-norm)等于1。）针对每行样本向量：l1: 每个元素/样本中每个元素绝对值的和，l2: 每个元素/样本中每个元素的平方和开根号，lp: 每个元素/每个元素的p次方和的p次根，默认用l2范数。

StandardScaler：数据标准化；(xi - u) / σ 【u:均值，σ：方差】当数据(x)按均值(μ)中心化后，再按标准差(σ)缩放，数据就会服从为均值为0，方差为1的正态分布（即标准正态分布）。

RobustScaler: (xi - median) / IQR 【median是样本的中位数，IQR是样本的 四分位距：根据第1个四分位数和第3个四分位数之间的范围来缩放数据】

MinMaxScaler：数据归一化，(xi - min(x)) / (max(x) - min(x)) ;当数据(x)按照最小值中心化后，再按极差（最大值 - 最小值）缩放，数据移动了最小值个单位，并且会被收敛到 [0,1]之间





# Embedding技术

用 Embedding 方法进行**相似物品推荐**，几乎成了业界最流行的做法

## 基础

### 什么是Embedding？

Embedding 就是用一个**数值向量“表示”一个对象**（Object）的方法

Netflix 应用的**电影 Embedding 向量方法**，就是一个非常直接的推荐系统应用。从 Netflix 利用**矩阵分解方法**生成的电影和用户的 Embedding 向量示意图中，我们可以看出不同的电影和用户分布在一个二维的空间内，<u>由于 Embedding 向量保存了它们之间的相似性关系</u>，因此有了这个 Embedding 空间之后，我们再进行电影推荐就非常容易了。具体来说就是，我们<u>直接找出某个用户向量周围的电影向量，然后把这些电影推荐给这个用户就可以了</u>。这就是 Embedding 技术在推荐系统中最直接的应用。

### Embedding 技术对深度学习推荐系统的重要性？

首先，Embedding 是处理稀疏特征的利器。

> 因为推荐场景中的类别、ID 型特征非常多，大量使用 **One-hot** 编码会**导致样本特征向量极度稀疏**，而**深度学习的结构特点又不利于稀疏特征向量的处理**，因此几乎所有深度学习推荐模型都会**由 Embedding 层负责将稀疏高维特征向量转换成稠密低维特征向量**。所以说各类 Embedding 技术是构建深度学习推荐模型的基础性操作。

其次，Embedding 可以**融合大量有价值信息，本身就是极其重要的特征向量** 。 

> 相比由原始信息直接处理得来的特征向量，Embedding 的**表达能力更强**，特别是 Graph Embedding 技术被提出后，Embedding **几乎可以引入任何信息进行编码**，使其本身就包含大量有价值的信息，所以**通过预训练得到的 Embedding 向量**本身就是极其重要的特征向量。

### 什么是 Word2vec？

经典的 Embedding 方法，Word2vec。

想要训练 Word2vec 模型，我们需要准备由一组**句子组成的语料库**。假设其中一个长度为 T 的句子包含的词有 w1,w2……wt，并且我们假定每个词都跟其相邻词的关系最密切。

根据模型假设的不同，Word2vec 模型分为两种形式，**CBOW 模型（图 3 左）和 Skip-gram 模型**（图 3 右）。

其中，CBOW 模型假设句子中**每个词的选取都由相邻的词决定**，因此我们就看到 CBOW 模型的**输入是 wt周边的词，预测的输出是 wt**。

Skip-gram 模型则正好相反，它假设句子中的**每个词都决定了相邻词的选取**，所以你可以看到 Skip-gram 模型的输入是 wt，预测的输出是 wt周边的词。

按照一般的经验，<u>Skip-gram 模型的效果会更好一些</u>，所以我接下来也会以 Skip-gram 作为框架，来给你讲讲 Word2vec 的模型细节。

<img src="/img/in-post/20_07/f28a06f57e4aeb5f826df466cbe6288a.jpeg" alt="img" style="zoom:50%;" />

### Word2vec 的样本是怎么生成的？

作为一个自然语言处理的模型，训练 Word2vec 的样本当然来自于**语料库**，比如我们想训练一个电商网站中**关键词**的 Embedding 模型，那么**电商网站中所有物品的描述文字就是很好的语料库**。

我们从语料库中抽取一个句子，选取一个长度为 2c+1（目标词前后各选 c 个词）的**滑动窗口**，将滑动窗口由左至右滑动，每**移动**一次，窗口中的<u>词组就形成了一个训练样本</u>。根据 Skip-gram 模型的理念，中心词决定了它的相邻词，我们就可以根据这个训练样本定义出 Word2vec 模型的输入和输出，**输入是样本的中心词，输出是所有的相邻词**。

为了方便你理解，我再举一个例子。这里我们选取了“Embedding 技术对深度学习推荐系统的重要性”作为句子样本。

1. 首先，我们对它进行**分词、去除停用词**的过程，**生成词序列**，
2. 再选取大小为 3 的滑动窗口从头到尾依次**滑动生成训练样本**，
3. 然后我们把**中心词当输入，边缘词做输出**，就得到了训练 Word2vec 模型可用的训练样本。

<img src="/img/in-post/20_07/e84e1bd1f7c5950fb70ed63dda0yy21f.jpeg" alt="img" style="zoom:50%;" />



### Word2vec 模型的结构是什么样的？

它的结构本质上就是一个三层的神经网络（如图 5）。

<img src="/img/in-post/20_07/9997c61588223af2e8c0b9b2b8e77139-20210527185126186.jpeg" alt="img" style="zoom:50%;" />

它的**输入层和输出层的维度都是 V**，这个 V 其实就是**语料库词典的大小**。假设语料库一共使用了 10000 个词，那么 V 就等于 10000。根据图 4 生成的训练样本，这里的**输入向量**自然就是<u>由输入词转换而来的 One-hot 编码向量</u>，**输出向量**则是<u>由多个输出词转换而来的 Multi-hot 编码向量</u>，显然，基于 Skip-gram 框架的 Word2vec 模型解决的是一个**多分类问题。**

> 🚩Multi-hot 编码向量：  [0,0,1,1,0,1,0,0,0]这种吧

隐层的维度是 N，N 的选择就需要一定的**调参能力**了，我们需要对模型的**效果和模型的复杂度进行权衡**，来决定最后 N 的取值，**并且最终每个词的 Embedding 向量维度也由 N 来决定**。<font color="#dd0000">浅红色文字：</font><br /> 

最后是激活函数的问题，这里我们需要注意的是，隐层神经元是没有激活函数的，或者说采用了输入即输出的恒等函数作为激活函数，而输出层神经元采用了 softmax 作为激活函数。



















