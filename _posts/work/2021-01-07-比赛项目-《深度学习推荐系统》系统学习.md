# 整体架构

只有建立起深度学习推荐系统的知识体系，从**系统的层面**考虑问题，我们才能够实现整体效果上的优化。与你一同从“0”开始，搭建一个**“工业级”**的“深度学习”推荐系统。

在所有业界巨头的推荐引擎都由深度学习驱动的今天，作为一名推荐系统从业者，我们不应该止步于，或者说满足于继续使用协同过滤、矩阵分解这类传统方法，而应该**加深对深度学习模型的理解，加强对大数据平台的熟悉程度，培养结合业务和模型的技术直觉，提高我们整体的技术格局**，这些都是我们取得成功的关键。

- **基础架构篇**：从 0 出发，建立深度学习推荐系统的知识体系
  - 会使用到 Spark、Flink、TensorFlow 这些业界目前最流行的机器学习和大数据框架，麻雀虽小，但五脏俱全。
- **特征工程篇**：又快又好，用心准备推荐系统的“食材”
  - 在特征工程篇中，我会和你一起讨论推荐系统会用到的**特征**，以及主要的**特征处理方式**，并且把它们都实践在 **Spark** 上。除此之外，我还会讲解深度学习中非常流行的 **Embedding、Graph Embedding** 技术。
- **线上服务篇**：实践出真知，掌握搭建工业级推荐系统的核心技能
  - 初步掌握 Jetty Server、Spark、Redis，这些工程领域的核心技能。
- **推荐模型篇**：深度学习推荐系统上的明珠
  - 学习深度学习推荐模型的原理和实现方法，主要包括 Embedding+MLP 、Wide&Deep、PNN 等深度学习模型的架构和 TensorFlow 实现，以及注意力机制、序列模型、增强学习等相关领域的前沿进展。
- **效果评估篇**：建立成体系的推荐系统评估机制
  - 建立起包括线下评估、线上 AB 测试、评估反馈闭环等整套的评估体系，真正能够用业界的方法而不是实验室的指标来评价一个推荐系统。
- **前沿拓展篇**：融会贯通，追踪业界前沿
  - 讲解 YouTube、阿里巴巴、微软、Pinterest 等一线公司的深度学习应用

<img src="/img/in-post/20_07/066c5f56f4e0a5e8d4648e0cfb85e72e.jpg" alt="img" style="zoom: 33%;" />

## 基础架构

### 解决什么问题？

推荐系统要解决的问题用一句话总结就是，**在“信息过载”的情况下，用户如何高效获取感兴趣的信息。**

推荐系统要处理的问题就可以被形式化地定义为：**对于某个用户U（User），在特定场景C（Context）下，针对海量的“物品”信息构建一个函数 ，预测用户对特定候选物品I（Item）的喜好程度，再根据喜好程度对所有候选物品进行排序，生成推荐列表的问题。**

抽象出推荐系统的逻辑框架：

<img src="/img/in-post/20_07/c75969c5fcc6e5e374a87d4b4b1d5d07.jpg" alt="img" style="zoom:25%;" />

### 深度学习到底给推荐系统带来了什么革命性的影响？

其实正是因为深度学习复杂的模型结构，让深度学习模型具备了理论上**拟合任何函数**的能力。

让深度学习模型的神经网络**模拟很多用户兴趣的变迁过程**，甚至用户做出决定的思考过程。

>  比如阿里巴巴的深度学习模型——**深度兴趣进化网络**（如图 3），它利用了三层序列模型的结构，模拟了用户在购买商品时兴趣进化的过程，如此强大的数据拟合能力和对用户行为的理解能力，是传统机器学习模型不具备的。

![img](/img/in-post/20_07/b2606096aa4ff97461dd91b87d748db9.jpg)





<img src="/img/in-post/20_07/0e269ebf95dcb772ed31f9c28cef2aa1.jpeg" alt="img" style="zoom:25%;" />





# spark解决特征处理

## 特征处理方式

### 经典的特征处理方法有什么？

如何利用 One-hot 编码处理类别型特征

广义上来讲，所有的特征都可以分为两大类。

1. 第一类是**类别、ID 型特征**（以下简称类别型特征）。拿电影推荐来说，电影的风格、ID、标签、导演演员等信息，用户看过的电影 ID、用户的性别、地理位置信息、当前的季节、时间（上午，下午，晚上）、天气等等，这些无法用数字表示的信息全都可以被看作是类别、ID 类特征。
2. 第二类是**数值型特征**，能用数字直接表示的特征就是数值型特征，典型的包括用户的年龄、收入、电影的播放时长、点击量、点击率等。

编码方式：

1. 数值型特征 - 直接把这个数值放到特征向量上相应的维度上就可以了

2. 类别特征、ID特征 - 利用onehot编码

   > 比如，**在我们的 SparrowRecsys 中，用户 U 观看过电影 M，**

3. Multi-hot 编码（多热编码）

   > 比如，对于历史行为序列类、标签特征等数据来说，用户往往会**与多个物品产生交互行为**， 或者**一个物品被打上多个标签**，这时最常用的特征向量生成方式就是把其转换成 Multi-hot 编码。在 SparrowRecsys 中，**因为每个电影都是有多个 Genre（风格）类别的**，所以我们就可以**用 Multi-hot 编码完成标签到向量的转换**。你可以自己尝试着用 Spark 实现该过程，也可以🚩参考 SparrowRecsys 项目中 `multiHotEncoderExample` 的实现，我就不多说啦。

### SparrowRecsys 是如何利用 Spark 完成这一过程的

这里，我们使用 Spark 的机器学习库 `MLlib` 来完成 One-hot 特征的处理。其中，**最主要的步骤是**，

1. 我们先创建一个负责 One-hot 编码的转换器，`OneHotEncoderEstimator`，
2. 然后通过它的 `fit` 函数完成指定特征的预处理，
3. 并利用 `transform` 函数将原始特征转换成 One-hot 特征。

实现思路大体上就是这样，具体的步骤可以参考下面给出的源码：

```java
def oneHotEncoderExample(samples:DataFrame): Unit ={
  //samples样本集中的每一条数据代表一部电影的信息，其中movieId为电影id
  val samplesWithIdNumber = samples.withColumn("movieIdNumber", col("movieId").cast(sql.types.IntegerType))


  //利用Spark的机器学习库Spark MLlib创建One-hot编码器
  val oneHotEncoder = new OneHotEncoderEstimator()
    .setInputCols(Array("movieIdNumber"))
    .setOutputCols(Array("movieIdVector"))
    .setDropLast(false)


  //训练One-hot编码器，并完成从id特征到One-hot向量的转换
  val oneHotEncoderSamples =      oneHotEncoder.fit(samplesWithIdNumber).transform(samplesWithIdNumber)
  //打印最终样本的数据结构
  oneHotEncoderSamples.printSchema()
  //打印10条样本查看结果
  oneHotEncoderSamples.show(10)

_（参考 com.wzhe.sparrowrecsys.offline.spark.featureeng.FeatureEngineering__中的oneHotEncoderExample函数）_
```

## 数值型特征的处理 - 归一化和分桶

### 数值型特征本身不就是数字，为什么还要处理呢？

一是**特征的尺度**，二是**特征的分布**。

1. 特征的尺度问题不难理解，由于特征的尺度差距太大，如果我们把特征的原始数值直接输入推荐模型，就会导致这**特征对于模型的影响程度有显著的区别**。

> 比如在电影推荐中有两个特征，一个是电影的评价次数 fr，一个是电影的平均评分 fs。评价次数其实是一个数值无上限的特征，在 SparrowRecsys 所用 MovieLens 数据集上，fr 的范围一般在[0,10000]之间。对于电影的平均评分来说，因为我们采用了 5 分为满分的评分，所以特征 fs 的取值范围在[0,5]之间。
>
> 由于 fr 和 fs 两个特征的尺度差距太大，如果我们把特征的原始数值直接输入推荐模型，就会导致这**两个特征对于模型的影响程度有显著的区别**。如果模型中未做特殊处理的话，fr 这个特征由于波动范围高出 fs 几个量级，可能会完全掩盖 fs 作用，这当然是我们不愿意看到的。为此我们希望把两个特征的尺度拉平到一个区域内，通常是[0,1]范围，这就是所谓**归一化。**

2. 归一化虽然能够解决特征取值范围不统一的问题，**但无法改变特征值的分布**。

> 比如图 5 就显示了 Sparrow Recsys 中编号在前 1000 的电影平均评分分布。你可以很明显地看到，由于人们打分有“中庸偏上”的倾向，因此评分大量集中在 3.5 的附近，而且越<u>靠近 3.5 的密度越大。这对于模型学习来说也不是一个好的现象，因为特征的区分度并不高。</u>
>
> <img src="/img/in-post/20_07/5675f0777bd9275b5cdd8aa166cebd4e.jpeg" alt="img" style="zoom:50%;" />
>
> 图5 电影的平均评分分布

### 这该怎么解决呢？

我们经常会用分桶的方式来解决**特征值分布极不均匀的问题**。所谓“**分桶**（Bucketing）”，<u>就是将样本按照某特征的值从高到低排序，然后按照桶的数量找到分位数，将样本分到各自的桶中，再用桶 ID 作为特征值。</u>

在 Spark MLlib 中，分别提供了两个转换器 `MinMaxScaler` 和 `QuantileDiscretizer`，来进行归一化和分桶的特征处理。它们的使用方法和之前介绍的 OneHotEncoderEstimator 一样，都是：

1. 先用 fit 函数进行数据预处理，
2. 再用 transform 函数完成特征转换。

下面的代码就是 SparrowRecSys 利用这两个转换器完成特征归一化和分桶的过程。

```java
def ratingFeatures(samples:DataFrame): Unit ={
  samples.printSchema()
  samples.show(10)


  //利用打分表ratings计算电影的平均分、被打分次数等数值型特征
  val movieFeatures = samples.groupBy(col("movieId"))
    .agg(count(lit(1)).as("ratingCount"),
      avg(col("rating")).as("avgRating"),
      variance(col("rating")).as("ratingVar"))
      .withColumn("avgRatingVec", double2vec(col("avgRating")))


  movieFeatures.show(10)


  //分桶处理，创建QuantileDiscretizer进行分桶，将打分次数这一特征分到100个桶中
  val ratingCountDiscretizer = new QuantileDiscretizer()
    .setInputCol("ratingCount")
    .setOutputCol("ratingCountBucket")
    .setNumBuckets(100)


  //归一化处理，创建MinMaxScaler进行归一化，将平均得分进行归一化
  val ratingScaler = new MinMaxScaler()
    .setInputCol("avgRatingVec")
    .setOutputCol("scaleAvgRating")


  //创建一个pipeline，依次执行两个特征处理过程
  val pipelineStage: Array[PipelineStage] = Array(ratingCountDiscretizer, ratingScaler)
  val featurePipeline = new Pipeline().setStages(pipelineStage)


  val movieProcessedFeatures = featurePipeline.fit(movieFeatures).transform(movieFeatures)
  //打印最终结果
  movieProcessedFeatures.show(

_（参考 com.wzhe.sparrowrecsys.offline.spark.featureeng.FeatureEngineering中的ratingFeatures函数）_
```

## 其他特征值处理方式-YouTube 深度推荐模型中

在经典的 YouTube 深度推荐模型中，我们就可以看到一些很有意思的处理方法。比如，在处理观看时间间隔（time since last watch）和视频曝光量（#previous impressions）这两个特征的时，YouTube 模型对它们进行归一化后，又将它们各自处理成了三个特征（图 6 中红框内的部分），分别是**原特征值 x，特征值的平方x^2，以及特征值的开方**，这又是为什么呢？

<img src="/img/in-post/20_07/69f2abc980b8d8448867b58468729eae.jpeg" alt="img" style="zoom:50%;" />

其实，无论是平方还是开方操作，改变的还是这个特征值的分布，这些操作与分桶操作一样，都是希望通过改变特征的分布，**让模型能够更好地学习到特征内包含的有价值信息**。但由于我们没法通过人工的经验判断哪种特征处理方式更好，所以索性把它们都输入模型，让模型来做选择。

这里其实自然而然地引出了我们进行特征处理的一个原则，就是**特征处理并没有标准答案**，不存在一种特征处理方式是一定好于另一种的。

## 总结

<img src="/img/in-post/20_07/b3b8c959df72ce676ae04bd8dd987e7b.jpeg" alt="img" style="zoom: 33%;" />

## 问题-

### 查阅一下 Spark MLlib 的编程手册，找出 Normalizer、StandardScaler、RobustScaler、MinMaxScaler 这个几个特征处理方法有什么不同。

Normalizer、StandardScaler、RobustScaler、MinMaxScaler 都是用让数据无量纲化
Normalizer: 正则化；（和Python的sklearn一样是按行处理，而不是按列[每一列是一个特征]处理，原因是：Normalization主要思想是对每个样本计算其p-范数，然后对该样本中每个元素除以该范数，这样处理的结果是使得每个处理后样本的p-范数(l1-norm,l2-norm)等于1。）针对每行样本向量：l1: 每个元素/样本中每个元素绝对值的和，l2: 每个元素/样本中每个元素的平方和开根号，lp: 每个元素/每个元素的p次方和的p次根，默认用l2范数。

StandardScaler：数据标准化；(xi - u) / σ 【u:均值，σ：方差】当数据(x)按均值(μ)中心化后，再按标准差(σ)缩放，数据就会服从为均值为0，方差为1的正态分布（即标准正态分布）。

RobustScaler: (xi - median) / IQR 【median是样本的中位数，IQR是样本的 四分位距：根据第1个四分位数和第3个四分位数之间的范围来缩放数据】

MinMaxScaler：数据归一化，(xi - min(x)) / (max(x) - min(x)) ;当数据(x)按照最小值中心化后，再按极差（最大值 - 最小值）缩放，数据移动了最小值个单位，并且会被收敛到 [0,1]之间





# Embedding技术

用 Embedding 方法进行**相似物品推荐**，几乎成了业界最流行的做法

## 基础

### 什么是Embedding？

Embedding 就是用一个**数值向量“表示”一个对象**（Object）的方法

Netflix 应用的**电影 Embedding 向量方法**，就是一个非常直接的推荐系统应用。从 Netflix 利用**矩阵分解方法**生成的电影和用户的 Embedding 向量示意图中，我们可以看出不同的电影和用户分布在一个二维的空间内，<u>由于 Embedding 向量保存了它们之间的相似性关系</u>，因此有了这个 Embedding 空间之后，我们再进行电影推荐就非常容易了。具体来说就是，我们<u>直接找出某个用户向量周围的电影向量，然后把这些电影推荐给这个用户就可以了</u>。这就是 Embedding 技术在推荐系统中最直接的应用。

### Embedding 技术对深度学习推荐系统的重要性？

首先，Embedding 是处理稀疏特征的利器。

> 因为推荐场景中的类别、ID 型特征非常多，大量使用 **One-hot** 编码会**导致样本特征向量极度稀疏**，而**深度学习的结构特点又不利于稀疏特征向量的处理**，因此几乎所有深度学习推荐模型都会**由 Embedding 层负责将稀疏高维特征向量转换成稠密低维特征向量**。所以说各类 Embedding 技术是构建深度学习推荐模型的基础性操作。

其次，Embedding 可以**融合大量有价值信息，本身就是极其重要的特征向量** 。 

> 相比由原始信息直接处理得来的特征向量，Embedding 的**表达能力更强**，特别是 Graph Embedding 技术被提出后，Embedding **几乎可以引入任何信息进行编码**，使其本身就包含大量有价值的信息，所以**通过预训练得到的 Embedding 向量**本身就是极其重要的特征向量。

### 什么是 Word2vec？

经典的 Embedding 方法，Word2vec。

想要训练 Word2vec 模型，我们需要准备由一组**句子组成的语料库**。假设其中一个长度为 T 的句子包含的词有 w1,w2……wt，并且我们假定每个词都跟其相邻词的关系最密切。

根据模型假设的不同，Word2vec 模型分为两种形式，**CBOW 模型（图 3 左）和 Skip-gram 模型**（图 3 右）。

其中，CBOW 模型假设句子中**每个词的选取都由相邻的词决定**，因此我们就看到 CBOW 模型的**输入是 wt周边的词，预测的输出是 wt**。

Skip-gram 模型则正好相反，它假设句子中的**每个词都决定了相邻词的选取**，所以你可以看到 Skip-gram 模型的输入是 wt，预测的输出是 wt周边的词。

按照一般的经验，<u>Skip-gram 模型的效果会更好一些</u>，所以我接下来也会以 Skip-gram 作为框架，来给你讲讲 Word2vec 的模型细节。

<img src="/img/in-post/20_07/f28a06f57e4aeb5f826df466cbe6288a.jpeg" alt="img" style="zoom:50%;" />

### Word2vec 的样本是怎么生成的？

作为一个自然语言处理的模型，训练 Word2vec 的样本当然来自于**语料库**，比如我们想训练一个电商网站中**关键词**的 Embedding 模型，那么**电商网站中所有物品的描述文字就是很好的语料库**。

我们从语料库中抽取一个句子，选取一个长度为 2c+1（目标词前后各选 c 个词）的**滑动窗口**，将滑动窗口由左至右滑动，每**移动**一次，窗口中的<u>词组就形成了一个训练样本</u>。根据 Skip-gram 模型的理念，中心词决定了它的相邻词，我们就可以根据这个训练样本定义出 Word2vec 模型的输入和输出，**输入是样本的中心词，输出是所有的相邻词**。

为了方便你理解，我再举一个例子。这里我们选取了“Embedding 技术对深度学习推荐系统的重要性”作为句子样本。

1. 首先，我们对它进行**分词、去除停用词**的过程，**生成词序列**，
2. 再选取大小为 3 的滑动窗口从头到尾依次**滑动生成训练样本**，
3. 然后我们把**中心词当输入，边缘词做输出**，就得到了训练 Word2vec 模型可用的训练样本。

<img src="/img/in-post/20_07/e84e1bd1f7c5950fb70ed63dda0yy21f.jpeg" alt="img" style="zoom:50%;" />



### Word2vec 模型的结构是什么样的？

它的结构本质上就是一个三层的神经网络（如图 5）。

<img src="/img/in-post/20_07/9997c61588223af2e8c0b9b2b8e77139-20210527185126186.jpeg" alt="img" style="zoom:50%;" />

它的**输入层和输出层的维度都是 V**，这个 V 其实就是**语料库词典的大小**。假设语料库一共使用了 10000 个词，那么 V 就等于 10000。根据图 4 生成的训练样本，这里的**输入向量**自然就是<u>由输入词转换而来的 One-hot 编码向量</u>，**输出向量**则是<u>由多个输出词转换而来的 Multi-hot 编码向量</u>，显然，基于 Skip-gram 框架的 Word2vec 模型解决的是一个**多分类问题。**

> 🚩Multi-hot 编码向量：  [0,0,1,1,0,1,0,0,0]这种吧

隐层的维度是 N，N 的选择就需要一定的**调参能力**了，我们需要对模型的**效果和模型的复杂度进行权衡**，来决定最后 N 的取值，<font color="red">**并且最终每个词的 Embedding 向量维度也由 N 来决定**。</font><br />

最后是激活函数的问题，这里我们需要注意的是，**隐层神经元是没有激活函数的**，或者说采用了输入即输出的恒等函数作为激活函数，而**输出层神经元采用了 softmax 作为激活函数**。

你可能会问为什么要这样设置 Word2vec 的神经网络，以及我们为什么要这样选择激活函数呢？因为这个神经网络其实是为了表达从输入向量到输出向量的这样的一个条件概率关系，我们看下面的式子：

<img src="/img/in-post/20_07/image-20210527193602602.png" alt="image-20210527193602602" style="zoom:50%;" />

这个由**输入词 WI 预测输出词 WO 的条件概率**，其实就是 Word2vec 神经网络要表达的东西。我们通过<u>**极大似然的方法**去最大化这个条件概率，就能够让相似的词的内积距离更接近，这就是我们希望 Word2vec 神经网络学到的。</u>



> 归一化指数函数（softmax）中，样本向量x属于第j类的概率是：
>
> ![img](/img/in-post/20_07/429a10437a03eca44e9f622211c76555.svg)
>
> 与上式相同。其在多种**基于概率的多分类问题**方法中都有着广泛应用。 
>
> Softmax 就是 Soft 版本的 ArgMax，“softmax 的作用是把 一个序列，变成概率，即被选为 max 的概率。
>
> **交叉熵损失函数**是搭配softmax使用的**损失函数**。

### softmax 与 交叉熵的关系：

> softmax是激活函数，交叉熵是损失函数。一句话概括：
>
> **softmax**把分类输出**标准化成概率分布**
>
> **cross-entropy（**交叉熵**）**刻画预测分类和真实结果之间的**相似度**。

### 如何节约word2vec的时间

参考论文：[Word2vec Parameter Learning Explained](https://github.com/wzhe06/Reco-papers/blob/master/Embedding/%5BWord2Vec%5D%20Word2vec%20Parameter%20Learning%20Explained%20%28UMich%202016%29.pdf)

如果你是一个理论派，其实 Word2vec 还有很多值得挖掘的东西，比如，为了节约训练时间，Word2vec 经常会采用**负采样**（Negative Sampling）或者**分层 softmax（Hierarchical Softmax）的训练方法**。关于这一点，我推荐你去阅读《Word2vec Parameter Learning Explained》这篇文章，相信你会找到最详细和准确的解释。

### 怎样把词向量从 Word2vec 模型中提取出来？

<font color="red">这个 Embedding 在哪呢？</font>其实，它就藏在**输入层到隐层的权重矩阵 WVxN** 中。

<img src="/img/in-post/20_07/0de188f4b564de8076cf13ba6ff87872.jpeg" alt="img" style="zoom:50%;" />

你可以看到，**输入向量矩阵 WVxN 的每一个行向量对应的就是我们要找的“词向量”**。比如我们要找词典里第 i 个词对应的 Embedding，因为输入向量是采用 One-hot 编码的，所以输入向量的第 i 维就应该是 1，那么输入向量矩阵 WVxN 中第 i 行的行向量自然就是该词的 Embedding 啦。输出向量矩阵 W′ 也遵循这个道理，确实是这样的，但一般来说，我们还是习惯于**使用输入向量矩阵作为词向量矩阵**。

在实际的使用过程中，我们往往会把输入向量矩阵转换成**词向量查找表（Lookup table，如图 7 所示）**。

> 例如，输入向量是 10000 个词组成的 One-hot 向量，隐层维度是 300 维，那么输入层到隐层的权重矩阵为 10000x300 维。
>
> 

在转换为词向量 Lookup table 后，**每行的权重即成了对应词的 Embedding 向量**。如果我们把这个**查找表存储到线上的数据库中，就可以轻松地在推荐物品的过程中使用 Embedding 去计算相似性等重要的特征了。**

<img src="/img/in-post/20_07/1e6b464b25210c76a665fd4c34800c96.jpeg" alt="img" style="zoom:50%;" />

Word2vec 的研究中提出的<u>模型结构、目标函数、负采样方法、负采样中的目标函数</u>在后续的研究中被重复使用并被屡次优化。掌握 Word2vec 中的**每一个细节**成了研究 Embedding 的基础。从这个意义上讲，熟练掌握本节课的内容是非常重要的。



## Item2Vec：Word2vec 方法的推广

既然 Word2vec 可以对词“序列”中的词进行 Embedding，那么对于用户购买“序列”中的一个商品，用户观看“序列”中的一个电影，也应该存在相应的 Embedding 方法。

<img src="/img/in-post/20_07/d8e3cd26a9ded7e79776dd31cc8f4807.jpeg" alt="img" style="zoom:50%;" />

于是，微软于 2015 年提出了 Item2Vec 方法，它是对 Word2vec 方法的推广，使 Embedding 方法适用于**几乎所有的序列数据。**Item2Vec 模型的技术细节几乎和 Word2vec 完全一致，只要能够<u>用序列数据的形式把我们要表达的对象表示出来，再把序列数据“喂”给 Word2vec 模型，我们就能够得到任意物品的 Embedding 了</u>。Item2vec 的提出对于推荐系统来说当然是至关重要的，因为它使得“万物皆 Embedding”成为了可能。对于**推荐系统**来说，Item2vec 可以利用物品的 Embedding **直接求得它们的相似性**，**或者作为重要的特征输入推荐模型进行训练，**这些都有助于提升推荐系统的效果



## 小结

这节课，我们一起学习了深度学习推荐系统中非常重要的知识点，Embedding。Embedding 就是用一个数值向量“表示”一个对象的方法。通过 Embedding，我们又引出了 Word2vec，Word2vec 是生成对“词”的向量表达的模型。其中，Word2vec 的训练样本是通过滑动窗口一一截取词组生成的。在训练完成后，模型输入向量矩阵的行向量，就是我们要提取的词向量。最后，我们还学习了 Item2vec，它是 Word2vec 在任意序列数据上的推广。

<img src="/img/in-post/20_07/0f0f9ffefa0c610dd691b51c251b567b.jpeg" alt="img" style="zoom: 33%;" />

## 问题

### 在我们通过 Word2vec 训练得到词向量，或者通过 Item2vec 得到物品向量之后，我们应该用什么方法计算他们的相似性呢？你知道几种计算相似性的方法？

[计算向量间相似度的常用方法：](https://cloud.tencent.com/developer/article/1668762)



# Graph Embedding

## 图结构数据

### 互联网中有哪些图结构数据？

<img src="https://static001.geekbang.org/resource/image/54/91/5423f8d0f5c1b2ba583f5a2b2d0aed91.jpeg" alt="img" style="zoom:33%;" />

最典型的就是我们每天都在使用的**社交网络**（如图 1-a）。从社交网络中，我们可以发现意见领袖，可以发现社区，再根据这些“社交”特性进行社交化的推荐，如果我们可以对社交网络中的**节点进行 Embedding 编码**，社交化推荐的过程将会非常方便。

**知识图谱**也是近来非常火热的研究和应用方向。像图 1b 中描述的那样，知识图谱中包含了不同类型的**知识主体**（如人物、地点等），附着在知识主体上的**属性**（如人物描述，物品特点），以及**主体和主体之间、主体和属性之间的关系**。如果我们能够对知识图谱中的**主体进行 Embedding 化，就可以发现主体之间的潜在关系，这对于基于内容和知识的推荐系统是非常有帮助的。**

还有一类非常重要的图数据就是**行为关系类图数据**。这类数据几乎存在于所有互联网应用中，它事实上是由用户和物品组成的“二部图”（也称二分图，如图 1c）。用户和物品之间的相互行为生成了行为关系图。借助这样的关系图，我们自然能够**利用 Embedding 技术发掘出物品和物品之间、用户和用户之间，以及用户和物品之间的关系**，从而应用于推荐系统的进一步推荐。

毫无疑问，图数据是具备巨大价值的，如果能将图中的**节点 Embedding 化**，对于推荐系统来说将是非常有价值的**特征**。那下面，我们就进入正题，一起来学习基于图数据的 Graph Embedding 方法。



## 基于随机游走的 Graph Embedding 方法：Deep Walk - 在业界影响力比较大，应用也很广泛

它的**主要思想**是在由物品组成的图结构上进行**随机游走，产生大量物品序列**，然后将这些物品序列作为训练样本输入 Word2vec 进行训练，最终得到物品的 Embedding。因此，DeepWalk 可以被看作连接序列 Embedding 和 Graph Embedding 的一种过渡方法。

<img src="/img/in-post/20_07/1f28172c62e1b5991644cf62453fd0ed.jpeg" alt="img" style="zoom:50%;" />

### DeepWalk 的详细算法流程是什么样的

学习 Deep Walk 方法<u>关键在于理解它的算法流程</u>：

1. 首先，我们<u>**基于原始的用户行为序列**来构建**物品关系图**，</u>
2. <u>然后采用随机游走的方式**随机选择起始点，重新产生物品序列**，</u>
3. <u>最后将这些随机游走生成的物品序列**输入 Word2vec 模型**，生成最终的物品 Embedding 向量。</u>

> 其中，随机游走采样的**次数、长度**等都属于超参数，需要我们根据具体应用进行调整。

### DeepWalk跳转概率是什么

唯一需要形式化定义的就是随机游走的跳转概率，也就是**到达节点 vi后，下一步遍历 vi 的邻接点 vj 的概率。**

如果物品关系图是**有向有权图**，那么从节点 vi 跳转到节点 vj 的概率定义如下：即 DeepWalk 的跳转概率就是**跳转边的权重占所有相关出边权重之和的比例**。

如果物品相关图是无向无权重图，那么跳转概率将是上面这个公式的一个特例，即权重 Mij将为常数 1，且 N+(vi) **应是节点 vi所有“边”的集合，而不是所有“出边”的集合。**

<img src="/img/in-post/20_07/image-20210527214623726.png" alt="image-20210527214623726" style="zoom:50%;" />

> 其中，N+(vi) 是节点 vi所有的出边集合，Mij是节点 vi到节点 vj边的权重，
>
> 

## 在同质性和结构性间权衡的方法，Node2vec

Node2vec 通过**调整随机游走跳转概率**的方法，让 Graph Embedding 的结果在网络的**同质性（Homophily）和结构性（Structural Equivalence）**中进行权衡，可以进一步把不同的 Embedding 输入推荐模型，让推荐系统学习到不同的网络结构特点。

我这里所说的网络的**“同质性”**指的是**距离相近**节点的 Embedding 应该尽量近似，（DFS）

> 如图 3 所示，节点 u 与其相连的节点 s1、s2、s3、s4的 Embedding 表达应该是接近的，这就是网络“同质性”的体现。
>
> 在电商网站中，同质性的物品很可能是**同品类、同属性**，或者经常被一同购买的物品。

而“结构性”指的是**结构上相似的节点**的 Embedding 应该尽量接近，（BFS）

> 比如图 3 中节点 u 和节点 s6都是各自局域网络的**中心节点**，它们在结构上相似，所以它们的 Embedding 表达也应该近似，这就是“结构性”的体现。
>
> 在电商网站中，结构性相似的物品一般是各品类的**爆款、最佳凑单商品等拥有类似趋势或者结构性属性的物品**。

<img src="/img/in-post/20_07/e28b322617c318e1371dca4088ce5a82-20210527215525589.jpeg" alt="img" style="zoom: 33%;" />



首先，为了使 Graph Embedding 的结果能够表达网络的**“结构性”**，在随机游走的过程中，我们需要让游走的过程更倾向于 BFS（Breadth First Search，宽度优先搜索），因为 **BFS** 会更多地在<u>当前节点的邻域中进行游走遍历，相当于对当前节点周边的网络结构进行一次“微观扫描”</u>。当前节点是“局部中心节点”，还是“边缘节点”，亦或是“连接性节点”，其生成的序列包含的节点数量和顺序必然是不同的，从而让最终的 Embedding 抓取到更多结构性信息。

而为了表达**“同质性”**，随机游走要更倾向于 **DFS**（Depth First Search，深度优先搜索）才行，因为 DFS 更有可能通过多次跳转，<u>游走到远方的节点上。</u>但无论怎样，DFS 的游走更大概率会在一个大的集团内部进行，这就使得<u>一个集团或者社区内部节点的 Embedding 更为相似，</u>从而更多地表达网络的“同质性”。

### 那在 Node2vec 算法中，究竟是怎样控制 BFS 和 DFS 的倾向性的呢？

它主要是通过节点间的**跳转概率**来控制跳转的倾向性

<img src="/img/in-post/20_07/image-20210527221005299.png" alt="image-20210527221005299" style="zoom:50%;" />

> αpq(t,x) 里的 dtx是指节点 t 到节点 x 的距离，比如节点 x1其实是与节点 t 直接相连的，所以这个距离 dtx就是 1，节点 t 到节点 t 自己的距离 dtt就是 0，而 x2、x3这些不与 t 相连的节点，dtx就是 2。
>
> 此外，αpq(t,x) 中的参数 p 和 q 共同控制着随机游走的倾向性。
>
> 1. 参数 **p 被称为返回参数**（Return Parameter），**p 越小**，随机游**走回节点 t 的可能性越大**，Node2vec 就更注重表达网络的**结构性**。
> 2. 参数 **q 被称为进出参数**（In-out Parameter），**q 越小**，随机游**走到远方节点的可能性越大**，Node2vec 更注重表达网络的**同质性。**反之，当前节点更可能在附近节点游走。你可以自己尝试给 p 和 q 设置不同大小的值，算一算从 v 跳转到 t、x1、x2和 x3的跳转概率。这样一来，应该就不难理解我刚才所说的随机游走倾向性的问题啦。

### 为什么要在特征工程这一模块里介绍 Embedding 呢？

已经学习了好几种主流的 Embedding 方法，包括序列数据的 Embedding 方法，Word2vec 和 Item2vec，以及图数据的 Embedding 方法，Deep Walk 和 Node2vec。

由于 Embedding 的产出就是一个**数值型特征向量**，所以 Embedding 技术本身就可以视作**特征处理方式的一种**。只不过与简单的 One-hot 编码等方式不同，Embedding **是一种更高阶的特征处理方法，它具备了把序列结构、网络结构、甚至其他特征融合到一个特征向量中的能力。**

### Embedding 是如何应用在推荐系统的特征工程中的？

应用方式大致有三种，分别是“直接应用”“预训练应用”和“End2End 应用”。

其中，“**直接应用**”最简单，就是在我们得到 Embedding 向量之后，直接利用 Embedding 向量的相似性实现某些推荐系统的功能。典型的功能有，利用物品 Embedding 间的相似性实现**相似物品推荐**，利用物品 Embedding 和用户 Embedding 的相似性实现**“猜你喜欢”等经典推荐功能**，还可以利用物品 Embedding 实现**推荐系统中的召回层**等。当然，如果你还不熟悉这些应用细节，也完全不用担心，我们在之后的课程中都会讲到。

“**预训练应用**”指的是在我们预先训练好物品和用户的 Embedding 之后，<u>不直接应用，而是**把这些 Embedding 向量作为特征向量的一部分，**跟其余的特征向量拼接起来，作为推荐模型的输入参与训练</u>。这样做能够更好地把其他特征引入进来，让推荐模型作出更为全面且准确的预测。

第三种应用叫做“**End2End 应用**”。看上去这是个新的名词，它的全称叫做“End to End Training”，也就是端到端训练。不过，它其实并不神秘，就是指我们不预先训练 Embedding，**而是把 Embedding 的训练与深度学习推荐模型结合起来**，采用统一的、端到端的方式一起训练，直接得到包含 Embedding 层的推荐模型。**这种方式非常流行，**比如图 6 就<u>展示了三个包含 Embedding 层的经典模型，分别是微软的 Deep Crossing，UCL 提出的 FNN 和 Google 的 Wide&Deep。</u>它们的实现细节我们也会在后续课程里面介绍，你这里只需要了解这个概念就可以了。

<img src="/img/in-post/20_07/e9538b0b5fcea14a0f4bbe2001919978.jpg" alt="img" style="zoom:59%;" />

## 问题

### 对比一下 Embedding 预训练和 Embedding End2End 训练这两种应用方法，说出它们之间的优缺点吗？

Embedding预训练的优点：

1.**更快。**因为对于End2End的方式，Embedding层的优化还受推荐算法的影响，这会增加计算量。

2.End2End难收敛。推荐算法是以Embedding为前提的，在端到端的方式中，在训练初期由于Embedding层的结果没有意义，所以推荐模块的优化也可能不太有意义，可能无法有效收敛。

Embedding端到端的优点：

1. **可能收敛到更好的结果**。端到端因为将Embedding和推荐算法连接起来训练，那么**Embedding层可以学习到最有利于推荐目标的Embedding结果。**



## 总结

这节课我们一起学习了 Graph Embedding 的两种主要方法，分别是 **Deep** **Walk** 和 **Node2vec**，并且我们还总结了 Embedding 技术在深度学习推荐系统中的应用方法。

学习 Deep Walk 方法<u>关键在于理解它的算法流程</u>：

1. 首先，我们<u>**基于原始的用户行为序列**来构建**物品关系图**，</u>
2. <u>然后采用随机游走的方式**随机选择起始点，重新产生物品序列**，</u>
3. <u>最后将这些随机游走生成的物品序列**输入 Word2vec 模型**，生成最终的物品 Embedding 向量。</u>

> 其中，随机游走采样的**次数、长度**等都属于超参数，需要我们根据具体应用进行调整。

而 Node2vec 相比于 Deep Walk，<u>增加了随机游走过程中跳转**概率的倾向性**</u>。如果倾向于**宽度优先搜索**，则 Embedding 结果更加体现“结构性”。如果倾向于**深度优先搜索**，则更加体现“同质性”。

最后，我们介绍了 Embedding 技术在深度学习推荐系统中的**三种应用方法**，“直接应用”“预训练”和“End2End 训练”。

<img src="/img/in-post/20_07/d03ce492866f9fb85b4fbf5fa39346e6.jpeg" alt="img" style="zoom: 33%;" />



# Embedding实战-基于spark

如何使用Spark生成Item2vec和Graph Embedding？

此节，会在 Spark 平台上，完成 Item2vec 和基于 Deep Walk 的 Graph Embedding 的训练。

但是 Spark 作为一个原生的分布式计算平台，在处理大数据方面还是比 TensorFlow 等深度学习平台更具有优势，而且业界的很多公司仍然在使用 Spark 训练一些结构比较简单的机器学习模型

## Item2vec：序列数据的处理

Item2vec 要处理的是类似文本句子、观影序列之类的序列数据。那在真正开始 Item2vec 的训练之前，我们还要先为它准备好训练用的序列数据。

那在真正开始 Item2vec 的训练之前，我们还要先为它**准备好训练用的序列数据**。在 MovieLens 数据集中，有一张叫 rating（评分）的数据表，里面包含了用户对看过电影的评分和评分的时间。既然时间和评分历史都有了，我们要用的观影序列自然就可以通过处理 rating 表得到啦。

在使用观影序列编码之前，我们还要再明确两个问题。

### 问题一是 MovieLens 这个 rating 表本质上只是一个评分的表，不是真正的“观影序列”。

但对用户来说，当然只有看过这部电影才能够评价它，所以，我们几乎**可以把评分序列当作是观影序列。**

### 问题二是我们是应该把所有电影都放到序列中，还是只放那些打分比较高的呢？

这里，**我是建议对评分做一个过滤，只放用户打分比较高的电影**。为什么这么做呢？我们要思考一下 Item2vec 这个模型本质上是要学习什么。我们是**希望 Item2vec 能够学习到物品之间的近似性。既然这样，我们当然是希望评分好的电影靠近一些，评分差的电影和评分好的电影不要在序列中结对出现。**



### 那到这里我们明确了**样本处理的思路**，

就是对一个用户来说，我们先**过滤掉他评分低的电影，再把他评论过的电影按照时间戳排序。**这样，我们就得到了一个**用户的观影序列**，所有用户的观影序列就**组成了 Item2vec 的训练样本集。**



### 怎么在 Spark 上实现呢？

其实很简单，我们只需要明白这 5 个关键步骤就可以实现了：

1. **读取** ratings 原始数据到 Spark 平台；
2. 用 `where` 语句**过滤评分低**的评分记录；
3. 用 `groupBy userId` 操作**聚合每个用户的评分记录**，DataFrame 中每条记录是一个用户的评分序列；
4. 定义一个自定义操作 `sortUdf`，用它实现**每个用户的评分记录按照时间戳进行排序**；
5. 把每个用户的评分记录**处理成一个字符串的形式**，供后续训练过程使用。

具体的实现过程，我还是建议你来参考我下面给出的代码，重要的地方我也都加上了注释，方便你来理解。

```java

def processItemSequence(sparkSession: SparkSession): RDD[Seq[String]] ={
  //设定rating数据的路径并用spark载入数据
  val ratingsResourcesPath = this.getClass.getResource("/webroot/sampledata/ratings.csv")
  val ratingSamples = sparkSession.read.format("csv").option("header", "true").load(ratingsResourcesPath.getPath)


  //实现一个用户定义的操作函数(UDF)，用于之后的排序
  val sortUdf: UserDefinedFunction = udf((rows: Seq[Row]) => {
    rows.map { case Row(movieId: String, timestamp: String) => (movieId, timestamp) }
      .sortBy { case (movieId, timestamp) => timestamp }
      .map { case (movieId, timestamp) => movieId }
  })


  //把原始的rating数据处理成序列数据
  val userSeq = ratingSamples
    .where(col("rating") >= 3.5)  //过滤掉评分在3.5一下的评分记录
    .groupBy("userId")            //按照用户id分组
    .agg(sortUdf(collect_list(struct("movieId", "timestamp"))) as "movieIds")     //每个用户生成一个序列并用刚才定义好的udf函数按照timestamp排序
    .withColumn("movieIdStr", array_join(col("movieIds"), " "))
                //把所有id连接成一个String，方便后续word2vec模型处理


  //把序列数据筛选出来，丢掉其他过程数据
  userSeq.select("movieIdStr").rdd.map(r => r.getAs[String]("movieIdStr").split(" ").toSeq)
```

### Item2vec模型训练

Item2vec：模型训练训练数据准备好了，就该进入我们这堂课的重头戏，模型训练了。手写 Item2vec 的整个训练过程肯定是一件让人比较“崩溃”的事情，好在 Spark MLlib 已经为我们准备好了方便调用的 Word2vec 模型接口。我先把训练的代码贴在下面，然后再带你一步步分析每一行代码是在做什么。

从上面的代码中我们可以看出，Spark 的 Word2vec 模型训练过程非常简单，只需要四五行代码就可以完成。接下来，我就按照从上到下的顺序，依次给你解析其中 3 个关键的步骤。

首先是创建 Word2vec 模型并设定模型参数。我们要清楚 Word2vec 模型的关键参数有 3 个，分别是 setVectorSize、setWindowSize 和 setNumIterations。其中，setVectorSize 用于设定生成的 Embedding 向量的维度，setWindowSize 用于设定在序列数据上采样的滑动窗口大小，setNumIterations 用于设定训练时的迭代次数。这些超参数的具体选择就要根据实际的训练效果来做调整了。

其次，模型的训练过程非常简单，就是调用模型的 fit 接口。训练完成后，模型会返回一个包含了所有模型参数的对象。

最后一步就是提取和保存 Embedding 向量，我们可以从最后的几行代码中看到，调用 getVectors 接口就可以提取出某个电影 ID 对应的 Embedding 向量，之后就可以把它们保存到文件或者其他数据库中，供其他模块使用了。在模型训练完成后，我们再来验证一下训练的结果是不是合理。我在代码中求取了 ID 为 592 电影的相似电影。这部电影叫 Batman 蝙蝠侠，我把通过 Item2vec 得到相似电影放到了下面，你可以从直观上判断一下这个结果是不是合理。



## 总结

小结这节课，我们运用 Spark 实现了经典的 Embedding 方法 Item2vec 和 Deep Walk。它们的理论知识你应该已经在前两节课的学习中掌握了，这里我就总结一下实践中应该注意的几个要点。

关于 Item2vec 的 Spark 实现，你应该注意的是训练 Word2vec 模型的几个参数 `VectorSize`、`WindowSize`、`NumIterations` 等，知道它们各自的作用。它们分别是用来**设置 Embedding 向量的维度**，在序列数据上采样的**滑动窗口大小**，以及训练时的**迭代次数**。

而在 Deep Walk 的实现中，我们应该着重理解的是，**生成物品间的转移概率矩阵的方法**，以及**通过随机游走生成训练样本过程**。最后，我还是把这节课的重点知识总结在了一张表格中，希望能帮助你进一步巩固。

<img src="/img/in-post/20_07/02860ed1170d9376a65737df1294faa7.jpeg" alt="img" style="zoom:33%;" />























