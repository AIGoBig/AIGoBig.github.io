---
marp: true
theme: gaia
paginate: true
# footer: 'jeremyxu 2020-04-23'
style: |
  section {
      font-size: 25px;
      color: black
  }
backgroundImage: url(images/background_XDU.png)
---
<!-- 字体设置 -->
<!-- <style>
section {
  font-family: 'Times New Roman', serif !important;
}
</style> -->

<style>
section {
  font-family: 'Microsoft YaHei', 'SimHei', sans-serif;
}
</style>

<!-- <style>
section {
  font-family: 'Microsoft YaHei', 'Times', sans-serif;
}
</style> -->


<!-- 
_backgroundImage: url(https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/20201211190830.png)
_class: lead
_paginate: false -->

<style scoped>
section {
    font-size: 30px;
}
</style>

# 组会报告

---
<!-- _class: lead 
_paginate: false -->

<style scoped>
section {
    text-align: center;
    font-size: 30px;
}
</style>

## 目录

1.度量学习

2.对比自监督学习

3.想法思路



---
<!-- 
_class: lead gaia
_paginate: false
_color: black -->

<style scoped>
section {
    font-size: 30px;
}
</style>
# 度量学习介绍

---
## 概念

即学习一个度量空间，在该空间中的学习异常高效，这种方法**多用于小样本分类**。

度量学习（Metric Learning），也称距离度量学习(Distance Metric Learning，DML) 属于机器学习的一种。  是人脸识别中常用的机器学习方法, 由Eric Xing在NIPS 2002提出。

度量学习的**对象**通常是**样本特征向量的距离**。

度量学习的**目的**是通过训练和学习，**减小或限制同类样本之间的距离，同时增大不同类别样本之间的距离。**

度量学习分为两种，一种是基于监督学习的，另外一种是基于非监督学习的。

---
## 与经典识别网络的区别 (优势)
- **经典识别网络**存在一个问题：必须提前设定好类别数，例如使用softmax loss的网络。这也就意味着，每增加一个新种类，就要重新定义网络模型，并从头训练一遍。

  > 训练和测试人脸识别分类器的时候经常被提到的Open-set 和Close-set：
  >
  > - close-set，就是所有的测试集都在训练集中出现过。看**这两张图片的预测ID是否一样**即可。
  > - open-set，就是测试的图片并没有在训练集中出现过，需要测试**图像特征向量的距离**。
  > 

- 所以，理想的Open-set下就需要度量学习。人脸识别学习到的特征应当在特定的度量空间中，**满足同一类的最大类内距离小于不同类的最小类间距离。**然后再使用**最近邻检索**就可以实现良好的人脸识别和人脸验证性能。
- 然而softmax loss仅仅能够使得特征可分，还不能够使得特征具有可判别性，所以需要对softmax loss进行改造。**一种较好的做法，是丢弃经典神经网络最后的softmax层，改成直接输出一个feature vector，去特征库里面按照Metric Learning寻找最近邻的类别作为匹配项。**

---
## 基本流程：

一般的度量学习包含以下步骤：

- **Encoder编码**模型：用于把原始数据编码为特征向量 **（重点如何训练模型）** 
- **相似度判别**算法：将一对特征向量进行相似度比对 **（重点如何计算相似度，阈值如何设定）** 
  - 根据不同的任务来**自主学习出针对某个特定任务的度量距离函数**。通过计算两张图片之间的相似度，使得输入图片被归入到相似度大的图片类别中去。

![bg right:40% w:13cm](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/20160926184002910.png)

---

## 关键问题：
- 网络设计：代表有孪生神经网络（Siamese network）
- hard negative mining：**找出难以区分的样本，更利于训练收敛。**
- 损失改进： **改进loss函数，促使网络优化， 使具有相同标签的样本在嵌入空间中尽量接近 ，具有不同标签的样本在嵌入空间中尽量远离。**




---
## 有监督度量学习

有监督度量学习算法**利用输入点$x$与目标标签$y$学习一个距离矩阵，** 这个距离矩阵拉近同类别的点（分类问题）或者目标值邻近的点（回归问题）的距离，并使不同类别或目标值相差大的点的互相远离。

![img](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/20180610151149936.png)

 [度量学习系列（2）：有监督度量学习 -- 程序](https://blog.csdn.net/u013468614/article/details/102846295)

---

## 基本loss概述

### 1. Softmax loss

![](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/20190622174411472.png)

![bg right w:18cm drop-shadow](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3h5czQzMDM4MV8x,size_16,color_FFFFFF,t_70-20201214044635795.png)



但是，Softmax loss做分类可以很好完成任务，但是如果进行**相似度比对就会有比较大的问题

---

**L2距离**：L2距离越小，向量相似度越高。可能**同类的特征向量距离（黄色）比不同类的特征向量距离（绿色）更大**



**cos距离**：夹角越小，cos距离越大，向量相似度越高。**可能同类的特征向量夹角（黄色）比不同类的特征向量夹角（绿色）更大**

![bg right w:10cm](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/v2-6034febeacba9f5c10a9875e7ba4e573_hd.jpg)





![bg right w:10cm](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/v2-21f6b9816a46ee12b624feed740d4ea2_hd.jpg)

------

**总结来说：**

1. **Softmax训练的深度特征，会把整个超空间或者超球，按照分类个数进行划分，保证类别是可分的，这一点对多分类任务如MNIST和ImageNet非常合适，因为测试类别必定在训练类别中。**
2. 但Softmax并**不要求类内紧凑和类间分离**，这一点非常不适合存在未知测试集样本的任务.
3. **所以需要改造Softmax，除了保证可分性外，还要做到特征向量类内尽可能紧凑，类间尽可能分离。**



这种方式只考虑了能否正确分类，**却没有考虑类间距离。所以提出了center loss 损失函数。**

---

### 2.Center loss

![在这里插入图片描述](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/20190622174504721.png)
![在这里插入图片描述](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/20190622174546809.png)
center loss 考虑到不仅仅是分类要对，而且要求类间有一定的距离。上面的公式中 $C_{y_{i}}$表示某一类的中心，$\mathcal{X}_{i}$表示每个人脸的特征值。作者在softmax loss的基础上加入了$L_{C}$，同时使用参数$λ$来控制类内距离，整体的损失函数如下：
![在这里插入图片描述](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/2019062217494518.png)
![bg right w:16cm drop-shadow](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3h5czQzMDM4MV8x,size_16,color_FFFFFF,t_70-20201214045052534.png)

---

### 3. Triplet Loss

![在这里插入图片描述](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/20190622175102498.png)
三元组损失函数，三元组由Anchor， Negative， Positive这三个组成。从上图可以看到，一开始Anchor离Positive比较远，我们想让Anchor和Positive尽量的靠近（同类距离），Anchor和Negative尽量的远离（类间距离）。
![在这里插入图片描述](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/20190622175146562.png)
表达式左边为同类距离 ，右边为不同的类之间的距离。**使用梯度下降法优化的过程就是让类内距离不断下降，类间距离不断提升，这样损失函数才能不断地缩小。**

---

#### triplet网络模型

https://github.com/SpikeKing/triplet-loss-mnist

Triplet Loss的核心是锚示例、正示例、负示例共享模型，通过模型，将锚示例与正示例聚类，远离负示例。
Triplet Loss Model的结构如下：
输入：三个输入，即锚示例、正示例、负示例，不同示例的结构相同；
模型：一个共享模型，支持替换为任意网络结构；
输出：一个输出，即三个模型输出的拼接。



![w:30cm](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3h5czQzMDM4MV8x,size_16,color_FFFFFF,t_70-20201214091614709.png)

---

#### 定义损失

Triplet可以理解为一个三元组，它由三部分组成：

- anchor在这里我们翻译为原点
- positive同类样本点（与原点同类）
- negative异类样本点

针对三元组中的每个元素（样本），训练一个**参数共享或者不共享的网络**，得到三个元素的**特征表达(embedings)**
![bg right w:18cm](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3h5czQzMDM4MV8x,size_16,color_FFFFFF,t_70-20201214092938052.png)

---

Triplet loss的目标：

- 使具有相同标签的样本在嵌入空间中尽量接近
- 使具有不同标签的样本在嵌入空间中尽量远离

值得注意的一点是，如果只遵循以上两点，最后嵌入空间中相同类别的样本可能collapse到一个很小的圈子里，即同一类别的样本簇中样本间的距离很小，**不同类别的样本簇之间也会偏小**。因此，我们加入**间隔(margin)** 的概念——跟SVM中的间隔意思差不多。只要不同类别样本簇简单距离大于这个间隔就阔以了。

我们要求，在嵌入空间d中，**三元组(a,p,n)的损失函数**为：

$$L=\max (d(a, p)-d(a, n)+\operatorname{margin}, 0)$$

最小化该L，则**d(a,p)→0, d(a,n)>margin**。

---

## Reference

[19_5_度量学习——综述](https://blog.csdn.net/xys430381_1/article/details/90705421)

[度量学习系列（2）：有监督度量学习 -- 程序](https://blog.csdn.net/u013468614/article/details/102846295)

[triplet网络模型](https://github.com/SpikeKing/triplet-loss-mnist)

[20_6_一文看懂人脸识别算法技术发展脉络](https://segmentfault.com/a/1190000022870641)

[20_1_Deep Metric Learning及其形式（附Pytorch代码）](https://zhuanlan.zhihu.com/p/100553403)

[深度度量学习 (metric learning deep metric learning ）度量函数总结](https://blog.csdn.net/u014386899/article/details/100173016)

[深度度量学习的这十三年，难道是错付了吗？](https://www.jiqizhixin.com/articles/2020-05-16-5)



---

<!-- 
_class: lead gaia
_paginate: false
_color: black -->

<style scoped>
section {
    font-size: 30px;
}
</style>


# 对比自监督学习

---

**20_CVPR_(Hinton)A Simple Framework for Contrastive Learning of Visual Representations**

提出SimCLR:  可用于视觉表示的一种**对比学习(Contrastive Learning)** 的简单框架

- 用的是对比损失函数, 最小化正对间距离, 最大化负对间距离 
- 自监督学习: 标签产生方式不同 

与度量学习结合, 训练网络

1. **无监督的方式学习表示网络(度量网络)** 
2. 卷积神经网络提取特征
3. 少量样本微调网络

样本增强方式

1. 随机裁剪
2. **颜色增强**

---

主要工作:

1. 我们**简化了最近提出的的对比自监督学习算法**, 使其无需专门的架构或存储库 。

2. 为了了解什么使对比预测任务能够学习有用的表示形式，我们系统地研究了框架的主要组成部分。

3. 提出了如下主要结论 :
   1. **多个数据扩充方法的组合**与**数据增强**非常重要, 尤其使用无监督学习方法时
   2. **在表示 (特征) 和对比损失间引入可学习的非线性变换**可以大幅度提高模型学到的表示的质量

   3. 对于对比学习更大的**批处理数量**和更多的**训练次数**的重要性

---

使用通过Sim-CLR自监督学习到的表示来训练线性分类器得到的效果: 

1. 大幅胜过ImageNet上先前用于自监督和半监督学习的先前方法。可达到76.5％的top-1准确性，与最新技术相比相对提**高了7％**.
2. 与监督的ResNet-50的性能相匹配。
   在仅使用 1% 的 ImageNet 标签进行微调时，SimCLR 实现了 85.8% 的 top-5 准确率，比之前的 SOTA 方法**提升了 10%**。

![bg right w:15cm](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/image-20201214163927589.png)



图为SimCLR 与此前各类自监督方法在 ImageNet 上的 Top-1 准确率对比（以 ImageNet 进行预训练），以及 ResNet-50 的有监督学习效果（灰色×）

------

### 自监督学习

* **训练数据集** -- 不是由人手动标记的，每个样本的标签是通过利用输入的相关性生成的（如来自不同的传感器模式）。

- **标签** -- 通常来自于数据本身: 即模型直接从无标签数据中自行学习，无需标注数据。

- **训练** -- **通过使用各种辅助任务 (auxiliary task ) 训练网络**, 来提高学习表征 (representation) 的质量.

- **核心** **--** 如何自动为数据产生标签。如随机旋转, 均匀分割而自动产生的标注

- **性能评价** -- 通过模型学出来的**feature的质量**来评价. feature质量是通过迁移学习的方式，把feature用到其它视觉任务中通过结果的好坏来评价。

![bg right:40% w:20cm drow-shadow](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/image-20201214164627891.png)



------



### 整体流程

SimCLR 通过**隐空间中的对比损失来最大化同一数据示例的不同增强视图之间的一致性，从而学习到特征表示**。具体说来，这一框架包含**四个主要部分**：

- 随机数据增强模块

- 基本的神经网络编码器 f(·) -- 特征网络

- 神经网络映射头 g(·) -- 变换网络

- 对比预测任务的对比损失函数 ![w:13cm](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/image-20201214165044531.png)
  其中, ![w:7cm](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/image-20201214165102360.png)

相比之前对比学习模型: 结构更简单, 省去数据存储队列

![bg right:33% w:10cm](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/image-20201214165327114.png)

------



![w:15cm](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/image-20201214170947825.png)

![bg right w:15cm](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/image-20201214170925308-20201214203603020.png)

> 如图, 来自同一图片（x1）的不同增广（z1, z2）互相吸引，它们的特征应该接近（红色的线）；
>
> 而来自不同图片的增广（例如z1和z2N）互相排斥，它们的特征应该偏离（蓝色的线）。

---

![w:25cm](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/image-20201214172628117.png)

---

**作者研究了一系列数据增广和数据增广的两两组合**

![](https://image.jiqizhixin.com/uploads/editor/8b86985a-f53a-4826-8dd5-291d10db8022/640.jpeg)



------

<!-- 
_class: lead gaia
_paginate: false
_color: black -->

<style scoped>
section {
    font-size: 30px;
}
</style>

# 想法思路-1 有未知样本时的基于度量学习的多任务小样本HSI分类

---
**20_TGRS_Few-Shot Hyperspectral Image Classification With Unknown Classes Using Multitask Deep Learning**

本文中，提出了一种新颖的**多任务深度学习方法**，用于在开放世界中**对未知类别的高光谱图像进行分类**，这是据我们所知在文献中尚属首次。

![](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/image-20201109215304855-8093352.png)

<!--【代码链接】https://sjliu.me/MDL4OW-->

---

## 思路1: 重建损失部分—考虑使用网络判别

1. 用来判别样本来自真实还是重建的判别部分, zero-shot 问题里可以根据损失来判别是否为未知样本, 其中判别部分：
   1. $l_1$损失

![image-20201216173116230](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/image-20201216173116230.png)

---

**20_TGRS_MTGAN_Classification of Hyperspectral Images via Multitask Generative Adversarial Networks**

在本文中，我们提出了一个多任务生成对抗网络（MTGAN），以**利用来自未标记样本的丰富信息**来缓解此问题。

1. 使用网络进行判别
2. **利用未标记样本实现重建任务**，然后利用标记样本对网络进行**微调**以完成分类任务。AE利用大量的未标记样本，能够获得鲁棒且强大的HSI分类识别特征, **可以使用更少的样本**

通过对抗学习的方法，生成器网络将生成真实的立方体，从而间接**提高了分类任务的判别和泛化能力**。更重要的是，为了充分挖掘浅层的有用信息，我们在重建和分类任务中均采用了跳过层连接。

![w:22cm](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/image-20201128134216737.png)

---

## 思路2: 度量学习替代 FC 分类

使用度量学习方法, 尝试共同提升网络分类任务, unknown样本识别

思路2-1: 直接获取最后的特征向量, 不使用全连接而直接进行距离度量, 取出 top 1 为分类标签, 并设置阈值以判断是否为unknown样本.

思路2-2: 先用fc层进行分类任务训练, 后去掉fc层进行特征度量学习.

思路2-3: 既使用特征度量又使用分类任务 

![image-20201216175349079](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/image-20201216175349079.png)

## <!--思路3: 进行多条短路连接(dense连接)-->

---

## 总体流程

1. 预训练网络
   1. 使用生成判别网络预训练特征提取网络
2. 有监督训练微调网络
   1. 直接训练FC网络, 后拿掉FC, 修改输入为样本对, 对特征向量进行度量学习
   2. 尝试使用3元组损失或对比损失 得到特征度量损失
   3. 一起使用

---

![image-20201215201846402](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/image-20201215201846402-8034800.png)



---

### 与平均数据进行对比

![image-20201216143344825](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/image-20201216143344825.png)

---

## 思路3 对比自监督学习

![image-20201216150317340](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/image-20201216150317340.png)

---

## 样本增强

A. Changing Radiation-Based Virtual Samples

![](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/image-20201106170354783.png)

B. Mixture-Based Virtual Samples

![](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/image-20201106170414304.png)

---



## <!--思路3 加入embedding对特征向量再次进行编码-->

## <!--深度度量学习-->

## <!--加入mean-teacher优化网络-->

## 问题

1. 度量学习可能需要存在未知类别时才能很好的体现优势

---







# <!--进展汇报-->



