---
marp: true
theme: gaia
paginate: true
# footer: 'jeremyxu 2020-04-23'
style: |
  section {
      font-size: 25px;
      color: black
  }
backgroundImage: url(images/background_XDU.png)
---
<!-- 字体设置 -->
<!-- <style>
section {
  font-family: 'Times New Roman', serif !important;
}
</style> -->

<!-- <style>
section {
  font-family: 'Microsoft YaHei', 'SimHei', sans-serif;
}
</style> -->

<style>
section {
  font-family: 'Microsoft YaHei', 'Times', sans-serif;
}
</style>


<!-- 
_backgroundImage: url(https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/20201211190830.png)
_class: lead
_paginate: false -->

<style scoped>
section {
    font-size: 30px;
}
</style>

# 组会报告

---
<!-- _class: lead 
_paginate: false -->

<style scoped>
section {
    text-align: center;
    font-size: 30px;
}
</style>

## 目录

1.度量学习介绍

2.相关文献

3.想法思路

4.实验进展

---
<!-- 
_class: lead gaia
_paginate: false
_color: black -->

<style scoped>
section {
    font-size: 30px;
}
</style>
# 度量学习介绍

---
## 概念

即学习一个度量空间，在该空间中的学习异常高效，这种方法**多用于小样本分类**。

度量学习（Metric Learning），也称距离度量学习(Distance Metric Learning，DML) 属于机器学习的一种。  是人脸识别中常用的机器学习方法, 由Eric Xing在NIPS 2002提出。

度量学习的**对象**通常是**样本特征向量的距离**。

度量学习的**目的**是通过训练和学习，**减小或限制同类样本之间的距离，同时增大不同类别样本之间的距离。**

度量学习分为两种，一种是基于监督学习的，另外一种是基于非监督学习的。

---
## 与经典识别网络的区别 (优势)
- **经典识别网络**存在一个问题：必须提前设定好类别数，例如使用softmax loss的网络。这也就意味着，每增加一个新种类，就要重新定义网络模型，并从头训练一遍。

  > 训练和测试人脸识别分类器的时候经常被提到的Open-set 和Close-set：
  >
  > - close-set，就是所有的测试集都在训练集中出现过。看**这两张图片的预测ID是否一样**即可。
  > - open-set，就是测试的图片并没有在训练集中出现过，需要测试**图像特征向量的距离**。
  > 

- 所以，理想的Open-set下就需要度量学习。人脸识别学习到的特征应当在特定的度量空间中，**满足同一类的最大类内距离小于不同类的最小类间距离。**然后再使用**最近邻检索**就可以实现良好的人脸识别和人脸验证性能。
- 然而softmax loss仅仅能够使得特征可分，还不能够使得特征具有可判别性，所以需要对softmax loss进行改造。**一种较好的做法，是丢弃经典神经网络最后的softmax层，改成直接输出一个feature vector，去特征库里面按照Metric Learning寻找最近邻的类别作为匹配项。**

---
## 基本流程：

一般的度量学习包含以下步骤：

- **Encoder编码**模型：用于把原始数据编码为特征向量 **（重点如何训练模型）** 
- **相似度判别**算法：将一对特征向量进行相似度比对 **（重点如何计算相似度，阈值如何设定）** 
  - 根据不同的任务来**自主学习出针对某个特定任务的度量距离函数**。通过计算两张图片之间的相似度，使得输入图片被归入到相似度大的图片类别中去。

![bg right:40% w:13cm](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/20160926184002910.png)

---

## 关键问题：

- 网络设计：代表有孪生神经网络（Siamese network）
- hard negative mining：**找出难以区分的样本，更利于训练收敛。**
- 损失改进：代表有xx-softmax, **改进loss函数，促使网络优化， 使具有相同标签的样本在嵌入空间中尽量接近 ，具有不同标签的样本在嵌入空间中尽量远离。**

---

**损失改进派**，是最近发展迅速，应用广泛的方法。

在人脸识别与声纹识别这种度量学习算法中，算法的提高主要体现在损失函数的设计上，**损失函数会对整个网络的优化有着导向性的作用**。可以看到许多常用的损失函数，从传统的softmax loss到cosface, arcface 都有这一定的提高。

无论是SphereFace、CosineFace还是ArcFace的损失函数，都是基于Softmax loss来进行修改的。

|                    |                                                              |
| ------------------ | ------------------------------------------------------------ |
| **Base line**      | **Softmax loss**                                             |
| **各种延伸的算法** | **Triplet loss, center loss**                                |
| **最新算法**       | **A-Softmax Loss(SphereFace), Cosine Margin Loss, Angular Margin Loss, Arcface** |




---
## 有监督度量学习

有监督度量学习算法**利用输入点$x$与目标标签$y$学习一个距离矩阵，** 这个距离矩阵拉近同类别的点（分类问题）或者目标值邻近的点（回归问题）的距离，并使不同类别或目标值相差大的点的互相远离。

![img](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/20180610151149936.png)

 [度量学习系列（2）：有监督度量学习 -- 程序](https://blog.csdn.net/u013468614/article/details/102846295)

---

## 基本loss概述

### 1. Softmax loss

![](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/20190622174411472.png)

![bg right w:18cm drop-shadow](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3h5czQzMDM4MV8x,size_16,color_FFFFFF,t_70-20201214044635795.png)



**可以发现，Softmax loss做分类可以很好完成任务，但是如果进行**相似度比对就会有比较大的问题

---

- **L2距离**：L2距离越小，向量相似度越高。可能**同类的特征向量距离（黄色）比不同类的特征向量距离（绿色）更大**

![w:13cm](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/v2-6034febeacba9f5c10a9875e7ba4e573_hd.jpg)

---

- **cos距离**：夹角越小，cos距离越大，向量相似度越高。**可能同类的特征向量夹角（黄色）比不同类的特征向量夹角（绿色）更大**

![w:13cm](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/v2-21f6b9816a46ee12b624feed740d4ea2_hd.jpg)

------

**总结来说：**

1. **Softmax训练的深度特征，会把整个超空间或者超球，按照分类个数进行划分，保证类别是可分的，这一点对多分类任务如MNIST和ImageNet非常合适，因为测试类别必定在训练类别中。**
2. 但Softmax并**不要求类内紧凑和类间分离**，这一点非常不适合人脸识别任务，因为训练集的1W人数，相对测试集整个世界70亿人类来说，非常微不足道，而我们不可能拿到所有人的训练样本，更过分的是，一般我们还要求训练集和测试集不重叠。
3. **所以需要改造Softmax，除了保证可分性外，还要做到特征向量类内尽可能紧凑，类间尽可能分离。**



这种方式只考虑了能否正确分类，**却没有考虑类间距离。所以提出了center loss 损失函数。**

---

### 2.Center loss

![在这里插入图片描述](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/20190622174504721.png)
![在这里插入图片描述](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/20190622174546809.png)
center loss 考虑到不仅仅是分类要对，而且要求类间有一定的距离。上面的公式中 $C_{y_{i}}$表示某一类的中心，$\mathcal{X}_{i}$表示每个人脸的特征值。作者在softmax loss的基础上加入了$L_{C}$，同时使用参数$λ$来控制类内距离，整体的损失函数如下：
![在这里插入图片描述](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/2019062217494518.png)
![bg right w:16cm drop-shadow](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3h5czQzMDM4MV8x,size_16,color_FFFFFF,t_70-20201214045052534.png)

---

### 3. Triplet Loss

![在这里插入图片描述](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/20190622175102498.png)
三元组损失函数，三元组由Anchor， Negative， Positive这三个组成。从上图可以看到，一开始Anchor离Positive比较远，我们想让Anchor和Positive尽量的靠近（同类距离），Anchor和Negative尽量的远离（类间距离）。
![在这里插入图片描述](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/20190622175146562.png)
表达式左边为同类距离 ，右边为不同的类之间的距离。**使用梯度下降法优化的过程就是让类内距离不断下降，类间距离不断提升，这样损失函数才能不断地缩小。**

---

#### triplet网络模型

https://github.com/SpikeKing/triplet-loss-mnist

Triplet Loss的核心是锚示例、正示例、负示例共享模型，通过模型，将锚示例与正示例聚类，远离负示例。
Triplet Loss Model的结构如下：
输入：三个输入，即锚示例、正示例、负示例，不同示例的结构相同；
模型：一个共享模型，支持替换为任意网络结构；
输出：一个输出，即三个模型输出的拼接。



![w:30cm](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3h5czQzMDM4MV8x,size_16,color_FFFFFF,t_70-20201214091614709.png)

---

![](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/R0cHM6Ly9ibG9nLmNzZG4ubmV0L3h5czQzM-7908881.png)

---

#### 为什么不用softmax？

谷歌的论文FaceNet: A Unified Embedding for Face Recognition and Clustering最早将triplet loss应用到人脸识别中。他们提出了一种实现人脸嵌入和在线triplet挖掘的方法，这部分内容我们将在后面章节介绍。

在监督学习中，我们通常都有一个有限大小的样本类别集合，因此可以使用softmax和交叉熵来训练网络。但是，有些情况下，我们的样本类别集合很大，比如在人脸识别中，标签集很大，而我们的任务仅仅是判断两个未见过的人脸是否来自同一个人。

**Triplet loss就是专为上述任务设计的。它可以帮我们学习一种人脸嵌入，使得同一个人的人脸在嵌入空间中尽量接近，不同人的人脸在嵌入空间中尽量远离。**

---

#### 定义损失

Triplet可以理解为一个三元组，它由三部分组成：

- anchor在这里我们翻译为原点
- positive同类样本点（与原点同类）
- negative异类样本点

针对三元组中的每个元素（样本），训练一个**参数共享或者不共享的网络**，得到三个元素的**特征表达(embedings)**
![bg right w:18cm](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3h5czQzMDM4MV8x,size_16,color_FFFFFF,t_70-20201214092938052.png)

---

Triplet loss的目标：

- 使具有相同标签的样本在嵌入空间中尽量接近
- 使具有不同标签的样本在嵌入空间中尽量远离

值得注意的一点是，如果只遵循以上两点，最后嵌入空间中相同类别的样本可能collapse到一个很小的圈子里，即同一类别的样本簇中样本间的距离很小，**不同类别的样本簇之间也会偏小**。因此，我们加入**间隔(margin)** 的概念——跟SVM中的间隔意思差不多。只要不同类别样本簇简单距离大于这个间隔就阔以了。

我们要求，在嵌入空间d中，**三元组(a,p,n)的损失函数**为：

$$L=\max (d(a, p)-d(a, n)+\operatorname{margin}, 0)$$

最小化该L，则**d(a,p)→0, d(a,n)>margin**。

---

## Reference

[19_5_度量学习——综述](https://blog.csdn.net/xys430381_1/article/details/90705421)

[度量学习系列（2）：有监督度量学习 -- 程序](https://blog.csdn.net/u013468614/article/details/102846295)

triplet网络模型：https://github.com/SpikeKing/triplet-loss-mnist

## Reference-未用

[20_6_一文看懂人脸识别算法技术发展脉络](https://segmentfault.com/a/1190000022870641)

[20_1_Deep Metric Learning及其形式（附Pytorch代码）](https://zhuanlan.zhihu.com/p/100553403)

[深度度量学习 (metric learning deep metric learning ）度量函数总结](https://blog.csdn.net/u014386899/article/details/100173016)

[深度度量学习的这十三年，难道是错付了吗？](https://www.jiqizhixin.com/articles/2020-05-16-5)

---

# 小样本学习

## 基于finetune

## 基于度量学习

该方法是**对样本间距离分布进行建模，使得属于同类样本靠近，异类样本远离。**简单地，我们可以采用无参估计的方法，如KNN。KNN虽然不需要训练，但效果依赖距离度量的选取, 一般采用的是一个比较随意的距离计算（L2）。**另一种，也是目前比较好的方法，即通过学习一个端到端的最近邻分类器，它同时受益于带参数和无参数的优点，使得不但能快速的学习到新的样本，而且能对已知样本有很好的泛化性。下面介绍3个相关的方法。**

### 孪生网络 （Siamese Neural Networks）[1]

这个方法对输入的结构进行限制并自动发现可以从新样本上泛化的特征。通过一个有监督的基于孪生网络的度量学习来训练，然后重用那个网络所提取的特征进行one/few-shot学习。
它是一个双路的神经网络，训练时，通过组合不同类的样本成对，同时输入网络进行训练，在最上层通过一个距离的交叉熵进行loss的计算，如图4。在预测的时候，以5way-5shot为例，从5个类中随机抽取5个样本，把这个mini-batch=25的数据输入网络，最后获得25个值，取分数最高对应的类别作为预测结果，如图5。
![在这里插入图片描述](/img/in-post/20_07/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MDEyMzEwOA==,size_16,color_FFFFFF,t_70-20201214145132330.png) 网络结构如图6所示，是一个**8层深度卷积孪生网络**，图中只展示了其中一路计算，在网络的4096维的全连接层后执行**component-wise 的L1距离计算**，产生一个4096维的特征向量，**并通过sigmoidal激活获得一个0到1的概率作为两个输入样本是否相似的结果。**
![在这里插入图片描述](/img/in-post/20_07/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MDEyMzEwOA==,size_16,color_FFFFFF,t_70.png)



### 匹配网络（matching networks）[2]

### `原型网络 （Prototypical Networks）[3]`

## 基于graph neural network

`基于graph neural network`

这是一篇比较新的文章，提交到ICLR 2018[4]。他定义了一个图神经网络框架，端到端地学习消息传递的“关系”型任务。在这里，每个样本看成图的节点，该方法不仅学习节点的embedding，也学习边的embedding。如图9，在网络第一层5个样本通过边模型A～构建了图，接着通过图卷积（graph conv）获得了节点的embedding，然后在后面的几层继续用A～更新图、用graph conv更新节点embedding, 这样便构成了一个深度GNN，最后输出样本的预测标签。
![在这里插入图片描述](/img/in-post/20_07/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MDEyMzEwOA==,size_16,color_FFFFFF,t_70-20201214145900970.png)在构建边模型时，先采用一个4层的CNN网络获得每个节点特征向量，然后将节点对xi,xj的差的绝对值过4层带Batch Norm和Leaky Relu的全连接层，从而获得边的embedding，如图10的左侧。随后，我们将节点的embedding和边的embedding一起过图卷积网络，从而获得更新后的节点的embedding，如图10的右侧。
![在这里插入图片描述](/img/in-post/20_07/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MDEyMzEwOA==,size_16,color_FFFFFF,t_70-20201214145900766.png)这篇文章和Siamese networks、Matching networks和Prototypical networks在本质上是有相同之处的，可以看作这3种方法的推广，而且用图来解决这类问题是一种创新，并取得了不错的效果。

## 效果对比

 比较one/fewshot learning的方法一般采用Omniglot和miniImagenet两个数据集，由于前者相对比较简单，准确率已经比较容易达到99%，所以这里只给出miniImagenet上的对比测试结果。miniImagenet的数据集从 https://drive.google.com/file/d/0B3Irx3uQNoBMQ1FlNXJsZUdYWEE/view 这里下载。

| method                     | 5-way 1-shot      | 5-way 5-shot      |
| -------------------------- | ----------------- | ----------------- |
| baseline finetune          | 28.86 ± 0.54%     | 49.79 ± 0.79%     |
| baseline nearest neighbors | 41.08 ± 0.70%     | 51.04 ± 0.65%     |
| matching network           | 43.56 ± 0.84%     | 55.31 ± 0.73%     |
| **Prototypical network**   | **49.42 ± 0.78%** | **68.20 ± 0.66%** |
| **graph neural network**   | **49.8% ±0.22%**  | **65.5% ±0.20%**  |
| meta-learning LSTM         | 43.44 ± 0.77%     | 60.60 ± 0.71%     |
| Model-Agnostic             | 48.70% ±1.84%     | 63.1% ±0.92%      |

参考文献

[1] G Koch, R Zemel, and R Salakhutdinov. Siamese neural networks for one-shot image recognition. In ICML Deep Learning workshop, 2015.
[2] Oriol Vinyals, Charles Blundell, Tim Lillicrap, Daan Wierstra, et al. Matching networks for one shot learning. In Advances in Neural Information Processing Systems, pages 3630–3638, 2016.
[3] Jake Snell, Kevin Swersky, and Richard S Zemel. Prototypical networks for few-shot learning. arXiv preprint arXiv:1703.05175, 2017.
[4] Victor Garcia, Joan Bruna. Few-shot learning with graph neural networs. Under review as a conference paper at ICLR 2018.
[5] Santoro, Adam, Bartunov, Sergey, Botvinick, Matthew, Wierstra, Daan, and Lillicrap, Timothy. Meta-learning with memory-augmented neural networks. In International Conference on Machine Learning (ICML), 2016.
[6] Ravi, Sachin and Larochelle, Hugo. Optimization as a model for few-shot learning. In International Conference on Learning Representations (ICLR), 2017.
[7] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. arXiv preprint arXiv:1703.03400, 2017.

## Reference

[※※19_4_小样本学习(one/few-shot learning)](https://blog.csdn.net/weixin_40123108/article/details/89003325?utm_medium=distribute.pc_feed_404.none-task-blog-BlogCommendFromBaidu-2.nonecase&depth_1-utm_source=distribute.pc_feed_404.none-task-blog-BlogCommendFromBaidu-2.nonecas)

---

<!-- 
_class: lead gaia
_paginate: false
_color: black -->

<style scoped>
section {
    font-size: 30px;
}
</style>


# 度量学习— 相关论文

---

## 20_CVPR_(Hinton)A Simple Framework for Contrastive Learning of Visual Representations

提出SimCLR:  可用于视觉表示的一种**对比学习(Contrastive Learning)** 的简单框架

- 用的是对比损失函数, 最小化正对间距离, 最大化负对间距离 
- 自监督学习: 标签产生方式不同 

与度量学习结合, 训练网络

1. **无监督的方式学习表示网络(度量网络)** 
2. 卷积神经网络提取特征
3. 少量样本微调网络

样本增强方式

1. 随机裁剪
2. **颜色增强**

---

主要工作:

1. 我们**简化了最近提出的的对比自监督学习算法**, 使其无需专门的架构或存储库 。

2. 为了了解什么使对比预测任务能够学习有用的表示形式，我们系统地研究了框架的主要组成部分。

3. 提出了如下主要结论 :
   1. **多个数据扩充方法的组合**与**数据增强**非常重要, 尤其使用无监督学习方法时
   2. **在表示 (特征) 和对比损失间引入可学习的非线性变换**可以大幅度提高模型学到的表示的质量

   3. 对于对比学习更大的**批处理数量**和更多的**训练次数**的重要性

---

使用通过Sim-CLR自监督学习到的表示来训练线性分类器得到的效果: 

1. 大幅胜过ImageNet上先前用于自监督和半监督学习的先前方法。可达到76.5％的top-1准确性，与最新技术相比相对提**高了7％**.
2. 与监督的ResNet-50的性能相匹配。
   在仅使用 1% 的 ImageNet 标签进行微调时，SimCLR 实现了 85.8% 的 top-5 准确率，比之前的 SOTA 方法**提升了 10%**。

![bg right w:15cm](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/image-20201214163927589.png)



图为SimCLR 与此前各类自监督方法在 ImageNet 上的 Top-1 准确率对比（以 ImageNet 进行预训练），以及 ResNet-50 的有监督学习效果（灰色×）

------

### 自监督学习

* **训练数据集** -- 不是由人手动标记的，每个样本的标签是通过利用输入的相关性生成的（如来自不同的传感器模式）。

- **标签** -- 通常来自于数据本身: 即模型直接从无标签数据中自行学习，无需标注数据。

- **训练** -- **通过使用各种辅助任务 (auxiliary task ) 训练网络**, 来提高学习表征 (representation) 的质量.

- **核心** **--** 如何自动为数据产生标签。如随机旋转, 均匀分割而自动产生的标注

- **性能评价** -- 通过模型学出来的**feature的质量**来评价. feature质量是通过迁移学习的方式，把feature用到其它视觉任务中通过结果的好坏来评价。

![bg right:40% w:20cm drow-shadow](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/image-20201214164627891.png)



------



### 整体流程

SimCLR 通过**隐空间中的对比损失来最大化同一数据示例的不同增强视图之间的一致性，从而学习到特征表示**。具体说来，这一框架包含**四个主要部分**：

- 随机数据增强模块

- 基本的神经网络编码器 f(·) -- 特征网络

- 神经网络映射头 g(·) -- 变换网络

- 对比预测任务的对比损失函数 ![w:13cm](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/image-20201214165044531.png)
  其中, ![w:7cm](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/image-20201214165102360.png)

相比之前对比学习模型: 结构更简单, 省去数据存储队列

![bg right:33% w:10cm](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/image-20201214165327114.png)

------



![w:15cm](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/image-20201214170947825.png)

![bg right w:15cm](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/image-20201214170925308-20201214203603020.png)

> 如图, 来自同一图片（x1）的不同增广（z1, z2）互相吸引，它们的特征应该接近（红色的线）；
>
> 而来自不同图片的增广（例如z1和z2N）互相排斥，它们的特征应该偏离（蓝色的线）。

---

![w:25cm](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/image-20201214172628117.png)

---

**作者研究了一系列数据增广和数据增广的两两组合**

![](https://image.jiqizhixin.com/uploads/editor/8b86985a-f53a-4826-8dd5-291d10db8022/640.jpeg)

------

##  (19TGRS)A CNN With Multiscale Convolution and Diversified Metric for Hyperspectral Image Classification

MS-CNN → multi-scale feature

determinantal point process (DPP) 

deep metric learning(DML)

------

### 创新点

- 提出了一种具有**多尺度卷积**和**使用DPP方法来促进多样性的深度度量**的新型CNN（**DPP-DML-MS-CNN**），该模型同时利用了多尺度特征和多样化的深度度量，用于高光谱图像分类。

   ![image-20200302104353207](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/image-20200302104353207.png)

为了加速模型的训练过程，首先通过**预训练**来学习MS-CNN的参数，然后采用**微调**的方法用 **基于DPP的结构损失**来**微调**训练深度模型 ,联合学习 MS-CNN的参数W 和度量因子B。最后，将学习到的 **度量因子B** 也加入到 **深度模型中** 用于从高光谱图像中提取判别特征。

------

<!-- 
_class: lead gaia
_paginate: false
_color: black -->

<style scoped>
section {
    font-size: 30px;
}
</style>

# 想法思路

------

模型





------







------



---

# 关联笔记

[2020-12-08-深度学习-度量学习.md](/Users/king/sunqinghu.github.io/_posts/2020-12-08-深度学习-度量学习.md)

[2020-12-14-深度学习-小样本学习](/Users/king/sunqinghu.github.io/_posts/2020-12-14-深度学习-小样本学习.md)

[2020-03-19-论文学习-20_(HInton|CL)A Simple Framework for Contrastive Learning of Visual Representations](/Users/king/sunqinghu.github.io/_posts/2020-03-19-论文学习-20_(HInton|CL)A Simple Framework for Contrastive Learning of Visual Representations?.md)

[2020-03-02-论文学习-A CNN With Multiscale Convolution and Diversified Metric   for Hyperspectral Image Classification](/Users/king/sunqinghu.github.io/_posts/2020-03-02-论文学习-A CNN With Multiscale Convolution and Diversified Metric   for Hyperspectral Image Classification.md)