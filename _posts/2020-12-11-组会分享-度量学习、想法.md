---
marp: true
theme: gaia
paginate: true
# footer: 'jeremyxu 2020-04-23'
style: |
  section {
      font-size: 25px;
      color: black
  }
backgroundImage: url(images/background_XDU.png)
---
<!-- 字体设置 -->
<!-- <style>
section {
  font-family: 'Times New Roman', serif !important;
}
</style> -->

<!-- <style>
section {
  font-family: 'Microsoft YaHei', 'SimHei', sans-serif;
}
</style> -->

<style>
section {
  font-family: 'Microsoft YaHei', 'Times', sans-serif;
}
</style>


<!-- 
_backgroundImage: url(https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/20201211190830.png)
_class: lead
_paginate: false -->

<style scoped>
section {
    font-size: 30px;
}
</style>

# 组会报告

---
<!-- _class: lead 
_paginate: false -->

<style scoped>
section {
    text-align: center;
    font-size: 30px;
}
</style>

## 目录

1.度量学习介绍

2.相关文献

3.想法思路

4.实验进展

---
<!-- 
_class: lead gaia
_paginate: false
_color: black -->

<style scoped>
section {
    font-size: 30px;
}
</style>

# 度量学习介绍

---
## 概念

度量学习

即学习一个度量空间，在该空间中的学习异常高效，这种方法**多用于小样本分类**。直观来看，如果我们的目标是从少量样本图像中学习，那么一个简单的方法就是**对比你想进行分类的图像和已有的样本图像。** 但是，正如你可能想到的那样，在像素空间里进行图像对比的效果并不好。不过，你可以**训练一个 Siamese 网络或在学习的度量空间里进行图像对比。** 与前一个方法类似，元学习通过梯度下降（或者其他神经网络优化器）来进行，而学习者对应对比机制，即在元学习度量空间里对比最近邻。**这些方法用于小样本分类时效果很好**，不过度量学习方法的效果尚未在回归或强化学习等其他元学习领域中验证。

来源： [机器之心](https://www.jiqizhixin.com/articles/2017-07-20-4)

---

度量学习（Metric Learning），也称距离度量学习(Distance Metric Learning，DML) 属于机器学习的一种。  是人脸识别中常用的机器学习方法, 由Eric Xing在NIPS 2002提出。

> 其本质就是相似度的学习，也可以认为距离学习。因为在一定条件下，相似度和距离可以相互转换。比如在空间坐标的两条向量，既可以用余弦相似度的大小，也可以使用欧式距离的远近来衡量相似程度。

度量学习的**对象**通常是**样本特征向量的距离**。

度量学习的**目的**是通过训练和学习，**减小或限制同类样本之间的距离，同时增大不同类别样本之间的距离。**

度量学习分为两种，一种是基于监督学习的，另外一种是基于非监督学习的。

---
## 与经典识别网络的区别 (优势)
- **经典识别网络**存在一个问题：必须提前设定好类别数，例如使用softmax loss的网络。这也就意味着，每增加一个新种类，就要重新定义网络模型，并从头训练一遍。

  > 训练和测试人脸识别分类器的时候经常被提到的Open-set 和Close-set：
  >
  > - close-set，就是所有的测试集都在训练集中出现过。所以预测结果是图片的ID，如果想要测试两张图片是否是同一个，那么就看**这两张图片的预测ID是否一样**即可。
  > - open-set，就是测试的图片并没有在训练集中出现过，那么每张测试图片的预测结果是特征向量，如果想要比较两张图片是否属于同一类别，需要测试**图像特征向量的距离**。
  > 

- 所以，理想的Open-set下就需要度量学习。人脸识别学习到的特征应当在特定的度量空间中，**满足同一类的最大类内距离小于不同类的最小类间距离。**然后再使用**最近邻检索**就可以实现良好的人脸识别和人脸验证性能。因此，Metric Learning作为经典识别网络的替代方案，可以很好地适应某些特定的图像识别场景。
- 然而softmax loss仅仅能够使得特征可分，还不能够使得特征具有可判别性，所以需要对softmax loss进行改造。**一种较好的做法，是丢弃经典神经网络最后的softmax层，改成直接输出一个feature vector，去特征库里面按照Metric Learning寻找最近邻的类别作为匹配项。**

---
## 基本流程：

一般的度量学习包含以下步骤：

- **Encoder编码**模型：用于把原始数据编码为特征向量 **（重点如何训练模型）** 
- **相似度判别**算法：将一对特征向量进行相似度比对 **（重点如何计算相似度，阈值如何设定）** 
  - 根据不同的任务来**自主学习出针对某个特定任务的度量距离函数**。通过计算两张图片之间的相似度，使得输入图片被归入到相似度大的图片类别中去。

![bg right:40% w:13cm](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/20160926184002910.png)

---

## 关键问题：

- 网络设计：代表有孪生神经网络（Siamese network）
- hard negative mining：**找出难以区分的样本，更利于训练收敛。**
- 损失改进：代表有xx-softmax, **改进loss函数，促使网络优化， 使具有相同标签的样本在嵌入空间中尽量接近 ，具有不同标签的样本在嵌入空间中尽量远离。**

---

本文介绍重点是**损失改进派**，是最近发展迅速，应用广泛的方法。

在人脸识别与声纹识别这种度量学习算法中，算法的提高主要体现在损失函数的设计上，**损失函数会对整个网络的优化有着导向性的作用**。可以看到许多常用的损失函数，从传统的softmax loss到cosface, arcface 都有这一定的提高。

无论是SphereFace、CosineFace还是ArcFace的损失函数，都是基于Softmax loss来进行修改的。



|                    |                                                              |
| ------------------ | ------------------------------------------------------------ |
| **Base line**      | **Softmax loss**                                             |
| **各种延伸的算法** | **Triplet loss, center loss**                                |
| **最新算法**       | **A-Softmax Loss(SphereFace), Cosine Margin Loss, Angular Margin Loss, Arcface** |

---

## 有监督度量和无监督度量

**一. 监督学习**

1）LDA Fisher 线性判别

2）Local LDA (Local Linear Discriminative Analysis)

3）RCA 相关成分分析 ( Relevant Component Analysis)

4）LPP 局部保留投影 ( Locality Preserving Projection)

5）LMNN 大间隔最近邻 ( Large-Margin Nearest Neighbors)

6）LLE 局部线性嵌入 (Locally linear embedding)

监督学习的方法应用比较多，包括上一节我们讲到的 **基于CNN的特征提取**都属于监督学习的范畴。

**二. 无监督学习**

   严格说来，**非监督的度量学习（主要是指降维方法）不算真正的度量学习**，我们也把他们列出来，方便读者记忆：

1）主成分分析(Pricipal Components Analysis, PCA)

2）多维尺度变换(Multi-dimensional Scaling, MDS)

3）独立成分分析(Independent components analysis, ICA)

4）拉普拉斯特征映射（Laplacian Eigenmaps）


---
## 有监督度量学习

有监督度量学习算法**利用输入点$x$与目标标签$y$学习一个距离矩阵，** 这个距离矩阵拉近同类别的点（分类问题）或者目标值邻近的点（回归问题）的距离，并使不同类别或目标值相差大的点的互相远离。

![img](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/20180610151149936.png)

 [度量学习系列（2）：有监督度量学习 -- 程序](https://blog.csdn.net/u013468614/article/details/102846295)

---

## 基本loss概述

### 1. Softmax loss

![](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/20190622174411472.png)

> 这就是softmax loss函数，${W^T_{j}x_i+b_{j}}$表示全连接层的输出。在计算Loss下降的过程中，我们让${W^T_{j}x_i+b_{j}}$ 的比重变大，从而使得log() 括号内的数更变大来更接近1，就会 log(1) = 0，整个loss就会下降。

---

![w:18cm drop-shadow](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3h5czQzMDM4MV8x,size_16,color_FFFFFF,t_70-20201214044635795.png)

> **图如何理解呢？倒数第二层输出不应该是很多维吗？** 形象的理解:当做是一个球体，但是为了可视化方便，把球给压扁了。就成为了二维的图像。（个人理解）
>
> **如何操作？** 应该通过降维方法。
>
> **这样如何完成分类的？** 我们知道，softmax分类时取的是最大那类（argmax），只要目标那一类大于其他类就可以了。反映在图上，每个点与各类中心的距离（W与b决定），距离哪个中心最近就会分成哪一类。

---

可以发现，Softmax loss做分类可以很好完成任务，但是如果进行**相似度比对**就会有比较大的问题（参加[[深度概念\]·Softmax优缺点解析](https://blog.csdn.net/xiaosongshine/article/details/88826715)）

- **L2距离**：L2距离越小，向量相似度越高。可能**同类的特征向量距离（黄色）比不同类的特征向量距离（绿色）更大**

![w:13cm](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/v2-6034febeacba9f5c10a9875e7ba4e573_hd.jpg)

---

- **cos距离**：夹角越小，cos距离越大，向量相似度越高。**可能同类的特征向量夹角（黄色）比不同类的特征向量夹角（绿色）更大**

![w:13cm](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/v2-21f6b9816a46ee12b624feed740d4ea2_hd.jpg)

------

总结来说：

1. **Softmax训练的深度特征，会把整个超空间或者超球，按照分类个数进行划分，保证类别是可分的，这一点对多分类任务如MNIST和ImageNet非常合适，因为测试类别必定在训练类别中。**
2. 但Softmax并**不要求类内紧凑和类间分离**，这一点非常不适合人脸识别任务，因为训练集的1W人数，相对测试集整个世界70亿人类来说，非常微不足道，而我们不可能拿到所有人的训练样本，更过分的是，一般我们还要求训练集和测试集不重叠。
3. **所以需要改造Softmax，除了保证可分性外，还要做到特征向量类内尽可能紧凑，类间尽可能分离。**



这种方式只考虑了能否正确分类，却没有考虑类间距离。所以提出了center loss 损失函数。

---

### 2.Center loss

![在这里插入图片描述](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/20190622174504721.png)
![在这里插入图片描述](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/20190622174546809.png)
center loss 考虑到不仅仅是分类要对，而且要求类间有一定的距离。上面的公式中 $C_{y_{i}}$表示某一类的中心，$\mathcal{X}_{i}$表示每个人脸的特征值。作者在softmax loss的基础上加入了$L_{C}$，同时使用参数$λ$来控制类内距离，整体的损失函数如下：
![在这里插入图片描述](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/2019062217494518.png)
![bg right w:16cm drop-shadow](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3h5czQzMDM4MV8x,size_16,color_FFFFFF,t_70-20201214045052534.png)

---

### 3. Triplet Loss

![在这里插入图片描述](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/20190622175102498.png)
三元组损失函数，三元组由Anchor， Negative， Positive这三个组成。从上图可以看到，一开始Anchor离Positive比较远，我们想让Anchor和Positive尽量的靠近（同类距离），Anchor和Negative尽量的远离（类间距离）。
![在这里插入图片描述](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/20190622175146562.png)
表达式左边为同类距离 ，右边为不同的类之间的距离。**使用梯度下降法优化的过程就是让类内距离不断下降，类间距离不断提升，这样损失函数才能不断地缩小。**

---

#### **triplet网络模型**

https://github.com/SpikeKing/triplet-loss-mnist

Triplet Loss的核心是锚示例、正示例、负示例共享模型，通过模型，将锚示例与正示例聚类，远离负示例。
Triplet Loss Model的结构如下：
输入：三个输入，即锚示例、正示例、负示例，不同示例的结构相同；
模型：一个共享模型，支持替换为任意网络结构；
输出：一个输出，即三个模型输出的拼接。



![w:30cm](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3h5czQzMDM4MV8x,size_16,color_FFFFFF,t_70-20201214091614709.png)

---

![](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/R0cHM6Ly9ibG9nLmNzZG4ubmV0L3h5czQzM-7908881.png)

---

#### 为什么不用softmax？

谷歌的论文FaceNet: A Unified Embedding for Face Recognition and Clustering最早将triplet loss应用到人脸识别中。他们提出了一种实现人脸嵌入和在线triplet挖掘的方法，这部分内容我们将在后面章节介绍。

在监督学习中，我们通常都有一个有限大小的样本类别集合，因此可以使用softmax和交叉熵来训练网络。但是，有些情况下，我们的样本类别集合很大，比如在人脸识别中，标签集很大，而我们的任务仅仅是判断两个未见过的人脸是否来自同一个人。

**Triplet loss就是专为上述任务设计的。它可以帮我们学习一种人脸嵌入，使得同一个人的人脸在嵌入空间中尽量接近，不同人的人脸在嵌入空间中尽量远离。**

---

#### 定义损失

Triplet可以理解为一个三元组，它由三部分组成：

- anchor在这里我们翻译为原点
- positive同类样本点（与原点同类）
- negative异类样本点

针对三元组中的每个元素（样本），训练一个**参数共享或者不共享的网络**，得到三个元素的**特征表达(embedings)**
![bg right w:18cm](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3h5czQzMDM4MV8x,size_16,color_FFFFFF,t_70-20201214092938052.png)

---

Triplet loss的目标：

- 使具有相同标签的样本在嵌入空间中尽量接近
- 使具有不同标签的样本在嵌入空间中尽量远离

值得注意的一点是，如果只遵循以上两点，最后嵌入空间中相同类别的样本可能collapse到一个很小的圈子里，即同一类别的样本簇中样本间的距离很小，**不同类别的样本簇之间也会偏小**。因此，我们加入**间隔(margin)** 的概念——跟SVM中的间隔意思差不多。只要不同类别样本簇简单距离大于这个间隔就阔以了。

我们要求，在嵌入空间d中，**三元组(a,p,n)的损失函数**为：

$$L=\max (d(a, p)-d(a, n)+\operatorname{margin}, 0)$$

最小化该L，则**d(a,p)→0, d(a,n)>margin**。

---

#### Triplets挖掘

基于前文定义的Triplet loss，可以将三元组分为一下三个类别：

- easy triplets：可以使loss = 0的三元组，即容易分辨的三元组
- hard triplets：d(a,n)<d(a,p)的三元组，即一定会误识别的三元组
- semi-hard triplets：d(a,p)<d(a,n)<d(a,p)+margin的三元组，即处在模糊区域（关键区域）的三元组
  ![bg right:40% w:13cm  drop-shadow](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3h5czQzMDM4MV8x,size_16,color_FFFFFF,t_70-20201214094248091-20201214123223361.png)

图中，a为原点位置，p为同类样本例子，不同颜色表示的区域表示异类样本分布于三元组类别的关系．

显然，中间的Semi-hard negatives样本对我们网络模型的训练至关重要。

---

**上面的几个算法都是比较传统老旧的，下面说一下比较新的算法。**

---

### **4. L-softmax**

前面Softmax loss函数没有考虑类间距离，Center loss函数可以使类内变得紧凑，但没有类间可分，而Triplet loss函数比较耗时，就产生了一下新的算法。

L-softmax函数开始就做了比较精细的改动，从softmax 函数log里面的 $e^{W^T_{y_i}x_i+b_{y_i}}$  转化到$e^{||W_{yi}|| ||x_i||\psi{(\theta_{y_i})}}$。L-softmax函数不仅希望类间距离拉的更大，还能够把**类内距离压缩的更紧凑**。
$$
L_4 = \frac{1}{N}\sum_{i=1}^N L_i = \frac{1}{N}\sum_{i=1}^N -log(\frac{e^{f_{yi}}}{\sum_{j}e^{f_i}})
$$

$$
L_i = -log(\frac{e^{||W_{yi}|| ||x_i||\psi{(\theta_{y_i})}}} {e^{||W_{yi}|| ||x_i||\psi{(\theta_{y_i})}} + \sum_{ j\neq y_i}{e^{||W_j|| ||x_i||cos(\theta_j)}}})
$$

把其中的cosθ改成了cos(mθ)，
$$
\psi(\theta) = \left\{\begin{matrix} \cos (m\theta ), 0\leqslant \theta \leqslant \frac{\pi }{m}& & \\ D(\theta), \frac{\pi}{m}\leqslant \theta \leqslant \pi & & \end{matrix}\right.
$$
m倍θ起到了增加 margin 的效果，让类内距离更加紧凑，同时类间距离变大。m越大类间距离就越大，因为在(0, π)区间cos函数单调递减，m越大 cos(mθ)趋向于0。

![img](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTI1MDU2MTc=,size_16,color_FFFFFF,t_70.png)



### 5. SphereFace(A-Softmax)

A-softmax 是在 L-softmax 函数上做了一个很小的修改，A-softmax 在考虑 margin时添加两个限制条件：将权重W归一化 ||W|| = 1，b = 0。**这使得模型的预测仅取决于 W 和 X 之间的角度。**


$$
\LARGE L_5 = -\frac{1}{N}\sum_{i=1}^{N}log( \frac{e^{||x_i||\cos(m\theta_{y_i})}} {e^{||x_i||\cos(m\theta_{y_i})} + \sum_{j \neq y_i}{e^{||x_i||cos(\theta_j)}}})
$$


### 6. CosFace

cosface的loss函数如下：
$$
\LARGE L_6 = -\frac{1}{N} \sum_{i=1}^{N} log( \frac{e^{s(cos(\theta_{yi})-m)}}{e^{s(cos(\theta_{yi})-m)}+ \sum_{j=1, j\neq y_i}^k e^{scos \theta_j}})
$$
上式中，s为超球面的半径，m为margin。



### 7. ArcFace

对比arcface和cosface这两个函数，**发现arcface是直接在角度空间中最大化分类界限，而cosface是在余弦空间中最大化分类界限，这样修改是因为角度距离比余弦距离在对角度的影响更加直接。** 
$$
\LARGE L_7= -\frac{1}{N} \sum_{i=1}^{N} log(\frac{e^{s(cos(\theta_{yi}+m))}}{e^{s(cos(\theta_{yi}+m))}+\sum_{j=1,j\neq y_i}^k e^{scos\theta_j}})
$$
分类的决策边界如下：

![img](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTI1MDU2MTc=,size_16,color_FFFFFF,t_70-20201214113209962.png)

 arcface算法流程如下：

![img](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTI1MDU2MTc=,size_16,color_FFFFFF,t_70-20201214113209827.png)

 

### N-pair loss

Sohn, K.: Improved deep metric learning with multi-class n-pair loss objective. In:
NIPS. (2016)
[Improved Deep Metric Learning with Multi-class N-pair Loss Objective论文N-pair loss解读与实现](https://blog.csdn.net/silence2015/article/details/84878692)
pytorch代码：1、https://github.com/ChaofWang/Npair_loss_pytorch/blob/master/Npair_loss.py
2、https://github.com/leeesangwon/PyTorch-Image-Retrieval/blob/public/losses.py
tensorflow代码（多种度量学习loss函数）：https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/contrib/losses/python/metric_learning/metric_loss_ops.py

---

### Angular Loss

[【2017_ICCV】Deep Metric Learning with Angular Loss](https://blog.csdn.net/booyoungxu/article/details/78507346)
tensorflow代码：https://github.com/geonm/tf_angular_loss
pytorch代码：https://github.com/leeesangwon/PyTorch-Image-Retrieval/blob/public/losses.py
[SphereFace论文学习](https://blog.csdn.net/cdknight_happy/article/details/79268613)：该文提到的A-softmax loss就是Angular Loss的源泉

- 提出了Angular Loss，用角度关系作为相似性度量。- 之前的方法主要用距离进行相似性度量，距离度量在尺度变化时比较敏感，并且对于不同的intra-class采用相同的margin也不合适，Angular Loss自带旋转不变和尺度不变。
- 优点：
  - 引入尺度不变，提高了目标对于特征差异的鲁棒性；
  - 增加了三阶几何限制，捕获了triplet triangles的附加局部结构；
  - 收敛更好。

------

### 相关文章列表

[Improved Deep Metric Learning with Multi-class N-pair Loss Objective论文N-pair loss解读与实现](https://blog.csdn.net/silence2015/article/details/84878692)

CVPR2019 [Ranked List Loss for Deep Metric Learning | 论文分享](https://www.jiqizhixin.com/articles/2019-03-12-19)

[距离和相似度度量方法](https://blog.csdn.net/yangdashi888/article/details/82628550)







---





---

## 可视化

![image-20201214123302743](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/image-20201214123302743.png)



## Reference

[19_5_度量学习——综述](https://blog.csdn.net/xys430381_1/article/details/90705421)

[度量学习系列（2）：有监督度量学习 -- 程序](https://blog.csdn.net/u013468614/article/details/102846295)

triplet网络模型：https://github.com/SpikeKing/triplet-loss-mnist

## Reference-未用

[20_6_一文看懂人脸识别算法技术发展脉络](https://segmentfault.com/a/1190000022870641)

[20_1_Deep Metric Learning及其形式（附Pytorch代码）](https://zhuanlan.zhihu.com/p/100553403)

[深度度量学习 (metric learning deep metric learning ）度量函数总结](https://blog.csdn.net/u014386899/article/details/100173016)

[深度度量学习的这十三年，难道是错付了吗？](https://www.jiqizhixin.com/articles/2020-05-16-5)



# 小样本学习

## 基于Finetune

> 这种方法已经被广泛使用。获得一定量的标注数据，然后基于一个基础网络进行微调  。
>
> 这个基础网络是通过含有丰富标签的大规模数据集获得的，比如imagenet，我们的淘宝电商数据，称为通用数据域。然后在特定的数据域上进行训练。训练时，会固定基础网络部分的参数，对领域特定的网络参数进行训练(这里有很多训练的trick，包括设置固定层和学习率等)。如下图，这个方法可以相对较快，依赖数据量也不必太多，效果还可以。

 

![img](/img/in-post/20_07/Center-7929717.png)

 

## 小样本学习— 基于度量学习

该方法是**对样本间距离分布进行建模，使得属于同类样本靠近，异类样本远离。**简单地，我们可以采用无参估计的方法，如KNN。KNN虽然不需要训练，但效果依赖距离度量的选取, 一般采用的是一个比较随意的距离计算（L2）。**另一种，也是目前比较好的方法，即通过学习一个端到端的最近邻分类器，它同时受益于带参数和无参数的优点，使得不但能快速的学习到新的样本，而且能对已知样本有很好的泛化性。下面介绍3个相关的方法。**

### 孪生网络 （Siamese Neural Networks）[1]

这个方法对输入的结构进行限制并自动发现可以从新样本上泛化的特征。通过一个有监督的基于孪生网络的度量学习来训练，然后重用那个网络所提取的特征进行one/few-shot学习。
它是一个双路的神经网络，训练时，通过组合不同类的样本成对，同时输入网络进行训练，在最上层通过一个距离的交叉熵进行loss的计算，如图4。在预测的时候，以5way-5shot为例，从5个类中随机抽取5个样本，把这个mini-batch=25的数据输入网络，最后获得25个值，取分数最高对应的类别作为预测结果，如图5。
![在这里插入图片描述](/img/in-post/20_07/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MDEyMzEwOA==,size_16,color_FFFFFF,t_70-20201214145132330.png) 网络结构如图6所示，是一个**8层深度卷积孪生网络**，图中只展示了其中一路计算，在网络的4096维的全连接层后执行**component-wise 的L1距离计算**，产生一个4096维的特征向量，**并通过sigmoidal激活获得一个0到1的概率作为两个输入样本是否相似的结果。**
![在这里插入图片描述](/img/in-post/20_07/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MDEyMzEwOA==,size_16,color_FFFFFF,t_70.png)



### 匹配网络（matching networks）[2]

这篇文章的工作被李飞飞高徒karpath点赞过，成为之后相关研究中经常被对比的参照。该文章也是在不改变网络模型的前提下能对未知类别生成标签，其主要创新体现在建模过程和训练过程上。对于建模过程的创新，文章提出了基于memory和attantion的matching nets，使得可以快速学习。对于训练过程的创新，文章基于传统机器学习的一个原则，即训练和测试是要在同样条件下进行的，提出在训练的时候不断地让网络只看每一类的少量样本，这将和测试的过程是一致的。
具体地，它尝试获得一个从支持集S(support set， 由k个样本及其标签组成)到分类器y^ 的一个映射，该映射是一个网络:P(y^ |x^ ,S)，它基于当前的S，对每个未见过的测试样本x^ 给出其标签y^，该标签让P达到最大值。这个模型可以表示为如公式1)，其中a是一个attetion。
![在这里插入图片描述](/img/in-post/20_07/20190403192852437.png)即一个新样本的输出（即在S上类别的分布）是S上的类attation线性组合，也就是对于离x^最远的xi，其在某度量下的attation是0， 那么其值就是和x^相似的xi所对应标签的权重融合。
上述的attention具体是，对训练样本xi和测试样本x^分别进行embedding，然后求内积(cosine)，这就是文章提出的"matching"，然后输入到一个softmax中，公式如公式2），其中c是cosine距离。其中两个embedding的模型是share的，比如用CNN。这个a是和度量学习（metric learning）相关的，对于待分类的样本x, 让其和那些标签为y的样本对齐，和其它的不对齐，这种loss其实就是和NCA，triplet loss和margin nearest neighbor相关的。
![在这里插入图片描述](/img/in-post/20_07/20190403192925421.png)进一步，支持集样本embedding模型g能继续优化，并且支持集样本应该可以用来修改测试样本的embedding模型f。这个可以通过如下两个方面来解决：即1）基于双向LSTM学习训练集的embedding，使得每个训练样本的embedding是其它训练样本的函数；2）基于attention-LSTM来对测试样本embedding，使得每个测试样本的embeding是训练集embedding的函数。文章称其为FCE(fully-conditional embedding)。
关于g 的优化。上述虽然是在整个支持集样本上做分类，但用来做cosine距离计算的embedding 的获得是互相独立的，因此文章把对支持集样本的embedding改为g(xi, S)， 这当比如xj十分接近xi时改变g函数是有用的。文章用了双向LSTM，即把S看成一个序列（文章并没有提到这个序列的顺序是怎么生成的，也许是随机的），然后对每个xi进行编码。具体公式如下，其中g’(xi)是原始只依赖自己的embedding，xi通过BiLSTM进行信息互通。
![在这里插入图片描述](/img/in-post/20_07/20190403193021475.png)
![在这里插入图片描述](/img/in-post/20_07/20190403192953518.png)![在这里插入图片描述](/img/in-post/20_07/20190403193043686.png)关于f的优化。支持集样本可以用来修改测试样本的embedding模型。这个可以通过一个固定步数的LSTM和对支持集的attention模型来解决, 即如下公式，其中f’(x)是只依赖测试样本自己的特征，作为LSTM的输入（每步不变），K是LSTM的步数，g(S)是支持集的embedding。由此，模型会忽略支持集S中的一些样本。
![在这里插入图片描述](/img/in-post/20_07/20190403193116763.png)![在这里插入图片描述](/img/in-post/20_07/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MDEyMzEwOA==,size_16,color_FFFFFF,t_70-20201214145900685.png)这两个embedding函数f和g是对特征空间进行了优化，从而让精度提升。
关于训练策略。文章对imagenet进行的采样，制作了3种适合做one/few shot的数据集，其中miniImageNet，它包含100类，每类600张图片，其中80个类用来训练，20类用来测试， 称为后续相关研究经常被采用的数据集。以5-way 5-shot为例。训练时，在80类中随机采样5个类，然后把这5类中的数据分成支持集S和测试B，训练matching net模型来使得在S条件下的B的预测结果误差最小。测试时，在20个未被训练过的类中抽取5类，每类5张图，作为测试支持集S’。如图7，MatchNet方法相对原始的Inception模型能正确识别模型从未见过的轮胎和自行车。
![在这里插入图片描述](/img/in-post/20_07/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MDEyMzEwOA==,size_16,color_FFFFFF,t_70-20201214145901169.png)

### `原型网络 （Prototypical Networks）[3]`

该方法思想十分简单高效，效果也非常好。它学习一个度量空间， 通过计算和每个类别的原型表达的距离来进行分类。文章基于这样的想法：每个类别都存在一个聚在某单个原型表达周围的embedding，该类的原型是support set在embedding空间中的均值。然后，分类问题变成在embedding空间中的最近邻。如图8，c1、c2、c3分别是三个类别的均值中心（称Prototype），将测试样本x进行embedding后，与这3个中心进行距离计算，从而获得x的类别。

![在这里插入图片描述](/img/in-post/20_07/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MDEyMzEwOA==,size_16,color_FFFFFF,t_70-20201214145901549.png)其伪代码也十分清晰：

![在这里插入图片描述](/img/in-post/20_07/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MDEyMzEwOA==,size_16,color_FFFFFF,t_70-20201214145900946.png)文章采用在Bregman散度下的指数族分布的混合密度估计，实验表明squared Euclidean距离比cosine距离要好14到17个点。另外，文章在训练时采用相对测试时更多的类别数，即训练时每个episodes采用20个类(20 way)，而测试对在5个类（5 way）中进行，其效果相对训练时也采用5 way的提升了2.5个点。

## 小样本学习— 基于graph neural network

`基于graph neural network`

这是一篇比较新的文章，提交到ICLR 2018[4]。他定义了一个图神经网络框架，端到端地学习消息传递的“关系”型任务。在这里，每个样本看成图的节点，该方法不仅学习节点的embedding，也学习边的embedding。如图9，在网络第一层5个样本通过边模型A～构建了图，接着通过图卷积（graph conv）获得了节点的embedding，然后在后面的几层继续用A～更新图、用graph conv更新节点embedding, 这样便构成了一个深度GNN，最后输出样本的预测标签。
![在这里插入图片描述](/img/in-post/20_07/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MDEyMzEwOA==,size_16,color_FFFFFF,t_70-20201214145900970.png)在构建边模型时，先采用一个4层的CNN网络获得每个节点特征向量，然后将节点对xi,xj的差的绝对值过4层带Batch Norm和Leaky Relu的全连接层，从而获得边的embedding，如图10的左侧。随后，我们将节点的embedding和边的embedding一起过图卷积网络，从而获得更新后的节点的embedding，如图10的右侧。
![在这里插入图片描述](/img/in-post/20_07/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MDEyMzEwOA==,size_16,color_FFFFFF,t_70-20201214145900766.png)这篇文章和Siamese networks、Matching networks和Prototypical networks在本质上是有相同之处的，可以看作这3种方法的推广，而且用图来解决这类问题是一种创新，并取得了不错的效果。

## 小样本学习— 基于元学习meta learning

## 效果对比

 比较one/fewshot learning的方法一般采用Omniglot和miniImagenet两个数据集，由于前者相对比较简单，准确率已经比较容易达到99%，所以这里只给出miniImagenet上的对比测试结果。miniImagenet的数据集从 https://drive.google.com/file/d/0B3Irx3uQNoBMQ1FlNXJsZUdYWEE/view 这里下载。

| method                     | 5-way 1-shot      | 5-way 5-shot      |
| -------------------------- | ----------------- | ----------------- |
| baseline finetune          | 28.86 ± 0.54%     | 49.79 ± 0.79%     |
| baseline nearest neighbors | 41.08 ± 0.70%     | 51.04 ± 0.65%     |
| matching network           | 43.56 ± 0.84%     | 55.31 ± 0.73%     |
| **Prototypical network**   | **49.42 ± 0.78%** | **68.20 ± 0.66%** |
| **graph neural network**   | **49.8% ±0.22%**  | **65.5% ±0.20%**  |
| meta-learning LSTM         | 43.44 ± 0.77%     | 60.60 ± 0.71%     |
| Model-Agnostic             | 48.70% ±1.84%     | 63.1% ±0.92%      |

参考文献

[1] G Koch, R Zemel, and R Salakhutdinov. Siamese neural networks for one-shot image recognition. In ICML Deep Learning workshop, 2015.
[2] Oriol Vinyals, Charles Blundell, Tim Lillicrap, Daan Wierstra, et al. Matching networks for one shot learning. In Advances in Neural Information Processing Systems, pages 3630–3638, 2016.
[3] Jake Snell, Kevin Swersky, and Richard S Zemel. Prototypical networks for few-shot learning. arXiv preprint arXiv:1703.05175, 2017.
[4] Victor Garcia, Joan Bruna. Few-shot learning with graph neural networs. Under review as a conference paper at ICLR 2018.
[5] Santoro, Adam, Bartunov, Sergey, Botvinick, Matthew, Wierstra, Daan, and Lillicrap, Timothy. Meta-learning with memory-augmented neural networks. In International Conference on Machine Learning (ICML), 2016.
[6] Ravi, Sachin and Larochelle, Hugo. Optimization as a model for few-shot learning. In International Conference on Learning Representations (ICLR), 2017.
[7] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. arXiv preprint arXiv:1703.03400, 2017.

## Reference

[※※19_4_小样本学习(one/few-shot learning)](https://blog.csdn.net/weixin_40123108/article/details/89003325?utm_medium=distribute.pc_feed_404.none-task-blog-BlogCommendFromBaidu-2.nonecase&depth_1-utm_source=distribute.pc_feed_404.none-task-blog-BlogCommendFromBaidu-2.nonecas)

---

<!-- 
_class: lead gaia
_paginate: false
_color: black -->

<style scoped>
section {
    font-size: 30px;
}
</style>


# 度量学习— 相关论文

---

## 20_CVPR_(Hinton)A Simple Framework for Contrastive Learning of Visual Representations

提出SimCLR:  可用于视觉表示的一种**对比学习(Contrastive Learning)** 的简单框架

- 用的是对比损失函数, 最小化正对间距离, 最大化负对间距离 
- 自监督学习: 标签产生方式不同 

与度量学习结合, 训练网络

1. **无监督的方式学习表示网络(度量网络)** 
2. 卷积神经网络提取特征
3. 少量样本微调网络

样本增强方式

1. 随机裁剪
2. **颜色增强**

---

主要工作:

1. 我们**简化了最近提出的的对比自监督学习算法**, 使其无需专门的架构或存储库 。

2. 为了了解什么使对比预测任务能够学习有用的表示形式，我们系统地研究了框架的主要组成部分。

3. 提出了如下主要结论 :
   1. **多个数据扩充方法的组合**与**数据增强**非常重要, 尤其使用无监督学习方法时
   2. **在表示 (特征) 和对比损失间引入可学习的非线性变换**可以大幅度提高模型学到的表示的质量

   3. 对于对比学习更大的**批处理数量**和更多的**训练次数**的重要性

---

使用通过Sim-CLR自监督学习到的表示来训练线性分类器得到的效果: 

1. 大幅胜过ImageNet上先前用于自监督和半监督学习的先前方法。可达到76.5％的top-1准确性，与最新技术相比相对提**高了7％**.
2. 与监督的ResNet-50的性能相匹配。
   在仅使用 1% 的 ImageNet 标签进行微调时，SimCLR 实现了 85.8% 的 top-5 准确率，比之前的 SOTA 方法**提升了 10%**。

![bg right w:15cm](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/image-20201214163927589.png)



图为SimCLR 与此前各类自监督方法在 ImageNet 上的 Top-1 准确率对比（以 ImageNet 进行预训练），以及 ResNet-50 的有监督学习效果（灰色×）

------

### 自监督学习

* **训练数据集** -- 不是由人手动标记的，每个样本的标签是通过利用输入的相关性生成的（如来自不同的传感器模式）。

- **标签** -- 通常来自于数据本身: 即模型直接从无标签数据中自行学习，无需标注数据。

- **训练** -- **通过使用各种辅助任务 (auxiliary task ) 训练网络**, 来提高学习表征 (representation) 的质量.

- **核心** **--** 如何自动为数据产生标签。如随机旋转, 均匀分割而自动产生的标注

- **性能评价** -- 通过模型学出来的**feature的质量**来评价. feature质量是通过迁移学习的方式，把feature用到其它视觉任务中通过结果的好坏来评价。

![bg right:40% w:20cm drow-shadow](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/image-20201214164627891.png)



------

### 整体流程

SimCLR 通过**隐空间中的对比损失来最大化同一数据示例的不同增强视图之间的一致性，从而学习到特征表示**。具体说来，这一框架包含**四个主要部分**：

- 随机数据增强模块

- 基本的神经网络编码器 f(·) -- 特征网络

- 神经网络映射头 g(·) -- 变换网络

- 对比预测任务的对比损失函数 ![w:13cm](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/image-20201214165044531.png)
  其中, ![w:7cm](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/image-20201214165102360.png)

相比之前对比学习模型: 结构更简单, 省去数据存储队列

![bg right:33% w:10cm](https://cdn.jsdelivr.net/gh/sunqinghu/PicRepo/img/2020/image-20201214165327114.png)

------



---

---

## 论文笔记

[2020-03-02-论文学习-A CNN With Multiscale Convolution and Diversified Metric   for Hyperspectral Image Classification](/Users/king/sunqinghu.github.io/_posts/2020-03-02-论文学习-A CNN With Multiscale Convolution and Diversified Metric   for Hyperspectral Image Classification.md)

[2020-03-19-论文学习-20_(HInton|CL)A Simple Framework for Contrastive Learning of Visual Representations](/Users/king/sunqinghu.github.io/_posts/2020-03-19-论文学习-20_(HInton|CL)A Simple Framework for Contrastive Learning of Visual Representations?.md)



